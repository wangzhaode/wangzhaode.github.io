<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zhaode.wang/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zhaode.wang/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-20T09:49:24+00:00</updated><id>https://zhaode.wang/feed.xml</id><title type="html">Zhaode’s blog</title><subtitle>A blog about technology and life. </subtitle><entry><title type="html">一图读懂Qwen</title><link href="https://zhaode.wang/blog/2025/qwenfamily/" rel="alternate" type="text/html" title="一图读懂Qwen"/><published>2025-09-25T00:00:00+00:00</published><updated>2025-09-25T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/qwenfamily</id><content type="html" xml:base="https://zhaode.wang/blog/2025/qwenfamily/"><![CDATA[<p>随着最近举办的云栖大会，“通义千问”系列模型的密集发布引发了不少讨论。 网络上流传的表情包所调侃的，从最初的几个核心模型，到现在“全家桶”式的发布，让不少人直呼“跟不上了”。</p> <p><img src="/assets/img/qwenfamily/gptqwen.png" alt="gpt-qwen"/></p> <p>不过调侃归调侃，通义千问命名相比GPT的“阴间”命名风格还是很“阳间”的。为了帮助大家更清晰地梳理“Qwen家族”的全貌，我制作了一张<strong>Qwen全系列模型汇总及发布时间线图</strong>。</p> <p>从这张图中，你可以直观地看到：</p> <ul> <li><strong>迭代速度快</strong>：通义千问在短时间内完成了多次重要更新，快速覆盖了从小型到超大规模（如参数超过万亿的Qwen3-Max）的完整模型尺寸梯度。</li> <li><strong>家族成员多</strong>：除了基础的语言模型，Qwen还衍生出了众多针对特定任务和场景的“专家模型”，例如强化数学和逻辑推理能力的模型、专注于代码生成的Coder系列以及具备音视频理解能力的多模态模型。</li> <li><strong>技术路线广</strong>：Qwen系列不仅有传统的密集（Dense）模型，还积极探索了混合专家（MoE）架构，旨在实现性能与效率的平衡。</li> </ul> <p><img src="/assets/img/qwenfamily/qwenfamily.png" alt="gpt-qwen"/></p> <p><em>标题图由Nano Banana生成，网页由Qwen Coder模型辅助生成，<a href="https://zhaode.wang/llm/qwenfamily">图片原始网页</a></em></p>]]></content><author><name></name></author><category term="llm"/><summary type="html"><![CDATA[随着最近举办的云栖大会，“通义千问”系列模型的密集发布引发了不少讨论。 网络上流传的表情包所调侃的，从最初的几个核心模型，到现在“全家桶”式的发布，让不少人直呼“跟不上了”。]]></summary></entry><entry><title type="html">端侧LLM硬件系列（二）：内存容量</title><link href="https://zhaode.wang/blog/2025/device-llm-memory-capacity/" rel="alternate" type="text/html" title="端侧LLM硬件系列（二）：内存容量"/><published>2025-09-22T00:00:00+00:00</published><updated>2025-09-22T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/device-llm-memory-capacity</id><content type="html" xml:base="https://zhaode.wang/blog/2025/device-llm-memory-capacity/"><![CDATA[<p>随着iPhone 17 Pro系列将运行内存（RAM）从8GB升级至12GB，一个明确的信号已经发出：为了迎接真正的端侧AI时代，移动设备正在着手应对一个新的、也是更基础的硬件瓶颈——内存容量。</p> <p>这次升级并非为了常规的多任务处理，而是为了给“Apple Intelligence”这类日益复杂的端侧大模型提供必要的运行空间。你或许也曾遇到过类似情况：功能强大的AI应用突然闪退，或在处理稍长文档时无响应。这背后的原因，往往不是芯片算力（TOPS）不足，而是手机的物理内存已经耗尽。</p> <p>内存容量是决定端侧AI模型的入场券，这篇文章将深入分析LLM的内存占用构成，探讨各类优化技术，并最终评估在2025年，多大的内存才足以支撑一个流畅的“口袋里的AI大脑”。</p> <h2 id="llm内存占用的构成三大核心部分的量化分析"><strong>LLM内存占用的构成：三大核心部分的量化分析</strong></h2> <p>要理解内存瓶颈，首先需要精确拆解LLM在运行时到底消耗了什么。其内存占用主要由三个动态和静态的部分构成，每一部分都有其详细的计算规则。</p> <h3 id="1-模型权重-model-weights"><strong>1. 模型权重 (Model Weights)</strong></h3> <p>这是模型最基础、最主要的内存开销，可以理解为模型的“知识库”。其大小由参数量和数据精度（Precision）决定。</p> <ul> <li><strong>数据精度与字节数:</strong> <ul> <li><strong>FP32 (单精度浮点):</strong> 每参数占4字节。</li> <li><strong>FP16/BF16 (半精度浮点):</strong> 每参数占2字节，是当前推理最常用的格式。</li> <li><strong>INT8 (8位整型):</strong> 每参数占1字节（量化后）。</li> <li><strong>INT4 (4位整型):</strong> 每参数占0.5字节（量化后）。</li> </ul> </li> <li><strong>计算公式:</strong> <blockquote> <p><strong>内存占用 (GB) ≈ 参数量 (十亿) × 单个参数大小 (Bytes)</strong></p> </blockquote> </li> </ul> <p>基于此公式，我们可以清晰地看到不同规模模型在加载时对内存的静态需求：</p> <table> <thead> <tr> <th style="text-align: left">模型规模（参数）</th> <th style="text-align: left">FP16 (2 Bytes/Param)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>7B (70亿)</strong></td> <td style="text-align: left">≈ 14 GB</td> </tr> <tr> <td style="text-align: left"><strong>13B (130亿)</strong></td> <td style="text-align: left">≈ 26 GB</td> </tr> </tbody> </table> <p><em>注：为方便估算，此表采用1GB = 10^9 Bytes的近似值。</em></p> <p>这个静态值直接决定了模型能否被加载进手机的“生死线”。从表中可见，一个未经优化的7B模型，仅权重部分就超过了市面上绝大多数手机的内存上限。</p> <h3 id="2-激活值-activations"><strong>2. 激活值 (Activations)</strong></h3> <p>在模型进行前向计算（即“思考”过程）时，每一层网络都会产生临时的中间数据。这部分内存占用是动态的，在处理长文本的Prefill（预填充）阶段会达到峰值。</p> <ul> <li><strong>经验估算法:</strong> 精确计算激活值内存较为复杂，因为它与具体模型架构和推理引擎实现紧密相关。在工程实践中，一个实用的快速估算法则是，<strong>激活值的峰值内存约等于模型权重（FP16）的25%</strong>。对于一个14GB的7B模型，其激活值峰值约为3.5GB。</li> </ul> <h3 id="3-kv-cache"><strong>3. KV Cache</strong></h3> <p>这是在Decoding（解码/逐字生成）阶段为了加速计算而设计的缓存机制。它存储了注意力机制已经计算过的键（Key）和值（Value），避免重复计算。</p> <ul> <li> <p><strong>核心特点:</strong> KV Cache的大小与上下文长度（Sequence Length）成正比，是长对话或长文总结场景下最主要的内存增长点。</p> </li> <li> <p><strong>精确计算公式:</strong></p> <blockquote> <p><strong>KV Cache内存 (Bytes) ≈ 2 × 上下文长度 × 模型层数 × 注意力头个数 × 注意力维度 × 精度(Bytes)</strong></p> </blockquote> <p>不同模型的具体架构参数不同，导致KV Cache大小差异巨大。以一个采用了GQA结构优化的现代模型，如<strong>Qwen2-7B</strong>（拥有28个Transformer层，K/V注意力头为4个，每个头的维度为128）为例，在FP16精度下，处理4096个Token的上下文，其KV Cache占用约为：</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">2 × 4096 × 28 × 4 × 128 × 2 Bytes ≈ 235 MB</code></p> </blockquote> </li> </ul> <h3 id="综合内存占用分析"><strong>综合内存占用分析</strong></h3> <p>现在，我们将一个典型的、未经优化的7B模型的FP16内存占用进行加总，看看它的峰值需求有多庞大：</p> <table> <thead> <tr> <th style="text-align: left">内存组成部分</th> <th style="text-align: left">FP16 (未优化)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>权重内存 (静态)</strong></td> <td style="text-align: left">≈ 14.0 GB</td> </tr> <tr> <td style="text-align: left"><strong>激活值内存 (动态峰值, 估算)</strong></td> <td style="text-align: left">≈ 3.5 GB</td> </tr> <tr> <td style="text-align: left"><strong>KV Cache (4K tokens, 典型值)</strong></td> <td style="text-align: left">≈ 2.1 GB</td> </tr> <tr> <td style="text-align: left"><strong>预估总内存峰值</strong></td> <td style="text-align: left"><strong>≈ 19.6 GB</strong></td> </tr> </tbody> </table> <p><em>注：KV Cache大小因模型结构而异，此处采用一个典型非GQA优化模型的数值以展示问题的严重性。</em></p> <p>结论已经非常清晰：一个未经优化的7B模型，其近20GB的峰值内存需求，在移动端是绝对无法满足的物理限制。这使得下一章将要讨论的内存优化技术，不再是“可选项”，而是让端侧大模型成为现实的“必需品”。</p> <hr/> <h2 id="内存优化技术在有限容量下实现可能"><strong>内存优化技术：在有限容量下实现可能</strong></h2> <p>在物理内存有限的前提下，软件和算法层面的优化是让大模型在端侧运行的关键。这些技术主要围绕内存占用的三个核心部分展开。</p> <h3 id="1-权重量化-weight-quantization"><strong>1. 权重量化 (Weight Quantization)</strong></h3> <p>这是最核心、效果最显著的模型压缩技术。其原理是将高精度（如FP16）的浮点数权重，转换为低精度（如INT8或INT4）的整数，从而大幅减少模型的存储体积。</p> <ul> <li><strong>技术细节：</strong> 为保证精度，现代量化方案（如GPTQ/AWQ）普遍采用<strong>分组量化 (Grouped Quantization)</strong>。即将权重分为多个小组（例如每32个或64个为一组），并为每个小组计算独立的量化参数（<strong>Scale</strong>，即比例尺）。</li> <li><strong>有效比特率 (bpw):</strong> 这也意味着量化后的实际成本会略高于其标称位数。例如，对一个INT4量化模型，如果每32个权重共享一个FP16（16 bit）的Scale，则每个权重的平均占用为 <code class="language-plaintext highlighter-rouge">(32 * 4 + 16) / 32 = 4.5 bpw</code>。</li> <li><strong>优化结果：</strong> 基于4.5 bpw计算，一个原本需要14GB的7B模型，其权重体积可以被压缩至约 <strong>3.7 GB</strong>，使其具备了在移动设备上加载的可能性。</li> </ul> <h3 id="2-激活值优化技术"><strong>2. 激活值优化技术</strong></h3> <p>主要用于降低处理长序列时产生的瞬时内存峰值，核心思路是“以时间换空间”。</p> <ul> <li><strong>激活重计算 (Activation Recomputation):</strong> 不在内存中保留所有中间层的激活值，而是在需要时通过前向计算从上一个“检查点”重新推导。</li> <li><strong>分块预填充 (Chunk Prefill):</strong> 将长输入切分成小块，逐块进行计算并填充KV Cache。每处理完一小块，其对应的激活值内存即可释放，从而避免了巨大的瞬时内存开销。</li> </ul> <h3 id="3-kv-cache-优化技术"><strong>3. KV Cache 优化技术</strong></h3> <p>旨在降低长上下文场景下的内存占用。</p> <ul> <li><strong>KV Cache量化:</strong> 同样可以将KV Cache中的数据从FP16量化至INT8，直接将其内存占用降低50%。</li> <li><strong>模型结构优化 (GQA):</strong> 分组查询注意力（Grouped-Query Attention, GQA）是目前主流的优化结构。它通过让多组“注意力头”共享同一套K和V缓存，在大幅降低KV Cache内存占用的同时，实现了性能和效果的最佳平衡。</li> </ul> <h3 id="4-混合存储方案"><strong>4. 混合存储方案</strong></h3> <p>当上述优化仍无法满足需求时，最后的手段是利用速度较慢但容量巨大的闪存。</p> <ul> <li><strong>内存卸载 (Offloading):</strong> 将当前不活跃的模型层（比如<code class="language-plaintext highlighter-rouge">Embedding</code>层）或较早的KV Cache从RAM中转移到闪存，需要时再加载回来。这是一种以牺牲响应速度（延迟）为代价，换取更大有效容量的终极方案。</li> </ul> <hr/> <h3 id="优化成效分析全新的内存占用"><strong>优化成效分析：全新的内存占用</strong></h3> <p>经过上述一系列组合拳式的优化，我们现在可以重新计算同一个7B模型在端侧运行时的实际内存占用：</p> <table> <thead> <tr> <th style="text-align: left">内存组成部分</th> <th style="text-align: left">优化后 (4-bit量化 + GQA)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>权重内存 (4.5 bpw)</strong></td> <td style="text-align: left">≈ 3.7 GB</td> </tr> <tr> <td style="text-align: left"><strong>激活值内存 (动态峰值, 估算)</strong></td> <td style="text-align: left">≈ 0.9 GB</td> </tr> <tr> <td style="text-align: left"><strong>KV Cache (4K tokens, Qwen2-7B, INT8)</strong></td> <td style="text-align: left">≈ 0.12 GB</td> </tr> <tr> <td style="text-align: left"><strong>预估总内存峰值</strong></td> <td style="text-align: left"><strong>≈ 4.7 GB</strong></td> </tr> </tbody> </table> <p>从接近20GB到不足5GB——这是一个超过75%的惊人降幅。这标志着，一个强大的70亿参数大模型，在理论上终于获得了进入主流8GB、畅行12GB内存手机的“资格”。然而，理论上的可行性，还需要通过操作系统层面严苛的现实考验。</p> <hr/> <h2 id="操作系统的内存管理机制"><strong>操作系统的内存管理机制</strong></h2> <p>优化后的模型最终仍需在操作系统的管理下运行。Android和iOS的机制差异，直接影响了应用的稳定性。</p> <ul> <li><strong>Android:</strong> 采用<strong>低内存杀手（Low-Memory Killer, LMK）</strong>机制。当系统总内存不足时，会根据进程优先级强制“杀死”后台应用。一个占用数GB内存的AI应用，即便能运行，也非常容易在切换到后台时被LMK清理。</li> <li><strong>iOS:</strong> 对每个应用有严格的内存使用上限。一旦应用消耗的“脏内存”（Dirty Memory，由程序动态分配的内存）超限，就会被系统强制终止（闪退）。 <ul> <li>在iOS中运行LLM类型的任务，需要申请<code class="language-plaintext highlighter-rouge">com.apple.developer.kernel.increased-memory-limit</code>权限，可以提升APP的内存上限。</li> <li>另外一个有效方案是使用<code class="language-plaintext highlighter-rouge">mmap</code>技术加载模型权重。<code class="language-plaintext highlighter-rouge">mmap</code>将只读的权重映射为“干净内存”（Clean Memory），这部分内存iOS并不会计入APP的内存占用，从而避免了因触及“脏内存”上限导致闪退。</li> </ul> </li> </ul> <hr/> <h2 id="结论多大内存的手机才配得上ai大脑的称号"><strong>结论：多大内存的手机，才配得上“AI大脑”的称号？</strong></h2> <p>综合以上分析，我们可以得出一份基于2025年主流设备的AI能力评估矩阵：</p> <h3 id="真实世界ai能力适配矩阵-2025年机型"><strong>真实世界AI能力适配矩阵 (2025年机型)</strong></h3> <table> <thead> <tr> <th style="text-align: left">设备典型代表</th> <th style="text-align: left">物理内存容量</th> <th style="text-align: left">运行7B模型 (4-bit, GQA) 体验评估</th> <th style="text-align: left">运行13B模型 (4-bit, GQA) 体验评估</th> <th style="text-align: left">场景分析与最终评价</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>标准版iPhone 17 / 主流安卓中端机</strong></td> <td style="text-align: left"><strong>8GB</strong></td> <td style="text-align: left"><strong>勉强可用</strong></td> <td style="text-align: left"><strong>几乎不可行</strong></td> <td style="text-align: left"><strong>体验受限</strong>：模型加载后系统可用内存极低，多任务处理能力差，后台易被终止。仅适合轻量级、非连续的AI任务。物理容量是硬伤。</td> </tr> <tr> <td style="text-align: left"><strong>iPhone 17 Pro / 安卓主流高端机</strong></td> <td style="text-align: left"><strong>12GB</strong></td> <td style="text-align: left"><strong>体验流畅</strong></td> <td style="text-align: left"><strong>基本可行</strong></td> <td style="text-align: left"><strong>AI体验基准</strong>：为当前主流的7B模型提供了充足的运行空间和良好的多任务缓冲。是硬件容量和软件优化结合的最佳平衡点，构成了高质量端侧AI体验的基础。</td> </tr> <tr> <td style="text-align: left"><strong>安卓旗舰机</strong></td> <td style="text-align: left"><strong>16GB</strong></td> <td style="text-align: left"><strong>游刃有余</strong></td> <td style="text-align: left"><strong>体验流畅</strong></td> <td style="text-align: left"><strong>面向未来</strong>：不仅能轻松驾驭13B模型，也为更复杂的AI应用（如多模态交互）预留了充足的硬件冗余。是追求极致性能和长期AI体验保障的最佳选择。</td> </tr> <tr> <td style="text-align: left"><strong>安卓顶级旗舰机</strong></td> <td style="text-align: left"><strong>24GB</strong></td> <td style="text-align: left"><strong>性能过剩</strong></td> <td style="text-align: left"><strong>游刃有余</strong></td> <td style="text-align: left"><strong>开发与探索</strong>：远超当前普通用户的需求，主要价值在于为开发者提供了一个不受内存束缚的实验平台，探索端侧AI的未来可能性，可运行<code class="language-plaintext highlighter-rouge">GPT-OSS-20B</code>。</td> </tr> </tbody> </table> <hr/> <h3 id="最终结论"><strong>最终结论</strong></h3> <p>分析指向一个明确的结果：<strong>物理内存容量是端侧AI能力的根基，而软件优化则是在这个根基上实现效率最大化的手段。</strong></p> <ol> <li><strong>8GB内存</strong> 在端侧AI时代已显不足，无法提供稳定、流畅的大模型体验。</li> <li><strong>12GB内存</strong> 正如iPhone 17 Pro所展示的，已成为2025年“AI手机”的有效起步线，是保证高质量体验的基础。</li> <li><strong>16GB内存</strong> 则提供了更强的性能保障和面向未来的扩展性，足以流畅运行下一代百亿参数模型。</li> </ol> <p>最终，决定手机AI体验上限的，不再仅仅是算力（TOPS）数字，而是那个实实在在的物理内存容量。在这场将AI装进口袋的竞赛中，内存，正是那张最关键的“入场券”。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[随着iPhone 17 Pro系列将运行内存（RAM）从8GB升级至12GB，一个明确的信号已经发出：为了迎接真正的端侧AI时代，移动设备正在着手应对一个新的、也是更基础的硬件瓶颈——内存容量。]]></summary></entry><entry><title type="html">Qwen3-Next：下一代MoE模型架构解析</title><link href="https://zhaode.wang/blog/2025/qwen3-next/" rel="alternate" type="text/html" title="Qwen3-Next：下一代MoE模型架构解析"/><published>2025-09-10T00:00:00+00:00</published><updated>2025-09-10T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/qwen3-next</id><content type="html" xml:base="https://zhaode.wang/blog/2025/qwen3-next/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">transformers</code> 库近期合并了 <code class="language-plaintext highlighter-rouge">Qwen3-Next</code> 的 <a href="https://github.com/huggingface/transformers/pull/40771/">PR</a>，正式将其纳入官方生态。这个 PR 提交了 <code class="language-plaintext highlighter-rouge">Qwen3-Next-80B-A3B-Instruct</code> 模型的实现，该模型被介绍为下一代基础模型，针对超长上下文和大规模参数效率进行了深度优化。官方的描述为：</p> <blockquote> <p>The Qwen3-Next series represents our next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency.</p> </blockquote> <p>该模型的四大核心亮点为：</p> <ul> <li><strong>高稀疏度 MoE (High-Sparsity MoE)</strong>: 实现了极低的计算激活比，在保持庞大知识容量的同时，追求极致的推理性能。</li> <li><strong>混合注意力 (Hybrid Attention)</strong>: 融合<strong>门控增量网络 (Gated DeltaNet)</strong> 与<strong>门控注意力 (Gated Attention)</strong>，高效建模不同距离的上下文依赖。</li> <li><strong>多词元预测 (MTP)</strong>: 提升模型性能并为推理加速设计的先进预训练目标。</li> <li><strong>其他优化</strong>: 包括<strong>零中心化的 RMSNorm</strong> 等，旨在增强训练稳定性。</li> </ul> <p>下面我们来深入 PR 内容，逐一解析 Qwen3-Next 的架构创新。</p> <h2 id="一高稀疏度-moe-与共享专家">一、高稀疏度 MoE 与共享专家</h2> <p>Qwen3-Next 的混合专家（MoE）设计有两个关键点：高稀疏度和共享专家。</p> <p><strong>1. 高稀疏度对解码性能的提升</strong></p> <p>高稀疏度是 Qwen3-Next 实现极致性能的核心。以 80B 版本为例，它拥有 800 亿总参数，但在生成每一个 token 时，仅需激活其中的 30 亿参数进行计算。这一激活比远低于当前主流 MoE 模型，是一项关键的架构选择。</p> <p>这并非简单的“节约”，而是对性能的直接赋能。在自回归生成（decode）任务中，模型需要逐词进行前向传播，此时<strong>每一步的计算量 (FLOPs) 直接决定了生成速度</strong>。通过将激活参数降低一个数量级，Qwen3-Next 实现了：</p> <ul> <li><strong>更高的吞吐量</strong>：在处理长上下文（&gt;32K tokens）时，其推理吞吐量可达 Qwen3-32B 的 <strong>10 倍以上</strong>。</li> <li><strong>更快的响应速度</strong>：对于用户而言，这意味着更低的延迟和更流畅的交互体验。</li> </ul> <p>可以说，高稀疏度 MoE 是 Qwen3-Next 在解码性能提升上的核心引擎。</p> <p><strong>2. 共享专家增强稳定性</strong></p> <p>为确保极致稀疏下的稳定性，Qwen3-Next 在 MoE 模块中额外增加了一个<strong>共享专家（Shared Expert）</strong>，形成了一种更稳健的“双轨”设计。</p> <pre><code class="language-mermaid">flowchart TD
    A[输入 Hidden State] --&gt;|所有 Token| B("共享专家 (Shared Expert)");
    A --&gt;|所有 Token| C{"路由 (Router)"};

    subgraph "稀疏专家路径 (Sparse Path)"
        C -- 为每个 Token 选择 Top-K --&gt; D[Expert 1];
        C -- 为每个 Token 选择 Top-K --&gt; E[...];
        C -- 为每个 Token 选择 Top-K --&gt; F[Expert N];
        D &amp; E &amp; F --&gt; G("对 Top-K 专家输出进行加权求和");
    end

    subgraph "共享路径 (Dense Path)"
        B --&gt; H(共享路径输出);
    end

    G --&gt; I{"add"};
    H --&gt; I;
    I --&gt; L[最终输出];

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style L fill:#ccf,stroke:#333,stroke-width:2px
</code></pre> <p>如上图所示，输入 Token 会兵分两路：一路通过路由器选择 Top-K 个稀疏专家进行<strong>专业化</strong>计算；另一路则全部通过一个共享专家进行<strong>通用化</strong>计算。这种设计好比一个会诊流程：共享专家如同经验丰富的“全科医生”，处理基础和通用的语言模式；稀疏专家则像“专科医生”，处理更细分、更专业的知识。<code class="language-plaintext highlighter-rouge">Qwen3NextSparseMoeBlock</code> 的代码清晰地实现了这一并行结构，共享专家的存在为模型提供了一个稳定的计算基座，极大地提升了模型的鲁棒性。</p> <h2 id="二混合注意力与-gateddeltanet-详解">二、混合注意力与 GatedDeltaNet 详解</h2> <p>Qwen3-Next 采用混合注意力架构以高效处理长上下文。它在不同层交替使用 O(N²) 复杂度的标准注意力和 O(N) 复杂度的线性注意力，实现了能力与效率的平衡。</p> <pre><code class="language-mermaid">graph LR
    Input --&gt; Layer_i["Layer i&lt;br/&gt;&lt;b&gt;Gated Full Attention&lt;/b&gt;&lt;br/&gt;(O(N²) 复杂度, 精准捕捉)"];
    Layer_i --&gt; Layer_i_plus_1["Layer i+1&lt;br/&gt;&lt;b&gt;Gated DeltaNet (线性)&lt;/b&gt;&lt;br/&gt;(O(N) 复杂度, 高效长距)"];
    Layer_i_plus_1 --&gt; Output;
</code></pre> <p><strong>GatedDeltaNet 计算机制详解</strong></p> <p>其中，<code class="language-plaintext highlighter-rouge">Qwen3NextGatedDeltaNet</code> 是实现线性注意力的核心模块。它通过一套精密的“输入-卷积-门控-递归-输出”流程，在保持线性复杂度的同时，实现了对长距离依赖的有效建模。</p> <ol> <li> <p><strong>输入映射</strong>: 输入 <code class="language-plaintext highlighter-rouge">hidden_states</code> 被线性层 (<code class="language-plaintext highlighter-rouge">in_proj_qkvz</code>, <code class="language-plaintext highlighter-rouge">in_proj_ba</code>) 投影成一系列中间状态：<code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code>, <code class="language-plaintext highlighter-rouge">value</code> 及用于门控的 <code class="language-plaintext highlighter-rouge">z</code>, <code class="language-plaintext highlighter-rouge">b</code>, <code class="language-plaintext highlighter-rouge">a</code>。</p> </li> <li><strong>因果卷积 (局部信息)</strong>: 拼接后的 <code class="language-plaintext highlighter-rouge">qkv</code> 张量经过一个一维因果卷积，它像一个滑动窗口，用于高效捕捉每个 Token 与其附近邻居之间的局部上下文。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 文件: modeling_qwen3_next.py -&gt; class Qwen3NextGatedDeltaNet
</span><span class="n">mixed_qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">causal_conv1d_fn</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">mixed_qkv</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">conv1d</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="c1"># ...
</span><span class="p">)</span>
</code></pre></div> </div> </li> <li><strong>门控信号生成</strong>: 模型从投影 <code class="language-plaintext highlighter-rouge">a</code> 和 <code class="language-plaintext highlighter-rouge">b</code> 中，学习两个关键的、类似 RNN 的门控信号： <ul> <li><code class="language-plaintext highlighter-rouge">beta</code> (输入门): 通过 <code class="language-plaintext highlighter-rouge">sigmoid</code> 函数，控制有多少新信息（<code class="language-plaintext highlighter-rouge">value</code>）可以被写入“状态”。</li> <li><code class="language-plaintext highlighter-rouge">g</code> (遗忘门): 控制历史信息的衰减率，决定“记忆”能保留多久。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">()</span>
<span class="n">g</span> <span class="o">=</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">A_log</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">exp</span><span class="p">()</span> <span class="o">*</span> <span class="n">F</span><span class="p">.</span><span class="nf">softplus</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">dt_bias</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> </li> <li><strong>门控增量规则 (全局信息)</strong>: 将 <code class="language-plaintext highlighter-rouge">q, k, v</code> 和门控信号 <code class="language-plaintext highlighter-rouge">beta, g</code> 送入核心的递归函数，进行全局信息传递。这一步是实现 O(N) 复杂度的关键。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">core_attn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">chunk_gated_delta_rule</span><span class="p">(</span>
    <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="bp">...</span>
<span class="p">)</span>
</code></pre></div> </div> </li> <li><strong>输出门控</strong>: <code class="language-plaintext highlighter-rouge">GatedDeltaNet</code> 的输出 <code class="language-plaintext highlighter-rouge">core_attn_out</code> 还会被 <code class="language-plaintext highlighter-rouge">z</code> 进行最终的门控调制，然后才传递给下一层。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">core_attn_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">core_attn_out</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</code></pre></div> </div> <p>通过这套流程，<code class="language-plaintext highlighter-rouge">GatedDeltaNet</code> 实现了“卷积捕捉局部，递归传递全局”的高效信息处理模式。</p> </li> </ol> <h2 id="三多词元预测-mtp">三、多词元预测 (MTP)</h2> <p>多词元预测 (MTP) 是一种先进的预训练目标，它在预训练和推理阶段都有显著增益。</p> <ul> <li><strong>预训练阶段</strong>: 传统模型在 <code class="language-plaintext highlighter-rouge">t</code> 时刻只预测 <code class="language-plaintext highlighter-rouge">t+1</code> 的词元。MTP 则要求模型在 <code class="language-plaintext highlighter-rouge">t</code> 时刻同时预测 <code class="language-plaintext highlighter-rouge">t+1</code>, <code class="language-plaintext highlighter-rouge">t+2</code>, …, <code class="language-plaintext highlighter-rouge">t+n</code> 多个未来的词元。这有助于模型学习更具前瞻性的语言模式，提升了其对因果关系的理解。</li> <li><strong>推理阶段</strong>: MTP 的能力天然适配<strong>思辨解码 (Speculative Decoding)</strong>。模型可以一次性生成 <code class="language-plaintext highlighter-rouge">n</code> 个候选 token，再由系统并行验证，从而在命中率高的情况下，数倍提升解码速度，是实现高效推理的关键技术之一。</li> </ul> <p><code class="language-plaintext highlighter-rouge">Qwen3NextPreTTrainedModel</code> 类中的 <code class="language-plaintext highlighter-rouge">_keys_to_ignore_on_load_unexpected = [r"^mtp.*"]</code> 这行代码，证实了 MTP 是其预训练阶段的一部分，相关的权重已经融入模型，即使推理代码中没有显式的多头预测结构。</p> <h2 id="四其他架构优化零中心化-rmsnorm">四、其他架构优化：零中心化 RMSNorm</h2> <p>Qwen3-Next 对 RMSNorm 进行了精巧的改进，以增强训练稳定性。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 文件: modeling_qwen3_next.py -&gt; class Qwen3NextRMSNorm
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 核心: 乘以 (1.0 + weight)，而非直接乘 weight
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_norm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">.</span><span class="nf">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>由于 <code class="language-plaintext highlighter-rouge">self.weight</code> 初始化为 0，该层在训练初期近似于一个无参数的纯归一化操作。这个看似微小的改动有助于稳定梯度在网络深层的传播，特别是在训练初期，对避免梯度爆炸或消失问题有积极作用。</p> <h2 id="总结">总结</h2> <p><code class="language-plaintext highlighter-rouge">Qwen3-Next</code> 的设计哲学是在“大而全”和“小而美”之间寻找最佳平衡点。它并非依赖单一技术的颠覆，而是通过一系列精心设计的技术组合：</p> <ul> <li><strong>高稀疏度 MoE 与共享专家</strong>，平衡了计算负载与模型鲁棒性。</li> <li><strong>混合注意力与 GatedDeltaNet</strong>，平衡了对不同距离上下文的建模能力与计算效率。</li> <li><strong>MTP</strong>，同时优化了模型的预训练效果与推理速度。</li> <li><strong>零中心化 RMSNorm</strong> 等细节，提升了训练的稳定性。</li> </ul> <p>这些技术细节的深度融合，最终造就了这款在性能、效率和训练成本上都极具竞争力的下一代大语言模型。</p>]]></content><author><name></name></author><category term="llm"/><summary type="html"><![CDATA[transformers 库近期合并了 Qwen3-Next 的 PR，正式将其纳入官方生态。这个 PR 提交了 Qwen3-Next-80B-A3B-Instruct 模型的实现，该模型被介绍为下一代基础模型，针对超长上下文和大规模参数效率进行了深度优化。官方的描述为：]]></summary></entry><entry><title type="html">端侧LLM硬件系列（一）：内存带宽</title><link href="https://zhaode.wang/blog/2025/device-llm-memory-bandwidth/" rel="alternate" type="text/html" title="端侧LLM硬件系列（一）：内存带宽"/><published>2025-09-02T00:00:00+00:00</published><updated>2025-09-02T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/device-llm-memory-bandwidth</id><content type="html" xml:base="https://zhaode.wang/blog/2025/device-llm-memory-bandwidth/"><![CDATA[<p>在AI大模型席卷一切的今天，我们都期待手机成为真正的“口袋里的AI大脑”。但你有没有感觉，手机上的AI助手总是慢半拍？无论是想让它快速总结一篇长文，还是在图片编辑时使用“AI消除”功能，那种等待的延迟感，总在提醒我们“理想与现实的差距”。</p> <p>这个瓶颈，或许不在于芯片厂商们大力宣传的算力（TOPS）数字有多高，而在于一个常常被忽视的参数——内存带宽。</p> <p>这就像拥有了一台超级跑车的引擎（AI计算核心），却只给它配了一根细细的油管，空有一身力气却踩不上油门。</p> <h2 id="为什么说llm的性能主要卡在内存带宽"><strong>为什么说LLM的性能，主要卡在内存带宽？</strong></h2> <p>要理解这个问题，我们首先需要知道，大模型在手机上生成内容，分为两个关键阶段：</p> <ol> <li><strong>Prefill（预填充/提示处理）</strong>：这是模型“读懂问题”的阶段。它会并行处理你输入的所有文字（Prompt），比如“帮我写一首关于星空的诗”。这个阶段计算量大，可以充分利用NPU/GPU的并行计算能力，速度通常很快。</li> <li><strong>Decoding（解码/逐字生成）</strong>：这是模型“思考并回答”的阶段。它会一个字一个字地生成答案，比如“浩瀚的…夜空…”。每生成一个新字，都需要把前面所有的字连同新生成的字一起，再和全部的模型参数进行一次运算。</li> </ol> <p><strong>而瓶颈，恰恰就出在这个决定了用户最终体验的“解码”阶段。</strong></p> <blockquote> <p><strong>一个反直觉的现象：为什么强大的NPU/GPU，在解码时反而可能不如CPU？</strong></p> <p>很多技术测试都发现一个奇怪的现象：在手机上运行LLM时，<strong>Prefill阶段用NPU/GPU处理，速度极快。但到了最关键的Decoding阶段，它们的生成速度经常还不如直接用CPU。</strong></p> <p><strong>原因正是内存带宽的制约，我们可以用一个简单的公式来揭示真相：</strong></p> \[\text{理论解码速度（Tokens/s）} \approx \frac{\text{内存带宽（GB/s）}}{\text{模型单次推理数据量（GB）}}\] <p>这个公式告诉我们，解码速度的上限，完全取决于“数据搬运”的效率。<strong>每生成一个字（Token），都需要将重达数GB的模型参数从系统内存（RAM）完整地“搬运”一遍。</strong></p> <ul> <li><strong>NPU/GPU的“窘境”</strong>：它们算力再强，也得等数据从内存中漫长地传输过来。大部分时间都在“等米下锅”，强大的算力被闲置，自然快不起来。</li> <li><strong>CPU的“智慧”</strong>：CPU核心虽少，但它自带的<strong>巨大高速缓存（Cache）</strong> 相当于一个“厨房里的冰箱”。它可以把最常用的参数暂存起来随用随取，避免了频繁访问“几公里外的大仓库（RAM）”，因此在解码这种“计算量小、数据访问频繁”的任务上反而效率更高。</li> </ul> <p><strong>让我们量化一下这个瓶颈：</strong> 假设一款手机的内存带宽是<strong>64 GB/s</strong>，运行一个需要加载<strong>4GB</strong>参数的7B模型。理论上，它每秒最多只能生成 <code class="language-plaintext highlighter-rouge">64 ÷ 4 = 16</code> 个Token。一个汉字约等于2个Token，这意味着手机每秒最多能生成8个汉字——这个速度已经低于人类的平均阅读速度，用户会明显感觉到“卡顿”。如果想让体验流畅（如32 Tokens/s），内存带宽就需要翻倍到<strong>128 GB/s</strong>。</p> <p><strong>至此，结论已经非常清晰：</strong> 对于决定LLM交互流畅度的解码环节，数据传输效率远比纯粹的计算能力更重要。这就是AI社区将解码称为“内存密集型”（Memory-bound）任务的根本原因。</p> </blockquote> <hr/> <h2 id="拆解带宽瓶颈天花板与破局之路"><strong>拆解带宽瓶颈：“天花板”与“破局之路”</strong></h2> <p>要理解手机AI性能的真正瓶颈，我们必须深入到内存带宽的“引擎室”，看看它由什么构成，又被什么所限制。</p> <p>内存带宽的计算公式看似简单，由两个核心参数相乘决定：</p> \[\text{内存带宽（GB/s）} = \frac{\text{数据传输速率（MT/s）} \times \text{总线宽度（bits）}}{8}\] <ul> <li><strong>数据传输速率 (Data Rate):</strong> 这是我们最常听到的参数，比如LPDDR5X-8533。它就像是数据高速公路上的<strong>“最高限速”</strong>，是各大厂商目前提升带宽最直接、竞争最激烈的战场。</li> <li><strong>内存总线宽度 (Bus Width):</strong> 这可以理解为高速公路的<strong>“车道数量”</strong>。目前，旗舰手机芯片几乎无一例外地被“焊死”在了<strong>64-bit</strong>这个宽度上。</li> </ul> <p>问题来了：既然车道越多，数据并行能力越强，为什么手机芯片不像PC显卡（如RTX 4090拥有384-bit总线）那样，把“路”修得更宽呢？</p> <p>答案是，在手机内部寸土寸金的空间里，拓宽总线这条“物理之路”几乎已经走到了尽头。它面临着三座难以逾越的大山，也就是我们所说的<strong>“物理天花板”</strong>：</p> <ol> <li><strong>空间限制：</strong> 每一bit总线都需要一条独立的、像头发丝一样精密的PCB走线。将64-bit翻倍到128-bit，意味着布线复杂度和面积需求的指数级增长，这在手机主板上是不可想象的。</li> <li><strong>信号干扰：</strong> “车道”越多、越密，彼此间的“信号串扰”就越严重，数据很容易出错。为了保证信号同步，所有走线的长度必须像军人列队般精确对齐，这对设计和制造工艺是巨大的挑战。</li> <li><strong>功耗失控：</strong> 每一条“车道”都需要消耗电能来驱动数据传输。粗暴地加宽总线会直接导致功耗和发热激增，这对于依靠电池续命的手机来说是致命的。</li> </ol> <p>既然横向的“扩路”已触及天花板，聪明的芯片工程师们便另辟蹊径，开启了三条通往未来的<strong>“破局之路”</strong>。</p> <h4 id="破局之路一纵向提速--不断刷新的数据率-lpddr6及后续"><strong>破局之路一：纵向提速 —— 不断刷新的数据率 (LPDDR6及后续)</strong></h4> <p>这是最经典、最确定的演进路线：既然路宽不了，那就让车速更快。从LPDDR4X的4266MT/s，到LPDDR5X的8533MT/s，再到即将登场的<strong>LPDDR6</strong>（起步速率可达<strong>12800MT/s</strong>），这条路一直在坚定地向前延伸。它虽然可靠，但边际效应也开始显现，更高的速率对功耗和信号控制的要求也越来越苛刻。</p> <h4 id="破局之路二空间折叠--用3d封装拓宽总线-先进封装技术"><strong>破局之路二：空间折叠 —— 用3D封装拓宽总线 (先进封装技术)</strong></h4> <p>既然平面上修路已无可能，那就“向上”发展，搭建“立交桥”。<strong>先进的3D封装技术</strong>，允许将内存芯片（DRAM）直接堆叠在SoC芯片的上方，通过成千上万条超短的垂直通道连接。这就像把两个城市直接叠在一起，交通距离无限缩短。</p> <ul> <li><strong>核心优势：</strong> 极短的距离意味着极低的功耗和信号干扰，这使得<strong>在垂直方向上实现128-bit甚至更宽的总线</strong>成为可能。PC领域的HBM（高带宽内存）正是该技术的体现，而移动端正在全力探索如何将其小型化、低功耗化，这被视为未来颠覆带宽格局的关键。</li> </ul> <h4 id="破局之路三修建前置仓--用系统级缓存抄近道-slc"><strong>破局之路三：修建“前置仓” —— 用系统级缓存抄近道 (SLC)</strong></h4> <p>如果每次都去几公里外的大仓库（系统内存）取货太慢，那就在车间旁边建一个“前置仓”（缓存）。<strong>扩大和优化系统级缓存（System Level Cache, SLC）</strong>正是这一思路的体现。这块大容量高速缓存由CPU、GPU、NPU等所有计算核心共享。</p> <ul> <li><strong>工作原理：</strong> 在运行LLM时，芯片可以将最核心的模型参数（如注意力权重）提前加载到SLC这个“前置仓”里。当NPU需要时，直接从“隔壁”高速调取，<strong>彻底避免了访问外部系统内存的漫长等待</strong>。苹果和联发科都是该策略的忠实拥趸，通过不断增大片上缓存，在实际AI应用中获得了远超理论带宽的能效表现。</li> </ul> <hr/> <h2 id="2025年旗舰芯片内存带宽大比拼谁的水管最粗"><strong>2025年旗舰芯片内存带宽大比拼：谁的“水管”最粗？</strong></h2> <p>进入2025年，各家旗舰芯片在内存带宽上展开激烈角逐。基于现有信息和行业预测，我们整理了以下旗舰芯片的内存系统分析：</p> <table> <thead> <tr> <th style="text-align: left">芯片型号</th> <th style="text-align: left">内存标准</th> <th style="text-align: left">理论峰值速率</th> <th style="text-align: left">理论峰值带宽</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>联发科 天玑9400/9500</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">10667 Mbps</td> <td style="text-align: left"><strong>85.3 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>高通 骁龙 8 Elite</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">9600 Mbps</td> <td style="text-align: left"><strong>76.8 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>三星 Exynos 2500</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">9600 Mbps</td> <td style="text-align: left"><strong>76.8 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>高通 骁龙 8 Elite Gen 5</strong></td> <td style="text-align: left">LPDDR6</td> <td style="text-align: left"><strong>最高14400 Mbps</strong></td> <td style="text-align: left"><strong>最高115.2 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>苹果 A19 Pro</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">待定</td> <td style="text-align: left">待定</td> </tr> </tbody> </table> <ul> <li> <p><strong>联发科：激进的LPDDR5X优化者</strong> 联发科一直致力于在LPDDR5X标准下压榨极限。<strong>天玑9400/9500</strong>系列率先将内存速率推高到惊人的<strong>10667Mbps</strong>，一举刷新行业记录，并以<strong>85.3GB/s</strong>的理论带宽领跑。同时，它还通过增大片上缓存（L3/SLC）来减少对主内存的访问，进一步提升了数据处理效率。</p> </li> <li> <p><strong>高通：LPDDR6的先行者</strong> 高通在<strong>骁龙8 Elite</strong>上采用了<strong>9600Mbps</strong>的LPDDR5X，与三星Exynos 2500并驾齐驱。但更引人注目的是，它很可能成为首个在<strong>骁龙8 Elite Gen 5</strong>上商用<strong>LPDDR6</strong>的手机芯片，这将使带宽一举突破100GB/s大关，达到<strong>115.2GB/s</strong>，为未来的端侧大模型奠定坚实基础。</p> </li> <li> <p><strong>苹果：从容的内存容量策略</strong> 尽管苹果的A系列芯片在内存带宽数据上相对封闭，但最新的A19 Pro芯片却做出了一个罕见而关键的升级：将<strong>iPhone 17 Pro</strong>的运行内存从8GB提升至<strong>12GB</strong>。这背后正是为了适配“Apple Intelligence”等AI功能，通过更大的容量来承载更复杂的模型和数据，间接缓解了带宽压力。这也可以看作是对‘破局之路三’中增大缓存、减少主内存访问策略的一种宏观体现。</p> </li> </ul> <hr/> <h2 id="总结"><strong>总结</strong></h2> <p>内存带宽无疑是当前制约手机LLM性能的头号瓶颈，其重要性甚至超过了单纯的算力堆砌。在这场竞赛中，联发科凭借对LPDDR5X的极致优化暂时领跑，而高通则可能通过率先拥抱LPDDR6标准，在未来实现颠覆性突破。</p> <p>然而，决定最终AI体验的，将是一个完整的“供水系统”。无论是继续提升内存速率，还是探索先进封装和更大系统缓存等未来技术，其最终目的都是为了构建一个高效的数据流。除了加粗“水管”（内存带宽），还需要强大的“水泵”（NPU/GPU算力）、足够大的“蓄水池”（内存容量），以及智能的“用水策略”（算法与软件优化）。</p> <p>只有当硬件、软件和AI模型三方协同进化，我们才能真正迎来那个理想的未来：在手机上与强大、流畅、懂你心意的AI进行实时互动。</p>]]></content><author><name></name></author><category term="llm"/><category term="device"/><summary type="html"><![CDATA[在AI大模型席卷一切的今天，我们都期待手机成为真正的“口袋里的AI大脑”。但你有没有感觉，手机上的AI助手总是慢半拍？无论是想让它快速总结一篇长文，还是在图片编辑时使用“AI消除”功能，那种等待的延迟感，总在提醒我们“理想与现实的差距”。]]></summary></entry><entry><title type="html">CoreML踩坑记：慎用Conv1D</title><link href="https://zhaode.wang/blog/2025/coreml-conv1d/" rel="alternate" type="text/html" title="CoreML踩坑记：慎用Conv1D"/><published>2025-08-18T00:00:00+00:00</published><updated>2025-08-18T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/coreml-conv1d</id><content type="html" xml:base="https://zhaode.wang/blog/2025/coreml-conv1d/"><![CDATA[<h4 id="背景">背景</h4> <p>最近在给MNN写CoreML后端，优化<code class="language-plaintext highlighter-rouge">Qwen2.5-Omni</code>的性能。在测试<code class="language-plaintext highlighter-rouge">BigVGAN</code>模型的时候发现结果对不齐，逐层调试后发现错误出现在<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>（一维转置卷积）算子上。</p> <p>因为MNN的后端是在通过线转换计算图的方式得到CoreML模型的，而且这个构图的过程都是我自己从头实现的，所以一般出错都是算子构造问题。但此类问题会出现在同类型的第一个算子后，而这次错误却是从<code class="language-plaintext highlighter-rouge">ups</code>的第二层开始出现的。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ups</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose1d</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">layer_idx</span><span class="p">),</span>
                    <span class="n">config</span><span class="p">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">layer_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span>
                    <span class="n">kernel_size</span><span class="p">,</span>
                    <span class="n">stride</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">stride</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">upsample_rates</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">upsample_kernel_sizes</span><span class="p">))</span>
    <span class="p">]</span>
</code></pre></div></div> <h4 id="排查过程">排查过程</h4> <p>首先怀疑构图出错。于是我用<code class="language-plaintext highlighter-rouge">coremltools</code>把一个PyTorch的<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>模型转成<code class="language-plaintext highlighter-rouge">.mlmodelc</code>，把<code class="language-plaintext highlighter-rouge">model.mil</code>抠出来当“标准答案”，跟我自己写的图定义对比发现一模一样。</p> <p>这就奇怪了，图的定义没问题，那问题出在哪？</p> <p>没有思路，就将出错的算子单独转换成一个模型进行深入测试，发现了一个奇怪的现象，当我把<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>改成<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>后，结果就对了。</p> <p>理论上他们应该是等价的，但结果却不一样。为了进一步测试，我尝试把<strong>把Bias参数去掉，全设成0</strong>进行对比。</p> <p>结果它俩输出就一样了！这下基本可以确定，问题就出在CoreML执行1D转置卷积时，处理Bias的逻辑有Bug。从错误的输出现象看，Bias没有被正确地加到整个输出通道上，只影响了每个通道的头几个数。</p> <p>然后好奇第一层也有bias为何没问题呢？就把第一层的<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>提取成独立模型，同时改造出对应的<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>版本，结果它俩输出一致。看来这个问题是与算子的尺寸有关的。</p> <p>到这里我估计就不是构图问题了，而是CoreML内部的问题。</p> <h4 id="最终验证">最终验证</h4> <p>为了验证是否是CoreML的问题，我写了个Python脚本来做测试。脚本里建了两个一模一样的PyTorch模型，一个用<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>，一个用<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>，然后用<code class="language-plaintext highlighter-rouge">coremltools</code>转成CoreML模型，对比输出。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">coremltools</span> <span class="k">as</span> <span class="n">ct</span>

<span class="n">COMPUTE_UNIT</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="n">ComputeUnit</span><span class="p">.</span><span class="n">ALL</span>
<span class="c1"># COMPUTE_UNIT = ct.ComputeUnit.CPU_ONLY
# COMPUTE_UNIT = ct.ComputeUnit.CPU_AND_GPU
</span>
<span class="k">class</span> <span class="nc">Deconv1DModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">deconv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose1d</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">deconv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Deconv2DModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">deconv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">deconv</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">convert_and_save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>
    <span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span>
        <span class="n">traced_model</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ct</span><span class="p">.</span><span class="nc">TensorType</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)],</span>
        <span class="n">convert_to</span><span class="o">=</span><span class="sh">"</span><span class="s">mlprogram</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">iOS18</span><span class="p">,</span>
        <span class="n">compute_units</span><span class="o">=</span><span class="n">COMPUTE_UNIT</span>
    <span class="p">)</span>
    <span class="n">model_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">./</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">.mlpackage</span><span class="sh">'</span>
    <span class="n">mlmodel</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model_path</span>

<span class="k">def</span> <span class="nf">predict_with_coreml</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">MLModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">compute_units</span><span class="o">=</span><span class="n">COMPUTE_UNIT</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">input_tensor</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)}</span>
    <span class="n">output_dict</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">output_key</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">output_dict</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">output_key</span><span class="p">]</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">model_1d</span> <span class="o">=</span> <span class="nc">Deconv1DModel</span><span class="p">()</span>
    <span class="n">model_2d</span> <span class="o">=</span> <span class="nc">Deconv2DModel</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="mi">600</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">model_2d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model_1d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">model_2d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model_1d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span>
    <span class="n">model_1d</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">model_2d</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">torch_output_1d</span> <span class="o">=</span> <span class="nf">model_1d</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
        <span class="n">torch_output_2d</span> <span class="o">=</span> <span class="nf">model_2d</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
    <span class="n">are_torch_outputs_close</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">torch_output_1d</span><span class="p">,</span> <span class="n">torch_output_2d</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PyTorch中1D和2D模型的输出是否一致? -&gt; </span><span class="si">{</span><span class="n">are_torch_outputs_close</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">are_torch_outputs_close</span><span class="p">,</span> <span class="sh">"</span><span class="s">错误：PyTorch模型不等价，测试无法继续！</span><span class="sh">"</span>

    <span class="c1"># --- CoreML转换与推理 ---
</span>    <span class="n">model_1d_path</span> <span class="o">=</span> <span class="nf">convert_and_save</span><span class="p">(</span><span class="n">model_1d</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="sh">"</span><span class="s">deconv1d_model_specific_data</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model_2d_path</span> <span class="o">=</span> <span class="nf">convert_and_save</span><span class="p">(</span><span class="n">model_2d</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="sh">"</span><span class="s">deconv2d_model_specific_data</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">coreml_output_1d</span> <span class="o">=</span> <span class="nf">predict_with_coreml</span><span class="p">(</span><span class="n">model_1d_path</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span>
    <span class="n">coreml_output_2d</span> <span class="o">=</span> <span class="nf">predict_with_coreml</span><span class="p">(</span><span class="n">model_2d_path</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">conv1d output: </span><span class="si">{</span><span class="n">coreml_output_1d</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">conv2d output: </span><span class="si">{</span><span class="n">coreml_output_2d</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">are_coreml_outputs_close</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">coreml_output_1d</span><span class="p">,</span> <span class="n">coreml_output_2d</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">max_abs_diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">coreml_output_1d</span> <span class="o">-</span> <span class="n">coreml_output_2d</span><span class="p">).</span><span class="nf">max</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">CoreML中1D和2D模型的输出是否一致? -&gt; </span><span class="si">{</span><span class="n">are_coreml_outputs_close</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">两个输出之间的最大绝对差值为: </span><span class="si">{</span><span class="n">max_abs_diff</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>通过切换<code class="language-plaintext highlighter-rouge">coremltools</code>的<code class="language-plaintext highlighter-rouge">compute_units</code>参数，我得到了决定性的证据：</p> <ul> <li><code class="language-plaintext highlighter-rouge">compute_units = ct.ComputeUnit.CPU_ONLY</code>：<strong>结果正确</strong>。</li> <li><code class="language-plaintext highlighter-rouge">compute_units = ct.ComputeUnit.CPU_AND_GPU</code>：<strong>结果正确</strong>。</li> <li><code class="language-plaintext highlighter-rouge">compute_units = ct.ComputeUnit.ALL</code>：<strong>结果错误</strong>！</li> </ul> <p><code class="language-plaintext highlighter-rouge">ALL</code>模式和<code class="language-plaintext highlighter-rouge">CPU_AND_GPU</code>模式唯一的区别就是前者会启用ANE（苹果的神经网络引擎）。这就说明，<strong>Bug的根源在于CoreML在ANE上的具体实现</strong>。只要计算任务被分配到ANE上，这个特定尺寸的<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>的Bias加法就会出错。</p> <h4 id="解决方案">解决方案</h4> <p>既然定位了Bug，解决方案就有了。</p> <ol> <li><strong>方案A：把Bias加法拆出来</strong>。先算一个不带Bias的<code class="language-plaintext highlighter-rouge">conv_transpose</code>，再手动加一个<code class="language-plaintext highlighter-rouge">add</code>算子。这个方法能解决问题，但多了一步操作，内存要多倒腾一次，可能会影响性能。</li> <li><strong>方案B：把1D伪装成2D</strong>。在1D算子前后，分别插入<code class="language-plaintext highlighter-rouge">expand_dims</code>和<code class="language-plaintext highlighter-rouge">squeeze</code>算子，把数据变成4D，然后调用我们已经验证过没问题的<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>来计算。</li> </ol> <p>考虑到性能，我最终选择了<strong>方案B</strong>。一个融合算子，内存读写一次，效率高。虽然在MNN的图转换逻辑里要多写几行代码，但这能让CoreML在底层执行一个高效的融合算子。这个维度变换的逻辑被封装在算子转换的内部，对整个计算图的其他部分是透明的，不会影响其他算子的实现。</p> <h4 id="总结">总结</h4> <p>这次踩坑经历耗费了不少时间，总结下来有几点：</p> <ol> <li><strong>CoreML的<code class="language-plaintext highlighter-rouge">Conv1d</code>算子在ANE上可能存在隐蔽的Bug</strong>，当你的模型里有这个算子并且结果不对时，可以优先排查它，并尽量使用<code class="language-plaintext highlighter-rouge">Conv2d</code>。</li> <li><strong>验证问题时，一定要切换<code class="language-plaintext highlighter-rouge">compute_units</code></strong>，对比CPU、GPU、ANE的行为差异，这能帮你快速定位问题是不是硬件相关的。</li> </ol> <p>希望这个排查过程能给遇到类似问题的朋友一点启发。</p>]]></content><author><name></name></author><category term="llm"/><category term="coreml"/><category term="ios"/><summary type="html"><![CDATA[背景]]></summary></entry><entry><title type="html">深入 gpt-oss-20b 架构：MNN 移动端性能实践</title><link href="https://zhaode.wang/blog/2025/gpt-oss/" rel="alternate" type="text/html" title="深入 gpt-oss-20b 架构：MNN 移动端性能实践"/><published>2025-08-08T00:00:00+00:00</published><updated>2025-08-08T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/gpt-oss</id><content type="html" xml:base="https://zhaode.wang/blog/2025/gpt-oss/"><![CDATA[<p>在 OpenAI 开源 gpt-oss-20b 模型之后，MNN 迅速完成了对这个 20B 参数大语言模型的高效适配，成功将其带到移动端。实测结果表明，该模型不仅知识渊博、推理能力不凡，并且在手机上也能实现令人印象深刻的运行速度。</p> <h2 id="智力测试gpt-oss-20b-vs-qwen3-30b-a3b-thinking-2507">智力测试：gpt-oss-20b vs. Qwen3-30B-A3B-Thinking-2507</h2> <p>为了深入探究 gpt-oss-20b 的能力边界，我们将其与同样强大的 Qwen3-30B-A3B-Thinking-2507 模型进行了一场横向对比评测。</p> <h3 id="测试环境说明">测试环境说明</h3> <p>两款模型均使用 <code class="language-plaintext highlighter-rouge">MNN</code> 的 <strong>4-bit HQQ</strong> 方法进行量化，并在 <code class="language-plaintext highlighter-rouge">Mac M3 Pro</code> 上通过 <code class="language-plaintext highlighter-rouge">MNN</code> 框架进行测试。我们采用了各模型官方推荐的采样配置，并使用 <code class="language-plaintext highlighter-rouge">Gemini 1.5 Pro</code> 生成了一系列涵盖数学、物理、逻辑和常识推理的测试问题。</p> <p>测试题目如下：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.11 和 9.9 哪个大?
有一架飞机，停在一个和跑道一样长、一样宽的巨型跑步机上。跑步机的系统被设定为：无论飞机机轮的速度是多少，它都会以完全相同的速度向相反方向&gt;运动，从而抵消机轮的转动。请问，这架飞机能否成功起飞？
你有一个完全密闭、内部是完美真空、并且100%不透光的坚固箱子。你在箱子里的天平上称量这个箱子。然后，你打开了箱子里的一支手电筒。请问，当手&gt;电筒亮着时，天平显示的重量会比之前更重、更轻，还是完全一样？
你有两根不均匀的绳子，每根绳子从一头烧到另一头都正好需要1个小时。所谓不均匀，是指绳子可能前半段10分钟就烧完了，后半段却烧了50分钟。现在，
只用这两根绳子和一个打火机，你如何精确地计时45分钟？
I want to cool down a sealed room, so I leave my refrigerator door open inside it. Will the room's overall temperature eventually rise, lower, or stay the same?
If all the people on Earth gathered in one place and all jumped at the exact same time, what would happen to the Earth? Would its orbit be altered?
</code></pre></div></div> <h3 id="-测试结果一览">📊 测试结果一览</h3> <table> <thead> <tr> <th style="text-align: left">测试项目</th> <th style="text-align: left">gpt-oss-20b</th> <th style="text-align: left">Qwen3-30B-A3B-Thinking-2507</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">数值比较 (9.11 vs 9.9)</td> <td style="text-align: left">✅ <strong>正确</strong> (认为9.9更大)</td> <td style="text-align: left">✅ <strong>正确</strong> (认为9.9更大)</td> </tr> <tr> <td style="text-align: left">物理推理 (跑步机与飞机)</td> <td style="text-align: left">❌ <strong>错误</strong> (认为不会起飞)</td> <td style="text-align: left">✅ <strong>正确</strong> (能够起飞)</td> </tr> <tr> <td style="text-align: left">高级物理 (密闭箱中的光)</td> <td style="text-align: left">✅ <strong>正确</strong> (E=mc²)</td> <td style="text-align: left">❌ <strong>错误</strong> (忽略质能等价)</td> </tr> <tr> <td style="text-align: left">逻辑推理 (烧绳计时45分钟)</td> <td style="text-align: left">✅ <strong>正确</strong> (标准解法)</td> <td style="text-align: left">✅ <strong>正确</strong> (标准解法)</td> </tr> <tr> <td style="text-align: left">热力学 (敞开的冰箱)</td> <td style="text-align: left">✅ <strong>正确</strong> (热力学第二定律)</td> <td style="text-align: left">✅ <strong>正确</strong> (直觉误判)</td> </tr> <tr> <td style="text-align: left">宏观力学 (全球人同时跳)</td> <td style="text-align: left">✅ <strong>正确</strong> (量化分析)</td> <td style="text-align: left">✅ <strong>正确</strong> (定性分析)</td> </tr> </tbody> </table> <p><strong>详细测试结果</strong></p> <ul> <li><a href="https://github.com/wangzhaode/llm-lab/blob/main/gpt-oss/gpt-oss-20b-MNN-answer.txt">gpt-oss-20b-MNN-answer.txt</a></li> <li><a href="https://github.com/wangzhaode/llm-lab/blob/main/gpt-oss/Qwen3-30B-A3B-Thinking-2507-MNN-answer.txt">Qwen3-30B-A3B-Thinking-2507-MNN-answer.txt</a></li> </ul> <h3 id="-关键洞察">🎯 关键洞察</h3> <p>通过这场对决，两个模型展现出截然不同的“智能画像”：</p> <ul> <li> <p><strong>gpt-oss-20b：知识渊博的“检索型专家”</strong> 它在需要精确、深入专业知识的领域表现卓越。例如，在“密闭箱中的光”问题上，它能准确调用质能等价性（E=mc²）这一高级物理知识，给出理论严谨的回答。这表明其庞大的参数量有效地存储了丰富的世界知识。</p> </li> <li> <p><strong>Qwen3-30B-A3B-Thinking-2507：逻辑缜密的“推理型思考者”</strong> 它在需要多步推理、理解相对关系和进行科学计算的场景中更胜一筹。例如，在“跑步机上的飞机”，它能摆脱直觉误导，运用正确的物理原理和量化分析来解决问题。</p> </li> </ul> <p><strong>结论</strong>：gpt-oss-20b 强大的知识储备使其在“是什么”的问题上表现突出，而 Qwen3-30B-A3B-Thinking-2507 则在“为什么”和“如何做”的问题上展现了更强的逻辑推理能力。</p> <h2 id="架构深度解析">架构深度解析</h2> <p>gpt-oss-20b 的独特性能源于其精妙的架构设计。</p> <p>首先，它采用了 <strong>MoE (混合专家)</strong> 架构。模型每层包含32个专家网络，但每个 Token 的前向传播仅激活其中4个（约13%的激活率）。这意味着，尽管其总参数量高达21B，但单次推理的计算量仅相当于一个 <strong>3.6B</strong> 的稠密模型，极大地降低了硬件门槛。同时，专家网络部分采用了 <strong>MXFP4</strong> 量化，将模型尺寸压缩至原来的四分之一。</p> <p>为了应对长文本处理带来的计算挑战，gpt-oss 采用了<strong>交替注意力机制</strong>：一半的注意力层使用全上下文（Full Attention），另一半则使用仅关注前128个 Token 的滑动窗口注意力（Sliding Window Attention）。这种设计将一半注意力计算的复杂度从 O(n²) 优化到了 O(n×w)，是其能够高效处理长序列的关键。</p> <p>除了这些宏观设计，gpt-oss-20b 的 Attention 结构中还蕴含着一些非常有趣的设计。</p> <h3 id="1-attention-sinks滑动窗口的稳定锚">1. Attention Sinks：滑动窗口的“稳定锚”</h3> <p>这是一个在其他模型中非常罕见的设计。我们知道，标准的注意力机制容易在序列开头形成“注意力汇点 (Attention Sinks)”，这些初始 Token 会像锚一样稳定全局注意力。然而，当使用滑动窗口时，这些头部的 Token 会被逐渐移出窗口，导致注意力分布变得不稳定。</p> <p>gpt-oss 通过在序列末尾显式地添加一个可学习的 <code class="language-plaintext highlighter-rouge">sinks</code> 向量来解决这个问题。这个 <code class="language-plaintext highlighter-rouge">sinks</code> 作为一个固定的“锚点”，确保即使在窗口滑动时，注意力也有一个稳定的归宿，从而维持了模型的推理连贯性和效率。</p> <h3 id="2-注意力心跳一项揭示架构特点的发现">2. “注意力心跳”：一项揭示架构特点的发现</h3> <p>gpt-oss-20b 依然保留了 Attention Bias，而许多新模型（如Qwen2系列）已转向 <code class="language-plaintext highlighter-rouge">LayerNorm</code>。通过分析我们发现，gpt-oss 的 Bias 设计得非常克制（仅 Q-Proj 有微小 Bias，K-Proj 全为0），避免了 Qwen2 系列中因 Bias 过大导致的 <code class="language-plaintext highlighter-rouge">Q@K^T</code> 溢出风险。</p> <p>然而，当我们进一步探究其 <code class="language-plaintext highlighter-rouge">Q@K^T</code> 的原始 Logits 分布时，一个极其规律且引人注目的现象浮出水面。</p> <p><img src="/assets/img/gpt-oss/gpt_oss_20b.png" alt=""/></p> <p>如图所示，模型的注意力表现出一种完美交替的“高峰-低谷”模式：</p> <ul> <li><strong>奇数层 (sliding_attention)</strong>，构成了图中所有红色的“<strong>山峰</strong>”。这些层的 <code class="language-plaintext highlighter-rouge">Q@K^T</code> Logits 值异常之高，动辄达到数百甚至上千。</li> <li><strong>偶数层 (full_attention)</strong>，则构成了图中所有蓝色的“<strong>谷底</strong>”。这些层的 <code class="language-plaintext highlighter-rouge">Q@K^T</code> Logits 值则要低得多，通常只有前者的十分之一甚至更少。</li> </ul> <p>这个巨大的、数量级上的差异，并非随机伪影或由偏置项驱动，而是 gpt-oss-20b 在架构层面的一种<strong>深刻的功能特化 (Functional Specialization)</strong>。</p> <p><strong>原因解析：两种注意力层的“能量”分工</strong></p> <p>这个“注意力心跳”的根本原因在于，模型在训练中为两种注意力层学会了生成<strong>能量（即向量范数）完全不同的 Q 和 K 向量</strong>。</p> <ol> <li><strong>Sliding Attention 层：“高增益”的局部信号放大器</strong> <ul> <li><strong>定位</strong>：这些层是“<strong>局部专家</strong>”，视野狭窄。</li> <li><strong>任务</strong>：由于看不见全局，它们必须在有限信息中，对最关键的局部模式做出<strong>毫不含糊、决定性</strong>的判断。</li> <li><strong>实现</strong>：模型通过生成<strong>高范数（高能量）的 Q/K 向量</strong>来实现这一点。当两个高能量向量对齐时，它们的点积会变得非常巨大，就像一个高增益放大器，在进入Softmax前就几乎“内定”了胜者，确保最强的局部信号能被稳定传递。</li> </ul> </li> <li><strong>Full Attention 层：“低增益”的全局信息协调者</strong> <ul> <li><strong>定位</strong>：这些层是“<strong>全局战略家</strong>”，视野广阔。</li> <li><strong>任务</strong>：它们的职责是审慎地评估和协调长距离依赖关系，这需要的是<strong>灵活性</strong>，而非武断。</li> <li><strong>实现</strong>：模型通过生成<strong>范数较小（低能量）的 Q/K 向量</strong>，使得在Softmax前，各个Token的竞争更温和公平。这让模型可以同时融合多个来自不同位置的弱信号，从而发现复杂的全局模式。</li> </ul> </li> </ol> <h3 id="3-短胖-vs-瘦长的架构哲学">3. “短胖” vs “瘦长”的架构哲学</h3> <p>最后，模型间的宏观架构差异也值得玩味：</p> <ul> <li><strong>gpt-oss-20b</strong>: 24层架构，相对“<strong>短胖</strong>”。</li> <li><strong>Qwen系列</strong>: 通常为48层甚至更多，相对“<strong>瘦长</strong>”。</li> </ul> <p>这种差异或许能解释它们的能力倾向：gpt-oss-20b“短胖”的结构让每层拥有更高的参数密度，可能更利于知识的存储和检索；而Qwen“瘦长”的结构通过更多层级的深度非线性变换，可能在逻辑推理和多步思考上更具优势。这也与我们在“智力测试”中观察到的“知识检索型”vs“情境推理型”的差异不谋而合。</p> <h2 id="mnn-适配与性能表现">MNN 适配与性能表现</h2> <p>为了将 gpt-oss-20b 的强大能力带到端侧，MNN 框架针对其独特架构进行了深度适配和优化。</p> <ul> <li><strong>MoE 适配</strong>：为简化导出流程，MNN 将 gpt-oss 的原生 MoE 实现转换为与 Qwen2-MoE 兼容的格式。</li> <li><strong>注意力优化</strong>：初期通过 <code class="language-plaintext highlighter-rouge">attention_mask</code> 实现滑动窗口，未来将持续优化以实现更高的计算效率。</li> </ul> <h3 id="实测性能">实测性能</h3> <table> <thead> <tr> <th style="text-align: left">平台</th> <th style="text-align: left">Prefill 速度</th> <th style="text-align: left">Decode 速度</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>OnePlus (骁龙8 Gen3)</strong></td> <td style="text-align: left">13.68 tokens/s</td> <td style="text-align: left">11.35 tokens/s</td> </tr> <tr> <td style="text-align: left"><strong>Mac M3 Pro</strong></td> <td style="text-align: left">62.72 tokens/s</td> <td style="text-align: left">23.98 tokens/s</td> </tr> </tbody> </table> <p><img src="/assets/img/gpt-oss/gptoss.gif" alt=""/></p> <p><em>移动端运行截图显示了gpt-oss-20b在手机上的流畅表现</em></p> <h3 id="模型下载与快速部署">模型下载与快速部署</h3> <p>下载模型文件后，即可通过 MNN 的标准推理接口轻松调用，移动端可以使用 MNN Chat APP快速体验。</p> <ul> <li><strong>ModelScope</strong>: <a href="https://modelscope.cn/models/MNN/gpt-oss-20b-MNN">gpt-oss-20b-MNN</a></li> <li><strong>HuggingFace</strong>: <a href="https://huggingface.co/MNN/gpt-oss-20b-MNN">gpt-oss-20b-MNN</a></li> <li><strong>MNN Chat APP</strong>: <a href="https://meta.alicdn.com/data/mnn/mnn_chat_0_7_0.apk">apk</a></li> </ul> <h2 id="结语当未来照进现实">结语：当未来照进现实</h2> <p>完成对 gpt-oss-20b 的所有测试和分析后，我心中涌起一阵强烈的感慨。AI 的发展速度如此迅猛，以至于那个两年前初次让我感到震撼、认为遥不可及的 GPT，如今其同等级别的能力，已经安然运行在我的掌心之中。</p> <p>从云端的庞然大物，到手机里的高效智能，这不仅是参数和代码的迁移，更是技术民主化的又一次伟大实践。这让我无比振奋，也让我对未来充满期待——端侧的 AI 能力必将日益强大，一个更加智能、更加个性化的世界正在我们眼前展开。</p>]]></content><author><name></name></author><category term="llm"/><category term="mnn"/><summary type="html"><![CDATA[在 OpenAI 开源 gpt-oss-20b 模型之后，MNN 迅速完成了对这个 20B 参数大语言模型的高效适配，成功将其带到移动端。实测结果表明，该模型不仅知识渊博、推理能力不凡，并且在手机上也能实现令人印象深刻的运行速度。]]></summary></entry><entry><title type="html">混元端侧模型分析</title><link href="https://zhaode.wang/blog/2025/Hunyuan/" rel="alternate" type="text/html" title="混元端侧模型分析"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/Hunyuan</id><content type="html" xml:base="https://zhaode.wang/blog/2025/Hunyuan/"><![CDATA[<p>模型之战的下半场，烽火已经烧到了端侧。继阿里Qwen之后，腾讯混元也学着一次性发了覆盖多种大小的端侧模型。这种直接对标的打法，让这场“国产内战”变得非常有看点。</p> <p>我一直比较关注端侧推理，这次正好把两个大厂的同级模型拉出来，硬碰硬地比一下。不扯别的，就看重合评测集里的硬核数据，看看指令微调之后，到底谁才是端侧场景的更优选。</p> <h3 id="性能对比"><strong>性能对比</strong></h3> <p>评测集就选这六个，都是硬骨头，覆盖了数学、科学、代码、指令遵循和工具调用这些核心能力：</p> <ul> <li><strong>AIME’24 &amp; AIME’25:</strong> 数学竞赛，纯看逻辑推理。</li> <li><strong>GPQA-Diamond:</strong> 专业科学难题，看知识和推理。</li> <li><strong>LiveCodeBench:</strong> 真实环境代码能力。</li> <li><strong>IF-Eval:</strong> 能不能听懂人话，按复杂指令办事。</li> <li><strong>BFCL v3:</strong> Agent能力，就是用工具干活的能力。</li> </ul> <p><img src="/assets/img/hunyuan/hunyuan_qwen3.png" alt=""/></p> <h4 id="入门级-05b-vs-06b"><strong>入门级 (0.5B vs 0.6B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-0.5B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-0.6B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>17.2</strong></td> <td style="text-align: left">10.7</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>20.0</strong></td> <td style="text-align: left">15.1</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left">23.3</td> <td style="text-align: left"><strong>27.9</strong></td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">11.1</td> <td style="text-align: left"><strong>12.3</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">49.7</td> <td style="text-align: left"><strong>59.2</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>49.8</strong></td> <td style="text-align: left">46.4</td> </tr> </tbody> </table> <h4 id="主流级-18b-vs-17b"><strong>主流级 (1.8B vs 1.7B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-1.8B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-1.7B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>56.7</strong></td> <td style="text-align: left">48.3</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>53.9</strong></td> <td style="text-align: left">36.8</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left"><strong>47.2</strong></td> <td style="text-align: left">40.1</td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">31.5</td> <td style="text-align: left"><strong>33.2</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">67.6</td> <td style="text-align: left"><strong>72.5</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>58.3</strong></td> <td style="text-align: left">56.6</td> </tr> </tbody> </table> <h4 id="甜点级-4b-vs-4b"><strong>甜点级 (4B vs 4B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-4B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-4B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>78.3</strong></td> <td style="text-align: left">73.8</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>66.5</strong></td> <td style="text-align: left">65.6</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left"><strong>61.1</strong></td> <td style="text-align: left">55.9</td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">49.4</td> <td style="text-align: left"><strong>54.5</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">76.6</td> <td style="text-align: left"><strong>81.9</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>67.9</strong></td> <td style="text-align: left">65.9</td> </tr> </tbody> </table> <h4 id="旗舰级-7b-vs-8b"><strong>旗舰级 (7B vs 8B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-7B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-8B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>81.1</strong></td> <td style="text-align: left">76.0</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>75.3</strong></td> <td style="text-align: left">67.3</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left">60.1</td> <td style="text-align: left"><strong>62.0</strong></td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">57.0</td> <td style="text-align: left"><strong>57.5</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">79.3</td> <td style="text-align: left"><strong>85.0</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>70.8</strong></td> <td style="text-align: left">68.1</td> </tr> </tbody> </table> <h4 id="数据分析"><strong>数据分析</strong></h4> <ul> <li> <p><strong>跑分下来，两边的画像非常清晰。</strong> Hunyuan的进化路径非常明确，就是猛点 <strong>“推理”</strong> 和 <strong>“增强的代理能力”</strong> 这两个技能点。从0.5B到7B，它在AIME（数学）和BFCL（工具调用）上几乎一路领先。Qwen3则更像个“六边形战士”，在IF-Eval（指令遵循）和LiveCodeBench（代码）上表现稳定且强大，通用基础能力很扎实。</p> </li> <li> <p><strong>7B vs 8B的对决最有意思。</strong> 参数量更小的Hunyuan-7B，在最难的数学（AIME）上反超了更大的Qwen3-8B，这说明Hunyuan的参数效率在推理任务上非常高。但Qwen3-8B凭借更大的体量，在知识、代码和指令遵循上还是更强，综合实力依然是标杆。</p> </li> </ul> <h4 id="benchmark之外的亮点"><strong>Benchmark之外的亮点</strong></h4> <p>除了跑分，Hunyuan这次还带了两个Qwen没有的“杀手锏”，而这可能才是决定某些应用场景选型的关键。</p> <ol> <li> <p><strong>超长上下文：原生256K vs 32K</strong> 这可能是最“不讲道理”的优势。Hunyuan原生支持<strong>256K</strong>的上下文窗口，<strong>这直接是Qwen3原生32K的8倍</strong>。这意味着什么？处理超长文档、几十页的财报、把一整本书扔进去当知识库…这些场景Qwen3可能就很难搞，但Hunyuan能直接吃下，而且官方宣称在长文本任务上性能稳定。对于想做长文本分析、RAG增强等应用的开发者来说，这吸引力太大了。</p> </li> <li> <p><strong>为Agent而生的设计</strong> 上面的BFCL跑分已经证明了Hunyuan在工具调用上的领先。这不只是“碰巧”考得好，而是腾讯在设计模型时，就奔着让它当一个能干活的<strong>Agent</strong>去的，而不是一个纯粹的聊天机器人。这一点在τ-Bench和C3-Bench等其他Agent评测中也得到验证，说明它的这个优势是系统性的。</p> </li> </ol> <h4 id="总结一下"><strong>总结一下</strong></h4> <p>所以到底怎么选？结论已经非常清晰了：</p> <ul> <li> <p><strong>选Hunyuan，如果你需要一个“长文本分析专家” + “超级助理(Agent)”</strong> 如果你的应用场景需要处理上万甚至十万字以上的文档，或者需要模型作为核心，去调度各种外部API和工具来完成复杂任务，那Hunyuan是好的选择。它在这两个方向上优势太明显了。</p> </li> <li> <p><strong>选Qwen3，如果你想要一个“可靠的全能执行者” + “编程好帮手”</strong> 如果你的需求是构建一个能准确理解用户各种指令的通用聊天助手，或是在代码生成、调试等开发场景中寻求稳定可靠的帮助，那基础更全面、指令遵循能力更强的Qwen3依然是那个不会错的选择。</p> </li> </ul> <h3 id="个人思考混合推理是条好走的路吗"><strong>个人思考：混合推理，是条好走的路吗？</strong></h3> <p>这里有个很有意思的细节。</p> <p>Hunyuan这次也提了“快慢思考”混合推理，这和之前Qwen3的做法很像，想让一个模型满足不同场景下对速度和精度的要求。</p> <p>但耐人寻味的是，Qwen3最新的版本反而把这条路给放弃了，直接把<strong>Thinking</strong>和<strong>Instruct</strong>拆成了两个独立模型。</p> <p>所以，“一体两用”的混合推理，到底是不是一条好走的路？</p> <p>Qwen3这波操作，看起来就像是踩过坑之后做的决定。这其实也印证了我自己做微调和部署时的感觉：这种混合模式听起来很美，但在实际部署、量化和持续优化上，远不如两个权责清晰的独立模型来得直接和高效。一个模型要同时兼顾两个差异巨大的优化目标，往往意味着两边都做不到极致。</p> <p>Hunyuan现在还在坚持这条路，后续的效果和社区反馈会怎么样，值得我们持续观察。</p> <h3 id="压轴好戏mnn支持">压轴好戏：MNN支持</h3> <p>MNN第一时间给出了Hunyuan系列模型的支持，安装MNN Chat可以直接在手机上体验Hunyuan的模型。</p> <p><img src="/assets/img/hunyuan/mnnchat.png" alt=""/></p> <h4 id="app--模型下载链接">APP &amp; 模型下载链接：</h4> <ul> <li><a href="https://play.google.com/store/apps/details?id=com.alibaba.mnnllm.android.release">MNN Chat</a></li> <li><a href="https://modelscope.cn/collections/Hunyuan-MNN-f088ac18d5db4a">ModelScope</a></li> <li><a href="https://huggingface.co/collections/taobao-mnn/hunyuan-mnn-68907bf27f4073975a8a8df6">HuggingFace</a></li> </ul>]]></content><author><name></name></author><category term="llm"/><category term="mnn"/><summary type="html"><![CDATA[模型之战的下半场，烽火已经烧到了端侧。继阿里Qwen之后，腾讯混元也学着一次性发了覆盖多种大小的端侧模型。这种直接对标的打法，让这场“国产内战”变得非常有看点。]]></summary></entry><entry><title type="html">gdb/lldb打印SIMD寄存器的值</title><link href="https://zhaode.wang/blog/2024/dump-register/" rel="alternate" type="text/html" title="gdb/lldb打印SIMD寄存器的值"/><published>2024-08-06T00:00:00+00:00</published><updated>2024-08-06T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2024/dump-register</id><content type="html" xml:base="https://zhaode.wang/blog/2024/dump-register/"><![CDATA[<p>在gdb或者lldb调试时打印SIMD寄存器的值的方法</p> <h2 id="gdb">gdb</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p <span class="nv">$v8</span>.b
p <span class="nv">$v8</span>.h
p <span class="nv">$v8</span>.s
</code></pre></div></div> <h2 id="lldb">lldb</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p <span class="o">(</span>char __attribute__<span class="o">((</span>ext_vector_type<span class="o">(</span>16<span class="o">))))</span> <span class="nv">$v8</span>
p <span class="o">(</span>short __attribute__<span class="o">((</span>ext_vector_type<span class="o">(</span>8<span class="o">))))</span> <span class="nv">$v8</span>
p <span class="o">(</span>float __attribute__<span class="o">((</span>ext_vector_type<span class="o">(</span>4<span class="o">))))</span> <span class="nv">$v8</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="linux"/><summary type="html"><![CDATA[在gdb或者lldb调试时打印SIMD寄存器的值的方法]]></summary></entry><entry><title type="html">使用tee时获取exitcode</title><link href="https://zhaode.wang/blog/2024/exitcode/" rel="alternate" type="text/html" title="使用tee时获取exitcode"/><published>2024-08-06T00:00:00+00:00</published><updated>2024-08-06T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2024/exitcode</id><content type="html" xml:base="https://zhaode.wang/blog/2024/exitcode/"><![CDATA[<p>当使用tee时，程序的exitcode无法正常获取，比如<code class="language-plaintext highlighter-rouge">a.out</code>的exitcode为1，但是使用tee之后得到的为<code class="language-plaintext highlighter-rouge">0</code>。这时可以使用<code class="language-plaintext highlighter-rouge">set -o pipefail</code>，如下：</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./a.out | <span class="nb">tee
echo</span> <span class="nv">$?</span> <span class="c"># 0</span>

<span class="nb">set</span> <span class="nt">-o</span> pipefail
./a.out | <span class="nb">tee
echo</span> <span class="nv">$?</span> <span class="c"># 1</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="linux"/><summary type="html"><![CDATA[当使用tee时，程序的exitcode无法正常获取，比如a.out的exitcode为1，但是使用tee之后得到的为0。这时可以使用set -o pipefail，如下： ```shell ./a.out | tee echo $? # 0]]></summary></entry><entry><title type="html">opencl特性</title><link href="https://zhaode.wang/blog/2023/opencl/" rel="alternate" type="text/html" title="opencl特性"/><published>2023-11-13T00:00:00+00:00</published><updated>2023-11-13T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2023/opencl</id><content type="html" xml:base="https://zhaode.wang/blog/2023/opencl/"><![CDATA[<h2 id="工作组">工作组</h2> <p>opencl 使用工作组来组织工作项。工作组使用N维的网格作为索引，被称为<code class="language-plaintext highlighter-rouge">NDRange</code>；目前这个N维的索引维度可以是1、2或者3。</p> <p>索引分为全局索引 (globale index) 个 局部索引 (loacl index)；具体用如下：</p> <ul> <li>全局索引空间为<code class="language-plaintext highlighter-rouge">(Gx, Gy)</code></li> <li>工作组空间为<code class="language-plaintext highlighter-rouge">(Wx, Wy)</code></li> <li>局部索引空间为<code class="language-plaintext highlighter-rouge">(Lx, Ly)</code></li> <li>全局坐标<code class="language-plaintext highlighter-rouge">(gx, gy)</code></li> <li>工作组索引<code class="language-plaintext highlighter-rouge">(wx, wy)</code></li> <li>局部坐标<code class="language-plaintext highlighter-rouge">(lx, ly)</code></li> </ul> <p>则有</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 工作组大小</span>
<span class="n">Lx</span> <span class="o">=</span> <span class="n">Gx</span> <span class="o">/</span> <span class="n">Wx</span>
<span class="n">Ly</span> <span class="o">=</span> <span class="n">Gy</span> <span class="o">/</span> <span class="n">Wy</span>

<span class="c1">// 工作组id与局部id转换为全局id</span>
<span class="n">gx</span> <span class="o">=</span> <span class="n">wx</span> <span class="o">*</span> <span class="n">Lx</span> <span class="o">+</span> <span class="n">lx</span>
<span class="n">gy</span> <span class="o">=</span> <span class="n">wy</span> <span class="o">*</span> <span class="n">Ly</span> <span class="o">+</span> <span class="n">ly</span>

<span class="c1">// 全局id转换为工作组id与局部id</span>
<span class="n">wx</span> <span class="o">=</span> <span class="n">gx</span> <span class="o">/</span> <span class="n">Lx</span>
<span class="n">wy</span> <span class="o">=</span> <span class="n">gy</span> <span class="o">/</span> <span class="n">Ly</span>
<span class="n">lx</span> <span class="o">=</span> <span class="n">gx</span> <span class="o">%</span> <span class="n">Lx</span>
<span class="n">ly</span> <span class="o">=</span> <span class="n">gy</span> <span class="o">%</span> <span class="n">Ly</span>
</code></pre></div></div> <p>在opencl的kernel内部获取这些信息：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">g0</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">l1</span> <span class="o">=</span> <span class="n">get_local_id</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">L0</span> <span class="o">=</span> <span class="n">get_local_size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">w1</span> <span class="o">=</span> <span class="n">get_group_id</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="opencl"/><summary type="html"><![CDATA[工作组 opencl 使用工作组来组织工作项。工作组使用N维的网格作为索引，被称为NDRange；目前这个N维的索引维度可以是1、2或者3。]]></summary></entry></feed>