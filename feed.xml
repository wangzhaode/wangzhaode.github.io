<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zhaode.wang/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zhaode.wang/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-05T08:24:29+00:00</updated><id>https://zhaode.wang/feed.xml</id><title type="html">Zhaode’s blog</title><subtitle>A blog about technology and life. </subtitle><entry><title type="html">LLM训练实战手册</title><link href="https://zhaode.wang/blog/2025/llm-train/" rel="alternate" type="text/html" title="LLM训练实战手册"/><published>2025-11-04T00:00:00+00:00</published><updated>2025-11-04T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/llm-train</id><content type="html" xml:base="https://zhaode.wang/blog/2025/llm-train/"><![CDATA[<h2 id="1-前言">1. 前言</h2> <p>表面上看，已发表的研究论文似乎把一切都说得轻描淡写：无非是战略性的架构选择、精心策划的数据集，以及充足的算力。结果光鲜亮丽，消融实验（ablations）结构清晰，事后看来每一步决策都显得理所当然。</p> <p>但是，这些报告往往只展示了成功的“果”，并进行了一番“玫瑰色的回顾”（Rosy Retrospection）。它们并没有记录下<strong>凌晨两点的 dataloader 调试</strong>、<strong>突然爆发的 Loss 尖刺</strong>，或者那个<strong>默默破坏你训练的细微张量并行（Tensor Parallelism）Bug</strong>（后面我们还会专门提到！）。现实更加混乱、迭代性更强，充满了那些最终没有写进论文的“纠结瞬间”。</p> <p>这一次，我们将带领大家深入幕后，直击 <strong>SmolLM3</strong> 的训练过程——这是一个在 <strong>11 万亿（11T）Token</strong> 上训练出来的 <strong>30 亿（3B）参数多语言推理模型</strong>。</p> <p>这绝非一篇普通的博客文章，而是对一系列决策、发现和死胡同的“蜘蛛网”式梳理，它将为你揭示构建世界级语言模型所需要的核心洞察。</p> <p>同时，本文也是我们<strong>模型训练长文系列</strong>的<strong>“收官之作”</strong>（或“压轴大戏”）：我们之前已经深入探讨了<strong>大规模数据集构建</strong>（FineWeb）、<strong>编排数千块 GPU 协同工作</strong>（Ultra Scale Playbook），以及<strong>在过程的每一步选择最佳评估方法</strong>（Evaluation Guidebook）。现在，我们将所有这些元素整合起来，共同打造一个强大的 AI 模型。</p> <p>我们将全程陪伴大家，不仅仅是分享最终成功的“秘方”，更包括那些<strong>塑造了每一个决策的失败、基础设施故障以及调试过程</strong>。</p> <p>这个故事读起来就像一出<strong>扣人心弦的“戏剧”</strong>：</p> <ul> <li>你会看到那些有前景的小规模消融实验为什么在大规模上却可能 <strong>“水土不服”</strong>；</li> <li>为什么我们在训练了 <strong>1 万亿（1T）Token 之后不得不重新启动</strong>；</li> <li>如何有效地在保持强大的英语性能的同时，<strong>平衡多语言、数学和代码这几个相互竞争的目标</strong>；</li> <li>以及最终，我们是如何<strong>后训练（post-train）</strong>出一个混合推理模型的。</li> </ul> <p>我们努力将这趟冒险之旅组织成一个连贯的故事，而非仅仅是<strong>冰冷的步骤列表</strong>。请将它视为一本<strong>实战指南</strong>，献给所有想从“我们有优秀的数据集和 GPU”迈向“我们构建了一个真正强大的模型”的实践者。</p> <p>我们希望这种毫无保留的开放分享，能够<strong>弥合研究与生产之间的鸿沟</strong>，让你的下一次训练跑起来<strong>少一点混乱，多一点笃定</strong>。</p> <h3 id="11-如何阅读这篇文章你可能不需要一次性看完">1.1 如何阅读这篇文章？（你可能不需要“一次性看完”）</h3> <p>老实说，这篇文章篇幅<strong>非常、非常长</strong>，想要一口气从头读到尾，在现实中可能不太可行。</p> <p>好消息是，我们已经将文章结构化为几个<strong>独立且清晰</strong>的板块，你可以根据自己的兴趣和需求选择性地跳过或单独阅读。</p> <p>以下是文章的结构指南：</p> <ul> <li> <p>🧭 训练指南针（Training compass）：</p> <ul> <li><strong>核心内容：</strong> 这是一个高层次的讨论，关于你 <strong>是否应该自行预训练（Pretrain）一个模型</strong>。</li> <li><strong>适用人群：</strong> 在你“烧光”所有风投资金之前，我们为你列出了几个必须问自己的基本问题，并系统地指导你完成决策过程。</li> <li><strong>温馨提示：</strong> 这是一个比较宏观的部分。如果你是 <strong>“技术党”</strong>，想要直接看硬核技术内容，请快速略过此部分。</li> </ul> </li> <li> <p>🚀 预训练（Pretraining）：</p> <ul> <li><strong>核心内容：</strong> 在这之后的部分，涵盖了构建你自己 <strong>预训练运行的可靠“配方”</strong>所需了解的一切：如何运行消融实验（Ablations）、选择评估指标、混合数据源、进行架构决策、调整超参数，以及最终 <strong>“熬过”</strong> 这场训练马拉松。</li> <li><strong>适用人群：</strong> 无论是计划 <strong>从头开始预训练</strong>，还是对 <strong>持续预训练（Continued Pretraining，或称 Mid-training</strong> 感兴趣的朋友，这个板块都适用。</li> </ul> </li> <li> <p>🍒 后训练（Post-training）：</p> <ul> <li><strong>核心内容：</strong> 在这部分，你将学到如何最大限度地利用你的预训练模型所需的所有“技巧”。从 <strong>SFT (监督式微调)</strong>、<strong>DPO (直接偏好优化)</strong>、<strong>GRPO</strong> 等一整套后训练“字母表”开始，一直到<strong>模型合并（Model Merging）</strong>这种充满“黑暗艺术与炼金术”的操作。</li> <li><strong>价值所在：</strong> 关于如何让这些算法真正起效的知识，往往是通过痛苦的教训换来的。我们将在这里分享我们的经验，希望能让你少走一些弯路。</li> </ul> </li> <li> <p>🏭 基础设施（Infrastructure）：</p> <ul> <li><strong>核心比喻：</strong> 如果说预训练是蛋糕，后训练是上面的糖霜和樱桃，那么<strong>基础设施就是那台工业级的烤箱</strong>。没有它，什么都做不了；如果它坏了，你愉快的周日烘焙时间就会变成一场火灾隐患。</li> <li><strong>核心内容：</strong> 理解、分析和调试 GPU 集群的知识分散在各种库、文档和论坛中。本节将详细讲解 <strong>GPU 布局、CPU/GPU/节点/存储之间的通信模式</strong>，以及如何<strong>识别和克服瓶颈</strong>。</li> </ul> </li> </ul> <p><strong>那么，我们从何处开始呢？</strong> 很简单，<strong>选择你最感兴趣的那个板块，让我们出发吧！</strong></p> <h2 id="2-训练指南针灵魂三问why--what--how">2. 训练指南针：灵魂三问（Why → What → How）</h2> <p>机器学习领域对于 <strong>“优化”</strong> 似乎有着一种近乎痴迷的关系。我们的注意力总是集中在 Loss 曲线、模型架构和训练吞吐量上；毕竟，机器学习从根本上说就是关于 <strong>优化模型的损失函数</strong>。然而，在深入这些技术细节之前，有一个更根本的问题却常常被忽略：<strong>我们真的有必要训练这个模型吗？</strong></p> <p>开源 AI 生态系统几乎每天都在发布世界级的模型：Qwen、Gemma、DeepSeek、Kimi、Llama（当然，还有它的继任者们）、Olmo……这份名单每个月都在持续增长。它们不仅仅是研究原型或“玩具”示例，而是 <strong>可以直接用于生产的模型</strong>，涵盖了令人震惊的广泛用例——从多语言理解到代码生成，再到复杂的推理能力。更重要的是，它们大多带有 <strong>宽松的许可协议（Permissive Licenses）</strong>，并拥有活跃的社区随时准备为你提供帮助。</p> <p>这也提出了一个<strong>令人不安的真相</strong>：也许，你根本不需要训练自己的模型。</p> <p>这听起来像是一个“大模型训练指南”<strong>最奇怪的开场白</strong>。但事实是，许多失败的训练项目，并非因为超参数设置不当或代码有 Bug，而是因为 <strong>有人决定训练一个他们根本不需要的模型</strong>。所以，在你投入资金和时间开始训练，并钻研“如何执行”之前，你必须先回答两个核心问题：<strong>你为什么要训练这个模型？</strong> <strong>你应该训练一个什么样的模型？</strong> 如果对这两个问题没有清晰的答案，你很可能会浪费数月的算力投入和工程师时间，最终却造出了一个“全世界早已拥有”的东西，甚至更糟—— <strong>一个根本没人需要的东西</strong>。</p> <p>让我们从 <strong>“Why”（为什么）</strong> 开始。因为如果不懂得你的目标，你后续的所有决策都将是 <strong>毫无章法</strong>。</p> <h3 id="21-why那个没人愿意回答的问题">2.1 Why：那个没人愿意回答的问题</h3> <p>让我们直言不讳地谈谈实践中经常发生的事情。</p> <p>某个人（如果他们足够幸运）获得了 GPU 集群的访问权限——可能是通过研究经费，也可能是利用公司闲置的算力，而他们的思路大致是这样的：“我们有 100 块 H100s，可以用三个月。来训练一个模型吧！”模型大小被随意选择，数据集则从各种可用的资源中拼凑起来。训练开始了。六个月后，在烧光了算力预算和团队士气之后，这个模型却无人问津，<strong>因为从没有人问过“为什么”</strong>。</p> <p>这里有一些<strong>你绝对不应该训练模型</strong>的常见理由：</p> <ul> <li><strong>“我们恰好有空闲的算力。”</strong>（这只是<strong>资源</strong>，不是<strong>目标</strong>。）</li> <li><strong>“其他人都在做。”</strong>（这是<strong>同侪压力</strong>，不是<strong>战略</strong>。）</li> <li><strong>“AI 是未来。”</strong>（这是<strong>陈词滥调</strong>，不是<strong>计划</strong>。）</li> <li><strong>“我们想要最强大的模型。”</strong>（这个目标<strong>不够具体</strong>，无法指导任何实际决策。）</li> </ul> <p>“我们训练了自己的模型”这种诱惑力是强大的。但在投入大量时间和资源之前，有必要问一句：<strong>你到底为什么需要训练这个模型？</strong></p> <p>下面的流程图提供了一个思路指导，这是你在开始一个大型预训练项目前应该经历的思考过程。从技术角度看，你首先应该搞清楚的是：<strong>是否已经有一个现成的模型，通过简单的提示（Prompt）或微调（Fine-tune）就能完成你的工作？</strong></p> <p>本质上，只有在以下三个常见领域中，定制化的预训练才可能真正有意义：<strong>你想进行开创性的研究，你的生产用例有非常特殊的需求，或者你想填补开源模型生态系统中的空白</strong>。让我们快速看看每一种情况：</p> <h4 id="211-科研research你到底想理解什么">2.1.1 科研（Research）：你到底想理解什么？</h4> <p>在大模型（LLM）领域，你可以做的研究课题非常多。</p> <p>这些 LLM 研究项目的共同点是：你通常都会从一个<strong>清晰定义的问题</strong>开始。例如：</p> <ul> <li>我们能否将基于这种<strong>新型优化器</strong>的训练扩展到 <strong>100 亿（10B）参数以上</strong>的模型？（<em>参考自：Muon 在 LLM 训练中的可扩展性</em>）</li> <li><strong>仅使用强化学习，不依赖 SFT</strong>，能否产出推理能力？（<em>参考自：DeepSeek-R1：通过强化学习激励 LLM 的推理能力</em>）</li> <li>我们能否<strong>仅靠纯合成的“教科书式”数据</strong>，训练出优秀的小模型？（<em>参考自：Textbooks Are All You Need</em>）</li> <li>我们能否通过仅训练<strong>公开许可（Openly Licensed）的数据</strong>来达到具有竞争力的性能？（<em>参考自：The Common Pile v0.1：一个 8TB 的公共领域和公开许可文本数据集</em>）</li> </ul> <p>把假设设定得<strong>尽可能具体</strong>，并提前思考所需的<strong>实验规模</strong>，能够大大增加你成功的几率。</p> <h4 id="212-产品化production为什么你不能直接用现成的模型">2.1.2 产品化（Production）：为什么你不能直接用现成的模型？</h4> <p>企业不能直接使用现成的通用模型（off-the-shelf models）来解决其特定用例，主要有三个原因。其中两个是技术性的，另一个则关乎治理。</p> <p><strong>第一个要自己训练模型的原因是：领域特殊性（Domain Specificity）。</strong> 当你的数据或任务涉及 <strong>高度专业化的词汇或结构</strong>，而现有通用模型无法很好地处理时。例如：</p> <ul> <li>一个关于 DNA 的模型，它需要独特的词汇表和处理长距离依赖关系的能力。</li> <li>一个法律或金融模型，要求对领域特定的术语和逻辑有深入的理解。</li> </ul> <p><strong>第二个相关原因是：部署约束（Deployment Constraints）。</strong> 当你需要一个模型来适配你的硬件、延迟或隐私要求时。例如，一个需要在 <strong>无人机上运行</strong>，或在 <strong>配备定制硬件（如 FPGA）的本地（on-prem）系统</strong> 上运行的大模型。</p> <p>这里有一个简单的测试方法：花几天时间，基于 Qwen3、Gemma3 或其他当前的 SOTA 模型进行构建。你能通过 <strong>Prompting（提示）、工具调用（Tool-use）或后训练（Post-training）</strong> 达到你的性能目标吗？如果不能，那么，可能就是时候自己训练一个了。</p> <ul> <li><strong>一个小小的提醒：</strong> 即使为了满足你的要求，所需的 <strong>后训练预算非常庞大，它仍可能比从头开始训练更经济。</strong> 毕竟，为你的模型微调 <strong>1 万亿（1T）Token</strong>，仍然比从头开始训练 <strong>10 万亿（10T）Token</strong> 更划算。</li> <li>（<em>也是从这个时候开始，大模型训练者开始<strong>神奇地</strong>称之为 <strong>“Mid-training”（中途训练）</strong>，而不是 Post-training 了。</em>）</li> </ul> <p><strong>第三个建立内部语言模型的原因是：安全与治理（Safety and Governance）。</strong> 由于你身处一个受严格监管的行业或处理高风险的应用，你需要对训练数据、模型行为和更新周期拥有<strong>完全的控制权</strong>。你需要确切地知道模型中包含了什么，并能够向监管机构证明这一点。在某些情况下，你可能别无选择，只能自建模型。</p> <p>以上是公司训练内部模型的主要原因。那么，那些发布开源模型的公司或组织又是怎么考虑的呢？</p> <h4 id="213-战略性开源strategic-open-source你看到了可以填补的空白吗">2.1.3 战略性开源（Strategic Open-Source）：你看到了可以填补的空白吗？</h4> <p>经验丰富的 AI 实验室发布新的开源模型，最常见的原因之一是：<strong>他们识别出了开源生态系统中的一个特定空白或一个新的 AI 用例。</strong></p> <p>这种模式通常是这样的：你注意到一个 <strong>未被充分探索的领域</strong>。也许现在没有强大且具备超长上下文能力的 <strong>设备端（on-device）模型</strong>；或者现有的多语言模型在 <strong>低资源语言上表现很弱</strong>；又或者，领域正在转向像 Genie3 那样的 <strong>交互式世界模型</strong>，但还没有好的开源模型出现。</p> <p>你有理由相信自己可以做得更好。也许你整理出了 <strong>更优质的训练数据</strong>，开发出了 <strong>更棒的训练“配方”</strong>，或者拥有其他机构无法达到的 <strong>算力优势来支持“过度训练”（Overtrain）</strong>。你的目标是具体的：不是“有史以来最好的模型”，而是“<strong>最适合设备端使用的 3B 模型</strong>”，或“<strong>第一个具备 1M 上下文的小模型</strong>”。</p> <p>这是一个真实且有价值的目标。成功会创造价值：开发者会采用你的模型，它会成为其他人的基础设施，或者为你建立技术信誉。但<strong>成功需要经验</strong>。在一个竞争激烈的领域中，你需要知道什么才是真正可行的，以及如何可靠地执行。</p> <p>为了让这个思路更具体，让我们来看看 Hugging Face 是如何思考这个问题的。</p> <h4 id="214-hugging-face-的思考我们为什么要训练开源模型">2.1.4 Hugging Face 的思考：我们为什么要训练开源模型？</h4> <p>那么，Hugging Face 为什么要训练和发布开源模型呢？答案很简单：<strong>我们致力于构建对开源生态系统有用的东西，并填补那些极少人涉足的空白。</strong></p> <p>这包括数据集、工具，当然也包括训练模型。我们启动的每一个 LLM 训练项目，都是始于 <strong>发现一个空白</strong>，并坚信我们能做出有意义的贡献。</p> <p>我们最初的 LLM 项目是在 <strong>GPT-3 (Brown et al., 2020)</strong> 发布之后启动的。当时，感觉没有人愿意构建一个开放的替代品，我们担心相关的知识最终会被锁定在少数几个工业实验室中。因此，我们发起了 <strong>BigScience 研讨会</strong>，旨在训练一个开源版本的 GPT-3。最终诞生的模型就是 <strong>Bloom</strong>，它汇集了数十位贡献者一年的努力，从构建训练堆栈、分词器（tokenizer）到预训练语料库，最终预训练了一个 <strong>1750 亿（175B）参数的模型</strong>。</p> <p>Bloom 的继任者是 2022 年的 <strong>StarCoder (Li et al., 2023)</strong>。当时 OpenAI 为 GitHub Copilot 开发了 <strong>Codex (Chen et al., 2021)</strong>，但它是闭源的。很明显，构建一个开源替代品将为整个生态系统带来巨大的价值。因此，我们与 ServiceNow 合作，在 <strong>BigCode</strong> 的框架下，构建了 <strong>The Stack</strong> 数据集，并训练了 <strong>StarCoder 15B</strong> 来复现 Codex 的能力。<strong>StarCoder2 (Lozhkov et al., 2024)</strong> 的诞生，则是源于我们认识到可以进行更长时间的训练，并意识到 <strong>更小但训练更久的模型可能比一个巨型模型更有价值</strong>。我们训练了一个模型家族（3B/7B/15B），使用了数万亿（Trillions）的 Token，远远超过当时任何开放代码模型的训练量。</p> <p><strong>SmolLM 系列</strong>遵循了类似的模式。我们注意到当时 <strong>强大的小型模型非常稀缺</strong>，而我们刚刚构建了 <strong>FineWeb-Edu (Penedo et al., 2024)</strong> 这个强大的预训练数据集。<strong>SmolLM (135M/360M/1.7B)</strong> 是我们的第一个版本。<strong>SmolLM2 (Allal et al., 2025)</strong> 则专注于更好的数据和更长的训练，在多个方面达到了 SOTA 性能。而 <strong>SmolLM3</strong> 则扩展到了 <strong>30 亿参数</strong>，同时加入了 <strong>混合推理（hybrid reasoning）、多语言能力和长上下文</strong> 等社区在 2025 年高度重视的功能。</p> <p>这种模式甚至延伸到了预训练之外：我们训练了 <strong>Zephyr (Tunstall et al., 2023)</strong> 来证明 <strong>DPO</strong> 可以进行大规模应用；启动了 <strong>Open-R1</strong> 来复现 DeepSeek R1 的蒸馏管线；并发布了用于 <strong>竞技编程（competitive programming）</strong> 的 <strong>OlympicCoder</strong>，在国际信息学奥林匹克竞赛中达到了 SOTA 性能。我们还探索了其他模态，例如用于视觉的 <strong>SmolVLM (Marafioti et al., 2025)</strong> 和用于机器人技术的 <strong>SmolVLA (Shukor et al., 2025)</strong>。</p> <p>希望这一部分已经成功说服你：<strong>深入思考你为什么要训练一个模型是极具价值的。</strong></p> <p>在本文接下来的部分，我们将假设你已经完成了这种“灵魂拷问”，并且有了一个<strong>合法的、充分的理由</strong>去启动你的训练项目。</p> <p>好的，这是下一段的翻译和润色：</p> <h3 id="22-what将目标转化为实际决策">2.2 What：将目标转化为实际决策</h3> <p>既然你已经明确了 <strong>为什么</strong> 要训练，那么接下来就该确定 <strong>应该训练什么</strong> 了。</p> <p>这里的“做什么”（What）指的是：<strong>模型类型</strong>（密集型 Dense、MoE 专家混合、混合型 Hybrid，还是全新的类型）、<strong>模型规模</strong>、<strong>架构细节</strong>和<strong>数据混合配比</strong>。</p> <p>一旦你确定了“为什么”（Why），你就可以由此推导出“做什么”（What）。举例来说：</p> <table> <thead> <tr> <th style="text-align: left">训练目的 (Why)</th> <th style="text-align: left">$\rightarrow$</th> <th style="text-align: left">模型规格 (What)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>追求设备端运行的快速模型</strong></td> <td style="text-align: left">$\rightarrow$</td> <td style="text-align: left"><strong>小且高效的模型</strong></td> </tr> <tr> <td style="text-align: left"><strong>构建强大的多语言模型</strong></td> <td style="text-align: left">$\rightarrow$</td> <td style="text-align: left"><strong>大型分词器词汇表（Large Tokenizer Vocab）</strong></td> </tr> <tr> <td style="text-align: left"><strong>需要超长上下文能力</strong></td> <td style="text-align: left">$\rightarrow$</td> <td style="text-align: left"><strong>混合型架构（Hybrid Architecture）</strong></td> </tr> </tbody> </table> <p>除了受用例驱动的决策外，还有一些选择是旨在优化训练本身的，例如让训练<strong>更稳定、更具样本效率，或速度更快</strong>。这些决策并非总是非黑即白，但你可以大致将决策过程分为两个阶段：</p> <p><strong>规划阶段（Planning）：</strong> 在进行实验之前，你需要将你的用例映射到需要确定的组件上：你的<strong>部署环境</strong>决定了<strong>模型规模</strong>的限制。你的<strong>时间表</strong>决定了你可以承担<strong>哪些架构风险</strong>。你的<strong>目标能力</strong>决定了<strong>数据集</strong>的要求。这个阶段的核心就是：将“Why”中的每一个约束条件，都与“What”中的具体规格<strong>紧密连接</strong>起来。</p> <p><strong>验证阶段（Validation）：</strong>一旦你有了一个起始点和一份潜在修改清单，就要进行<strong>系统性的测试</strong>。由于测试成本高昂，你应将精力集中在那些能 <strong>有意义地提升你的用例性能</strong>，或能 <strong>显著优化你的训练过程</strong>的改动上。这就是 <strong>消融实验（Ablations）</strong> 发挥作用的地方，我们将在后续的“消融实验”章节详细介绍。</p> <p>在接下来的章节中，你将了解到定义模型的所有选项，以及如何通过系统性实验来缩小选择范围。但在那之前，我们想分享一些关于<strong>如何组织团队和项目</strong>的经验——这些经验来自于我们自己训练模型的实践，以及观察那些成功构建优秀 LLM 团队的优秀做法。</p> <p>好的，这是下一段的翻译和润色：</p> <h3 id="23-how迭代速度与数据质量">2.3 How：迭代速度与数据质量</h3> <p>通往罗马的道路当然不止一条，但我们发现，成功的大模型（LLM）训练团队之所以能脱颖而出，其关键因素在于 <strong>迭代速度</strong>。</p> <p>训练 LLM 本质上是一种 <strong>“边训边学”</strong> 的学科，你训练的次数越多，你的团队就会变得越优秀。因此，那些一年只训练一个模型的团队，和那些一个季度就能训练一个模型的团队相比，<strong>后者的进步速度会快得多</strong>。你可以参考 Qwen 和 DeepSeek 等团队，他们现在已是家喻户晓的名字，凭借的就是长期以来持续快速地发布新模型。</p> <p>除了迭代速度之外，<strong>数据策划（Data Curation）</strong>是迄今为止对 LLM 训练<strong>最具影响力</strong>的方面。人们天然倾向于钻研架构选择来改进模型，但那些在大模型训练中表现优异的团队，却是<strong>对高质量数据痴迷程度高于一切</strong>的团队。</p> <p>另一个与迭代速度紧密相关的因素是<strong>团队规模</strong>：对于主要的预训练任务，你只需要<strong>少数几个人</strong>，并为他们配备足够的算力来执行即可。例如，如今要预训练一个像 Llama 3 这样的模型，你可能只需要 <strong>2 到 3 个人</strong>。只有当你开始涉足更多元化的训练和下游任务（如多模态、多语言、后训练等）时，才需要慢慢增加更多的人手，以在每个领域做到精通。</p> <p>因此，秘诀就是：<strong>从一个小型、装备精良的团队开始，每隔两到三个月就构建一个新模型</strong>，你将在短时间内攀升至行业的顶端。</p> <p>好了，接下来的文章将专注于这个团队的<strong>日常技术细节</strong>！</p> <h2 id="3-每个大模型的诞生都始于一场小小的消融实验">3. 每个大模型的诞生，都始于一场小小的“消融实验”</h2> <p>在我们开始训练一个大型语言模型（LLM）之前，我们需要做出无数将影响模型性能和训练效率的决策。我们该选择什么样的架构最适合我们的用例？使用哪种优化器和学习率调度？如何混合不同的数据源？</p> <p>“这些决策是如何做出的？”这是一个被频繁问及的问题。人们有时期望它们是通过<strong>深入的思考</strong>得出的。虽然战略性思维至关重要——正如我们在前一节中讨论的，它能帮你识别出哪些架构更改值得测试——但<strong>仅仅依靠推理是不够的</strong>。在 LLM 领域，事物并不总是直觉化的，那些“应该有效”的假设在实践中往往会落空。</p> <p>举个例子，使用看起来 <strong>“质量最高的数据”</strong> 并不一定能产出更强的模型。以 <strong>arXiv</strong> 为例，它是人类科学知识的巨大宝库。直觉上，用如此丰富的 STEM 数据进行训练应该能产出更优秀的模型，对吗？但实际上，它<strong>并非总是如此</strong>，特别是对于小模型，<strong>甚至可能损害性能</strong> (Shao et al., 2024)。<strong>为什么会这样？</strong> 原因是虽然 arXiv 论文知识丰富，但它们<strong>高度专业化</strong>，并以一种狭隘的学术风格撰写，这与模型最擅长学习的<strong>多样化、通用性文本</strong>截然不同。</p> <p>那么，如果苦思冥想没有帮助，我们如何知道什么才是有效的呢？<strong>答案是：像优秀的经验主义者一样，运行大量的实验！</strong> 机器学习不是纯粹的数学，它更像是一门<strong>实验科学</strong>。由于这些实验将指导我们许多关键决策，因此将它们设置好至关重要。我们本质上希望从这些实验中获得两个主要属性：</p> <ol> <li><strong>速度（Speed）：</strong> 它们应该尽可能快地运行，以便我们能够频繁迭代。我们能运行的消融实验越多，能验证的假设就越多。</li> <li><strong>可靠性（Reliability）：</strong> 它们应该提供强大的<strong>判别力（discriminative power）</strong>。如果我们关注的指标无法在早期有意义地分辨不同设置之间的优劣，那么我们的消融实验可能提供的洞察就很少（如果结果充满噪音，我们就有追逐噪音的风险！）。</li> </ol> <p>但在设置消融实验之前，我们需要对<strong>架构类型</strong>和<strong>模型规模</strong>做出一些基础性的选择。这些由我们的“指南针”指导的决策，会影响我们使用哪种训练框架、如何分配算力预算，以及从哪个基线开始。</p> <p>对于 SmolLM3，我们选择了 <strong>30 亿参数的密集型（dense）Llama 风格架构</strong>，因为我们的目标是小型设备端模型。但正如你将在 <strong>“设计模型架构”</strong> 一章中看到的，<strong>MoE 或混合模型</strong>可能更适合你的用例，不同的模型规模也伴随着不同的权衡取舍。稍后我们将深入探讨这些选择，并向你展示如何做出这些决定。现在，让我们从最实际的第一步开始：<strong>选择你的基线（Baseline）</strong>。</p> <h3 id="31-选择你的基线baseline">3.1 选择你的基线（Baseline）</h3> <p>每一个成功的模型都是建立在一个<strong>经过验证的基础</strong>之上，然后根据自身需求进行修改的。</p> <ul> <li>当 Qwen 训练他们的第一个模型家族时 (Bai et al., 2023)，他们以 <strong>Llama 的架构</strong>为起点。</li> <li>当 Meta 训练 Llama 3 时，他们从 <strong>Llama 2</strong> 开始。</li> <li>Kimi K2 则始于 <strong>DeepSeek-V3 的 MoE 架构</strong>。</li> </ul> <p>这种“继承”不仅适用于架构，也适用于训练超参数和优化器。</p> <p><strong>为什么呢？</strong> 优秀的架构和训练设置需要多年的迭代，并汇集众多组织的智慧。标准的 Transformer 结构和 Adam 等优化器，都是经过<strong>数千次实验</strong>才被完善的。人们已经发现了它们的失败模式，调试了不稳定性，并优化了实现。</p> <p><strong>从一个经过验证的基础开始，意味着你继承了所有这些积累的知识。</strong> 而从零开始，则意味着你需要自己重新发现每一个问题。</p> <p>以下是一个好的架构起点应该具备的条件：</p> <ol> <li><strong>符合你的约束条件：</strong> 契合你的部署目标和实际用例。</li> <li><strong>经过大规模验证：</strong> 在相似或更大的规模上，跑过<strong>数万亿（multi-trillion）Token</strong> 的训练。</li> <li><strong>文档完善：</strong> 有明确的、被证明在开源模型中有效的超参数配置。</li> <li><strong>框架支持良好：</strong> 理想情况下，它应该被你考虑使用的<strong>训练框架</strong>和计划用于推理的<strong>推理框架</strong>所支持。</li> </ol> <p>下面列出了一份非详尽的清单，展示了 2025 年针对不同架构类型和模型规模的一些强大基线选项：</p> <table> <thead> <tr> <th style="text-align: left">架构类型</th> <th style="text-align: left">模型家族</th> <th style="text-align: left">常见规模 (Sizes)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>密集型（Dense）</strong></td> <td style="text-align: left">Llama 3.1</td> <td style="text-align: left">8B, 70B</td> </tr> <tr> <td style="text-align: left"><strong>密集型（Dense）</strong></td> <td style="text-align: left">Llama 3.2</td> <td style="text-align: left">1B, 3B</td> </tr> <tr> <td style="text-align: left"><strong>密集型（Dense）</strong></td> <td style="text-align: left">Qwen3</td> <td style="text-align: left">0.6B, 1.7B, 4B, 14B, 32B</td> </tr> <tr> <td style="text-align: left"><strong>密集型（Dense）</strong></td> <td style="text-align: left">Gemma3</td> <td style="text-align: left">12B, 27B</td> </tr> <tr> <td style="text-align: left"><strong>密集型（Dense）</strong></td> <td style="text-align: left">SmolLM2, SmolLM3</td> <td style="text-align: left">135M, 360M, 1.7B, 3B</td> </tr> <tr> <td style="text-align: left"><strong>MoE (专家混合)</strong></td> <td style="text-align: left">Qwen3 MoE</td> <td style="text-align: left">30B-A3B, 235B-A122B</td> </tr> <tr> <td style="text-align: left"><strong>MoE (专家混合)</strong></td> <td style="text-align: left">GPT-OSS</td> <td style="text-align: left">21B-A3B, 117B-A5B</td> </tr> <tr> <td style="text-align: left"><strong>MoE (专家混合)</strong></td> <td style="text-align: left">Kimi Moonlight</td> <td style="text-align: left">16B-A3B</td> </tr> <tr> <td style="text-align: left"><strong>MoE (专家混合)</strong></td> <td style="text-align: left">Kimi-k2</td> <td style="text-align: left">1T-A32B</td> </tr> <tr> <td style="text-align: left"><strong>MoE (专家混合)</strong></td> <td style="text-align: left">DeepSeek V3</td> <td style="text-align: left">671B-A37B</td> </tr> <tr> <td style="text-align: left"><strong>混合型（Hybrid）</strong></td> <td style="text-align: left">Zamba2</td> <td style="text-align: left">1.2B, 2.7B, 7B</td> </tr> <tr> <td style="text-align: left"><strong>混合型（Hybrid）</strong></td> <td style="text-align: left">Falcon-H1</td> <td style="text-align: left">0.5B, 1.5B, 3B, 7B, 34B</td> </tr> <tr> <td style="text-align: left"><strong>MoE + 混合型</strong></td> <td style="text-align: left">Qwen3-Next</td> <td style="text-align: left">80B-A3B</td> </tr> <tr> <td style="text-align: left"><strong>MoE + 混合型</strong></td> <td style="text-align: left">MiniMax-01</td> <td style="text-align: left">456B-A46B</td> </tr> </tbody> </table> <p>因此，请找到你的架构类型，并选择一个参数数量接近你目标模型的基线。<strong>不要过度纠结于此</strong>，因为你最初选择的架构并非一成不变。在下一节中，我们将看到如何从一个基线出发，一步步构建出最适合你的<strong>最终架构</strong>。</p> <h4 id="311-修改你的基线去风险化的原则">3.1.1 修改你的基线：<strong>“去风险化”</strong>的原则</h4> <p>现在你拥有了一个有效且符合你用例的基线模型。你大可以停在这里，用你的数据混合集（假设数据质量不错）进行训练，很可能会得到一个体面的模型。许多成功的项目正是这样做的。然而，基线模型并非为你的<strong>特定约束</strong>而优化，它们是为其构建者的用例和部署目标而设计的。因此，你很可能需要进行一些修改，使其更好地契合你的目标。但请注意，<strong>每一次架构上的更改都伴随着风险</strong>：它可能提升性能、彻底搞砸，或者什么也没做，只是浪费了你的消融实验算力。</p> <p>让你保持在正轨上的纪律是 <strong>“去风险化”（Derisking）</strong>：<strong>除非你已经测试并确认它有帮助，否则绝不更改任何东西。</strong></p> <p>棘手之处在于，你的基线和训练设置有太多可修改的组件：<strong>注意力机制、位置编码、激活函数、优化器、训练超参数、归一化方案、模型布局</strong>等等。每一个都代表着一个潜在的实验，而这些组件往往以<strong>非线性</strong>的方式相互作用。你既没有时间，也没有算力来测试所有组合或探索所有的交互。</p> <p>正确的做法是：<strong>从测试有前景的改动开始，并以当前基线为参照。</strong> 当某个改动有效时，就将其集成进来，创建一个<strong>新的基线</strong>，然后针对这个新基线测试下一个改动。如果你的算力预算允许，你可以独立测试多项改动，并运行 <strong>“留一法分析”（leave-one-out analysis）</strong>。</p> <p><strong>千万不要掉入陷阱：</strong> 避免对每一个超参数进行详尽的<strong>网格搜索（Grid Searches）</strong>，也避免测试每一个新出现的架构变体。</p> <p>现在你知道了如何通过战略规划来确定哪些改动是有前景的，接下来就该进入<strong>经验验证</strong>了。在接下来的部分中，我们将向你展示如何在实践中真正测试这些更改。我们将涵盖如何设置可靠的实验、如何解读结果以及避免常见的陷阱。随后，在后续章节中，我们将通过具体案例，讲解如何测试流行的架构、数据、基础设施和训练决策。</p> <p>那么，让我们先搭建一个可用于实验的简单消融设置。第一步，我们需要决定选择哪个训练框架。</p> <h3 id="32-挑选训练框架">3.2 挑选训练框架</h3> <p>我们需要做的第一个决策是：选择哪个框架来训练模型，进而也决定了用来运行所有消融实验的框架。这个选择需要平衡以下三个关键考量点：</p> <ol> <li><strong>架构兼容性：</strong> 框架必须支持我们的目标架构，或能让我们轻松地进行扩展。</li> <li><strong>稳定性和生产就绪：</strong> 框架需要稳定、成熟，不会在训练中途神秘地崩溃。</li> <li><strong>高吞吐量：</strong> 它应该能提供强大的吞吐量，以便我们快速迭代，最大限度地利用算力预算。</li> </ol> <p>在实践中，这些要求可能相互掣肘，形成权衡取舍。让我们来看看可用的选项。</p> <table> <thead> <tr> <th style="text-align: left">框架</th> <th style="text-align: left">特性覆盖</th> <th style="text-align: left">实战检验</th> <th style="text-align: left">优化程度</th> <th style="text-align: left">核心/总代码行数</th> <th style="text-align: left">扩展性与调试难度</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Megatron-LM</strong></td> <td style="text-align: left">✅ 功能全面</td> <td style="text-align: left">✅ Kimi-K2, Nemotron</td> <td style="text-align: left">✅ 3D 并行先驱</td> <td style="text-align: left">93k / 269k</td> <td style="text-align: left">⚠️ 难度大，不适合新手</td> </tr> <tr> <td style="text-align: left"><strong>DeepSpeed</strong></td> <td style="text-align: left">✅ 功能全面</td> <td style="text-align: left">✅ BLOOM, GLM</td> <td style="text-align: left">✅ ZeRO &amp; 3D 并行先驱</td> <td style="text-align: left">94k / 194k</td> <td style="text-align: left">⚠️ 难度大，不适合新手</td> </tr> <tr> <td style="text-align: left"><strong>TorchTitan</strong></td> <td style="text-align: left">⚡ 功能集不断增长</td> <td style="text-align: left">⚠️ 较新，但经 PyTorch 团队检验</td> <td style="text-align: left">⚡ 针对密集模型优化，MoE 正在改进中</td> <td style="text-align: left">7k / 9k</td> <td style="text-align: left">⚡ 适中：需了解并行知识</td> </tr> <tr> <td style="text-align: left"><strong>Nanotron</strong></td> <td style="text-align: left">🎯 极简，为 HF 预训练定制</td> <td style="text-align: left">✅ 是 (StarCoder, SmolLM)</td> <td style="text-align: left">✅ 高度优化 (UltraScale Playbook)</td> <td style="text-align: left">15k / 66k</td> <td style="text-align: left">⚡ 适中：需了解并行知识</td> </tr> </tbody> </table> <p>上表总结了流行框架之间的关键权衡。（前三个框架的代码行数数据来自 TorchTitan 技术报告 (Liang et al., 2025)）。让我们详细讨论每一个框架：</p> <p><strong>Megatron-LM</strong> Nvidia 的 Megatron-LM 已经存在多年，久经沙场。它是 Kimi 的 K2 (Team et al., 2025) 等模型的幕后功臣，提供了可靠的吞吐量和我们想要的大多数生产功能。但这种成熟也带来了<strong>复杂性</strong>：当你刚接触它时，代码库可能会让人难以理解和修改。</p> <p><strong>DeepSpeed</strong> DeepSpeed 属于类似的类别。它是 <strong>ZeRO 优化</strong>的先驱，为 BLOOM 和 GLM 等模型提供了动力。和 Megatron-LM 一样，它经过了广泛的实战检验和优化，但面临着同样的<strong>复杂性挑战</strong>。庞大的代码库（总计 194k 行）在入门时可能令人生畏，尤其是当你需要实现自定义功能或调试意外行为时。</p> <p><strong>TorchTitan</strong> 另一方面，PyTorch 最近推出的 <strong>TorchTitan</strong> 库则<strong>轻量且更易于导航</strong>，这得益于其紧凑和模块化的代码库。它具备预训练所需的核心功能，非常适合<strong>快速实验</strong>。然而，由于它相对较新，实战检验不如前两者充分，并且由于仍在积极开发中，<strong>稳定性可能略逊一筹</strong>。</p> <p><strong>Nanotron</strong> 我们 Hugging Face 采取了不同的路径，从头构建了自己的框架 <strong>Nanotron</strong>。这为我们带来了<strong>完全的灵活性</strong>和对大规模预训练的<strong>深入理解</strong>——这些洞察后来演变成了《Ultra Scale Playbook》。虽然我们开源了该库并获得了社区的宝贵反馈，但在大多数情况下，我们不得不先自己对功能进行实战检验。该框架现在支持我们训练所需的所有生产功能，但仍在构建如 <strong>MoE 支持</strong>等区域。</p> <p>对于我们而言，从头构建当时是合理的，但这需要对团队专业知识和时间进行重大投入，用于调试问题和添加缺失功能。一个强大的替代方案是<strong>分叉（fork）</strong>一个现有框架，并根据你的需求进行增强。例如，Thinking Machines Lab 就是将他们的内部预训练库作为 TorchTitan 的一个分叉来构建的（来源）。</p> <p>最终，你的选择取决于<strong>团队的专业知识、目标功能，以及你愿意投入多少时间进行开发，而不是直接使用最成熟的生产选项。</strong></p> <p>如果多个框架都能满足你的需求，那么请在你的特定硬件上<strong>比较它们的吞吐量</strong>。对于快速实验和竞速运行，<strong>更简洁的代码库通常会获胜</strong>。</p> <h3 id="33-消融实验设置">3.3 消融实验设置</h3> <p>既然训练框架已定，我们现在就需要设计我们的消融实验设置。我们需要实验足够快以便快速迭代，但又需要足够大，以确保结果能提供有价值的信号，并能 <strong>外推（extrapolate）</strong> 到最终的模型。让我们来看看如何设置它。</p> <h4 id="331-搭建消融实验框架">3.3.1 搭建消融实验框架</h4> <p>消融实验的目标是在小规模上运行实验，并获得可以自信地外推到最终生产运行的结论。</p> <p>主要有两种方法：</p> <ol> <li><strong>方法一（减 Token）：</strong> 保持目标模型大小不变，但在<strong>更少的 Token</strong> 上进行训练。例如，在 SmolLM3 的消融实验中，我们用完整的 <strong>30 亿参数模型</strong>，但在 <strong>1000 亿（100B）Token</strong> 上进行训练，而不是最终的 11 万亿 Token。</li> <li><strong>方法二（减模型）：</strong> 如果目标模型太大，我们可以训练一个<strong>更小的代理模型</strong>来进行消融。例如，Kimi 在开发他们拥有 32 亿激活参数的 <strong>1 万亿（1T）参数 Kimi K2 模型</strong>时，对所有消融实验都使用完整大小显然过于昂贵，因此他们用一个 <strong>30 亿参数 MoE 模型</strong>（0.5B 激活参数）运行了部分消融实验 (Team et al., 2025)。</li> </ol> <p>一个关键问题是：这些小规模的发现真的能迁移吗？根据我们的经验，如果某个改动在<strong>小规模上损害了性能，你可以放心地将其排除在更大规模的训练之外</strong>。但如果某个改动在小规模上有效，你仍然需要确保在<strong>合理的 Token 数量</strong>上进行了训练，才能高概率地得出这些发现可以外推到更大规模的结论。<strong>训练时间越长，消融模型与最终模型越接近，结果就越可靠。</strong></p> <p>在本文中，我们将使用一个<strong>基线版（Vanilla）Transformer</strong> 来进行所有消融实验。我们的主要设置为：一个遵循 <strong>Llama 3.2 1B 架构的 10 亿参数 Transformer</strong>，在 <strong>450 亿 Token</strong> 上训练。这在一个配备 <strong>8 块 H100</strong> 的节点上大约需要 <strong>1.5 天</strong>（使用 nanotron 配置，速度约为每 GPU 每秒 42k Token）。在 SmolLM3 的训练过程中，我们是在一个 3B 模型上用 100B Token 运行这些消融的（配置）。我们会在每个章节的末尾分享这些结果（你会看到结论是吻合的）。</p> <p>我们的基线 1B 配置以结构化的 YAML 格式捕获了所有重要的训练细节。以下是关键部分：</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## 数据集及其混合权重</span>
<span class="na">data_stages</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">data</span><span class="pi">:</span>
    <span class="na">dataset</span><span class="pi">:</span>
      <span class="na">dataset_folder</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">fineweb-edu</span>           <span class="c1"># FineWeb 教育数据集</span>
      <span class="pi">-</span> <span class="s">stack-edu-python</span>      <span class="c1"># Python 代码数据集</span>
      <span class="pi">-</span> <span class="s">finemath-3plus</span>        <span class="c1"># 针对 3 级以上数学数据集</span>

      <span class="na">dataset_weights</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="m">0.7</span>                   <span class="c1"># 权重 70%</span>
      <span class="pi">-</span> <span class="m">0.2</span>                   <span class="c1"># 权重 20%</span>
      <span class="pi">-</span> <span class="m">0.1</span>                   <span class="c1"># 权重 10%</span>

<span class="c1">## 模型架构，Llama3.2 1B 配置</span>
<span class="na">model</span><span class="pi">:</span>
  <span class="na">model_config</span><span class="pi">:</span>
    <span class="na">hidden_size</span><span class="pi">:</span> <span class="m">2048</span>         <span class="c1"># 隐藏层大小</span>
    <span class="na">num_hidden_layers</span><span class="pi">:</span> <span class="m">16</span>     <span class="c1"># 隐藏层数量</span>
    <span class="na">num_attention_heads</span><span class="pi">:</span> <span class="m">32</span>   <span class="c1"># 注意力头数量</span>
    <span class="na">num_key_value_heads</span><span class="pi">:</span> <span class="m">8</span>    <span class="c1"># K/V 头数量 (GQA)</span>
    <span class="na">intermediate_size</span><span class="pi">:</span> <span class="m">8192</span>   <span class="c1"># 中间层大小</span>
    <span class="na">max_position_embeddings</span><span class="pi">:</span> <span class="m">4096</span>  <span class="c1"># 最大位置嵌入</span>
    <span class="na">rope_theta</span><span class="pi">:</span> <span class="m">50000.0</span>       <span class="c1"># RoPE 旋转角度</span>
    <span class="na">tie_word_embeddings</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># 绑定词嵌入</span>

<span class="c1">## 训练超参数，带余弦调度的 AdamW 优化器</span>
<span class="na">optimizer</span><span class="pi">:</span>
  <span class="na">clip_grad</span><span class="pi">:</span> <span class="m">1.0</span>              <span class="c1"># 梯度裁剪</span>
  <span class="na">learning_rate_scheduler</span><span class="pi">:</span>
    <span class="na">learning_rate</span><span class="pi">:</span> <span class="m">0.0005</span>     <span class="c1"># 最大学习率</span>
    <span class="na">lr_decay_starting_step</span><span class="pi">:</span> <span class="m">2000</span>
    <span class="na">lr_decay_steps</span><span class="pi">:</span> <span class="m">18000</span>
    <span class="na">lr_decay_style</span><span class="pi">:</span> <span class="s">cosine</span>
    <span class="na">lr_warmup_steps</span><span class="pi">:</span> <span class="m">2000</span>
    <span class="na">lr_warmup_style</span><span class="pi">:</span> <span class="s">linear</span>
    <span class="na">min_decay_lr</span><span class="pi">:</span> <span class="s">5.0e-05</span>     <span class="c1"># 最小衰减学习率</span>
  <span class="na">optimizer_factory</span><span class="pi">:</span>
    <span class="na">adam_beta1</span><span class="pi">:</span> <span class="m">0.9</span>
    <span class="na">adam_beta2</span><span class="pi">:</span> <span class="m">0.95</span>
    <span class="na">adam_eps</span><span class="pi">:</span> <span class="s">1.0e-08</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">adamW</span>

<span class="c1">## 并行化，单节点配置</span>
<span class="na">parallelism</span><span class="pi">:</span>
  <span class="na">dp</span><span class="pi">:</span> <span class="m">8</span>  <span class="c1"># 数据并行，跨 8 块 GPU</span>
  <span class="na">tp</span><span class="pi">:</span> <span class="m">1</span>  <span class="c1"># 1B 规模不需要张量或流水线并行</span>
  <span class="na">pp</span><span class="pi">:</span> <span class="m">1</span>

<span class="c1">## 分词器</span>
<span class="na">tokenizer</span><span class="pi">:</span>
  <span class="na">tokenizer_max_length</span><span class="pi">:</span> <span class="m">4096</span>
  <span class="na">tokenizer_name_or_path</span><span class="pi">:</span> <span class="s">HuggingFaceTB/SmolLM3-3B</span>

<span class="c1">## Batch size, 序列长度和总共 30B Token 的训练量</span>
<span class="na">tokens</span><span class="pi">:</span>
  <span class="na">batch_accumulation_per_replica</span><span class="pi">:</span> <span class="m">16</span> <span class="c1"># 每个副本的批次累积</span>
  <span class="na">micro_batch_size</span><span class="pi">:</span> <span class="m">3</span> <span class="c1"># GBS (全局批次大小)=dp * batch_acc* MBS * sequence=1.5M tokens</span>
  <span class="na">sequence_length</span><span class="pi">:</span> <span class="m">4096</span>
  <span class="na">train_steps</span><span class="pi">:</span> <span class="m">20000</span> <span class="c1"># GBS * 20000 = 30B</span>

<span class="nn">...</span>
</code></pre></div></div> <p>在我们的消融实验中，我们会根据测试的内容修改不同的部分，同时保持其他一切不变：测试架构选择时修改 <code class="language-plaintext highlighter-rouge">model</code> 部分，测试优化器和训练超参数时修改 <code class="language-plaintext highlighter-rouge">optimizer</code> 部分，测试数据策略时修改 <code class="language-plaintext highlighter-rouge">data_stages</code> 部分。</p> <p>运行消融实验时，某些架构更改会显著改变参数数量。例如，从 <strong>绑定（tied）</strong> 词嵌入切换到 <strong>不绑定（untied）</strong> 会使我们的嵌入参数翻倍，而从 MHA 切换到 GQA 或 MQA 则会大幅减少我们的注意力参数。为了确保公平比较，我们需要跟踪参数数量，并偶尔调整其他超参数（如隐藏层大小或层数）以<strong>保持模型大小大致相同</strong>。</p> <h4 id="332-理解效果评估至关重要">3.3.2 理解效果：评估至关重要</h4> <p>一旦我们启动了消融实验，我们如何判断哪些改动有效，哪些无效呢？</p> <p>任何训练模型的人第一直觉可能是查看 <strong>Loss 曲线</strong>，这确实很重要。你希望看到它平滑下降，没有剧烈的尖刺或不稳定。对于许多架构选择，Loss 与下游性能有很好的相关性，可能就足够了 (Y. Chen et al., 2025)。</p> <p>然而，<strong>仅看 Loss 并不总是可靠的</strong>。以<strong>数据消融实验</strong>为例，你会发现用维基百科训练比用网页训练得到<strong>更低的 Loss</strong>（预测下一个 Token 更容易），但这<strong>并不意味着你会得到一个更有能力的模型</strong>。同样，如果我们在不同训练时更改了分词器，Loss 就没有直接可比性，因为文本被分割的方式不同了。某些更改可能还会专门影响推理和数学等特定能力，而这些影响在平均 Loss 中会被稀释。最后但同样重要的一点是，<strong>模型即使在预训练 Loss 收敛后，仍可能在下游任务上持续改进</strong> (Liu et al., 2022)。</p> <p>我们需要更细致的评估，才能看清全貌并理解这些细微的影响。一个自然的方法是使用 <strong>下游评估（downstream evaluations）</strong> 来测试知识、理解、推理以及对我们重要的任何其他领域。</p> <p>对于这些消融实验，最好关注那些 <strong>能提供良好早期信号</strong> 并 <strong>避免嘈杂基准（noisy benchmarks）</strong> 的任务。在 FineTasks 和 FineWeb2 中，可靠的评估任务由四个关键原则定义：</p> <ol> <li><strong>单调性（Monotonicity）：</strong> 基准分数应随着模型训练时间的延长而持续改善。</li> <li><strong>低噪音（Low noise）：</strong> 当我们使用相同设置但不同随机种子训练模型时，基准分数不应有剧烈波动。</li> <li><strong>高于随机水平的性能（Above-random performance）：</strong> 许多能力只有在训练后期才会浮现，因此长期保持随机水平性能的任务对消融实验没有用处。例如，我们稍后将解释的多项选择格式的 MMLU 就是这种情况。</li> <li><strong>排名一致性（Ranking consistency）：</strong> 如果某种方法在早期阶段优于另一种方法，那么随着训练的继续，这种排序应该保持稳定。</li> </ol> <p>任务的质量还取决于<strong>任务表述（task formulation）</strong>（我们如何向模型提问）和<strong>指标选择（metric choice）</strong>（我们如何计算答案得分）。</p> <p>三种常见的任务表述是：<strong>多项选择格式 (MCF)、完形填空表述 (CF)</strong> 和<strong>自由形式生成 (FG)</strong>。</p> <ul> <li><strong>MCF</strong> 要求模型从 Prompt 中明确给出并标有 A/B/C/D 的选项中进行选择（例如 MMLU 的做法）。</li> <li><strong>CF</strong> 中，我们比较不同选项的似然性（Likelihood），以查看哪个更可能，而无需在 Prompt 中提供它们。</li> <li><strong>FG</strong> 中，我们查看模型对给定 Prompt 进行贪婪生成（greedy generation）的准确性。FG 需要模型具备大量的<strong>潜在知识</strong>，对于进行完整的预训练之前的短时消融实验来说，通常难度太大而没有太大用处。</li> </ul> <p>因此，在运行小型消融实验时，我们主要关注 <strong>MCF 或 CF</strong>。</p> <p>研究表明，模型在训练早期阶段难以应对 MCF，只有经过大量训练后才能掌握这项技能，这使得 <strong>CF 更适合用于获取早期信号</strong> (Du et al., 2025; Gu et al., 2025; J. Li et al., 2025)。因此，我们使用 <strong>CF</strong> 进行小型消融实验，并在主运行中集成 MCF，因为它在模型达到一定阈值、信噪比足够高时，能提供更好的中期训练信号。<strong>快速说明：</strong> 为了在 CF 这样的序列似然评估中对模型的答案进行评分，我们将准确率计算为<strong>正确答案具有最高对数概率（Log Probability）</strong>（并按字符数标准化）的问题百分比。这种标准化可以防止偏向较短的答案。</p> <p>我们的消融评估套件包括 FineWeb 消融实验中的基准，但排除了 SIQA（我们发现它噪音太大）。我们增加了 <strong>GSM8K</strong> 和 <strong>HumanEval</strong> 等数学和代码基准，以及用于长上下文消融的 <strong>RULER</strong> 长上下文基准。如下表所示，这些任务聚合在一起，以多种格式测试<strong>世界知识、推理和常识</strong>。为了加快评估速度，以牺牲一些额外噪音为代价，我们只评估每个基准的 <strong>1,000 个问题</strong>（GSM8K、HumanEval 和 RULER 除外，我们在 3B SmolLM3 消融中使用了完整集合，但在下面的 1B 实验中省略）。我们还对所有多项选择基准使用了<strong>完形填空表述（CF）</strong>的评估方式，如上所述。请注意，对于多语言消融和实际训练，我们增加了更多基准来测试多语言能力，这将在后面详述。下表总结了每个基准的关键特征：</p> <table> <thead> <tr> <th style="text-align: left">基准</th> <th style="text-align: left">领域</th> <th style="text-align: left">任务类型</th> <th style="text-align: left">问题数量</th> <th style="text-align: left">测试能力</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>MMLU</strong></td> <td style="text-align: left">知识</td> <td style="text-align: left">多项选择</td> <td style="text-align: left">14k</td> <td style="text-align: left">跨 57 个学科的广泛学术知识</td> </tr> <tr> <td style="text-align: left"><strong>ARC</strong></td> <td style="text-align: left">科学与推理</td> <td style="text-align: left">多项选择</td> <td style="text-align: left">7k</td> <td style="text-align: left">小学级别的科学推理</td> </tr> <tr> <td style="text-align: left"><strong>HellaSwag</strong></td> <td style="text-align: left">常识推理</td> <td style="text-align: left">多项选择</td> <td style="text-align: left">10k</td> <td style="text-align: left">关于日常情景的常识推理（叙事补全）</td> </tr> <tr> <td style="text-align: left"><strong>WinoGrande</strong></td> <td style="text-align: left">常识推理</td> <td style="text-align: left">二元选择</td> <td style="text-align: left">1.7k</td> <td style="text-align: left">需要世界知识的代词消解</td> </tr> <tr> <td style="text-align: left"><strong>CommonSenseQA</strong></td> <td style="text-align: left">常识推理</td> <td style="text-align: left">多项选择</td> <td style="text-align: left">1.1k</td> <td style="text-align: left">关于日常概念的常识推理</td> </tr> <tr> <td style="text-align: left"><strong>OpenBookQA</strong></td> <td style="text-align: left">科学</td> <td style="text-align: left">多项选择</td> <td style="text-align: left">500</td> <td style="text-align: left">包含推理的小学科学事实</td> </tr> <tr> <td style="text-align: left"><strong>PIQA</strong></td> <td style="text-align: left">物理常识</td> <td style="text-align: left">二元选择</td> <td style="text-align: left">1.8k</td> <td style="text-align: left">关于日常物体的物理常识</td> </tr> <tr> <td style="text-align: left"><strong>GSM8K</strong></td> <td style="text-align: left">数学</td> <td style="text-align: left">自由形式生成</td> <td style="text-align: left">1.3k</td> <td style="text-align: left">小学数学应用题</td> </tr> <tr> <td style="text-align: left"><strong>HumanEval</strong></td> <td style="text-align: left">代码</td> <td style="text-align: left">自由形式生成</td> <td style="text-align: left">164</td> <td style="text-align: left">根据文档字符串合成 Python 函数</td> </tr> </tbody> </table> <p>注意 MMLU 和 ARC 如何用多项选择来测试事实知识，GSM8K 如何需要计算数学问题的数值答案，以及 HumanEval 如何需要生成完整的 Python 代码。这种多样性确保我们在整个消融实验中测试了模型能力的各个方面。</p> <h5 id="3321-消融实验使用什么数据混合配比">3.3.2.1 消融实验使用什么数据混合配比？</h5> <p>对于<strong>架构消融实验</strong>，我们使用<strong>固定混合的高质量数据集</strong>进行训练，这些数据集能在广泛的任务上提供早期信号。我们使用了<strong>英语 (FineWeb-Edu)、数学 (FineMath) 和代码 (Stack-Edu-Python)</strong>。架构上的发现应该可以很好地外推到其他数据集和领域，包括多语言数据，因此我们可以保持我们的数据混合相对简单。</p> <p>对于<strong>数据消融实验</strong>，我们采取相反的方法：我们<strong>固定架构</strong>，并<strong>系统地改变数据混合配比</strong>，以了解不同的数据源如何影响模型性能。</p> <p>有时评估结果的差异可能很小。如果你的算力充足，可能值得用 <strong>不同的随机种子（Seeds）</strong> 重新运行相同的消融实验，以观察结果的变化程度。</p> <p>一个可靠的消融设置的真正价值，并不仅仅在于构建一个好的模型。当我们在主训练运行中不可避免地出现问题时（无论我们准备得多么充分，问题总会发生），我们希望对我们所做的每一个决定都充满信心，并能<strong>快速识别哪些组件没有经过充分测试，可能是问题的根源</strong>。这种准备工作可以节省调试时间，并 <strong>“武装”</strong> 我们未来的心理健康。</p> <h4 id="333-估算消融实验成本">3.3.3 估算消融实验成本</h4> <p>消融实验非常棒，但它们需要 GPU 时间，因此有必要理解这些实验的成本。下表显示了 SmolLM3 预训练的完整算力分解：主运行（包括偶尔的停机时间）、训练前后的消融实验，以及用于解决意外的扩展问题（迫使我们重启训练）和一些调试所花费的算力（我们将在后面详细介绍）。</p> <table> <thead> <tr> <th style="text-align: left">阶段</th> <th style="text-align: left">GPU 数量</th> <th style="text-align: left">天数</th> <th style="text-align: left">GPU-小时</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">主预训练运行</td> <td style="text-align: left">384</td> <td style="text-align: left">30</td> <td style="text-align: left">276,480</td> </tr> <tr> <td style="text-align: left">消融实验（预训练前）</td> <td style="text-align: left">192</td> <td style="text-align: left">15</td> <td style="text-align: left">69,120</td> </tr> <tr> <td style="text-align: left">消融实验（训练中）</td> <td style="text-align: left">192</td> <td style="text-align: left">10</td> <td style="text-align: left">46,080</td> </tr> <tr> <td style="text-align: left">训练重启与调试</td> <td style="text-align: left">384/192</td> <td style="text-align: left">3/4</td> <td style="text-align: left">46,080</td> </tr> <tr> <td style="text-align: left"><strong>总成本</strong></td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> <td style="text-align: left"><strong>437,760</strong></td> </tr> </tbody> </table> <p>这些数字揭示了一个重要的事实：<strong>消融实验和调试总共消耗了 161,280 个 GPU 小时，超过了我们主训练运行（276,480 个 GPU 小时）的一半成本。</strong> 在 SmolLM3 的开发过程中，我们总共进行了超过 100 次消融实验：我们在预训练消融上花费了 20 天，在中途训练消融上花费了 10 天，并花费了 7 天从意外的训练问题中恢复，导致重启和一些调试。</p> <p>这突出表明了为什么<strong>消融实验的成本必须计入你的算力预算</strong>：你需要为<strong>训练成本 + 消融成本 + 应对意外的缓冲</strong>进行规划。如果你以 SOTA 性能为目标，正在实施新的架构更改，或者还没有一个经过验证的配方，那么消融实验将成为一个<strong>主要的成本中心</strong>，而不仅仅是小实验。</p> <p>在我们进入下一节之前，让我们先确立一些每个进行实验的人都应该遵守的<strong>基本规则</strong>。</p> <h3 id="34-实验守则">3.4 实验守则</h3> <p><strong>验证你的评估套件。</strong> 在训练任何模型之前，请确保你的评估套件能够<strong>复现</strong>你将要比较的模型的已发布结果。如果任何基准测试本质上是<strong>生成式</strong>的（例如 GSM8K），请额外<strong>保持警惕</strong>，手动检查几个样本，以确保 Prompt 格式正确，并且任何后处理步骤都能提取到正确的信息。由于评估结果将指导你做出每一个决策，因此<strong>正确完成这一步对于项目的成功至关重要！</strong></p> <p><strong>测试每一个更改，无论多小。</strong> 不要低估那个看似无害的库升级，或者那个声称“只修改了两行代码”的 Commit 所带来的影响。这些微小的变化可能会引入<strong>微妙的 Bug</strong> 或<strong>性能偏移</strong>，从而污染你的结果。你需要一个在对你重要的用例上具有强大测试套件的库，以避免回归。</p> <p><strong>一次只改变一件事。</strong> 在实验之间，保持所有其他设置<strong>完全相同</strong>。有些变化可能会以意想不到的方式相互作用，所以我们首先要评估<strong>每个变化的单独贡献</strong>，然后才能尝试将它们结合起来，看看它们的总体影响。</p> <p><strong>训练足够的 Token，并使用充分的评估。</strong> 正如我们前面提到的，我们需要确保评估套件有良好的覆盖，并训练足够长的时间以获得<strong>可靠的信号</strong>。在这里“偷工减料”将导致嘈杂的结果和错误的决策。</p> <p>遵守这些规则可能会让你感觉<strong>过于谨慎</strong>，但另一种选择是花费数天时间调试<strong>神秘的性能下降</strong>，而结果发现这只是几天前一个<strong>不相关的依赖更新</strong>造成的。<strong>黄金原则：一旦你拥有一个良好的设置，任何更改都应该经过测试！</strong></p> <h2 id="4-设计模型架构">4. 设计模型架构</h2> <p>既然我们已经有了实验框架，是时候做出定义我们模型的<strong>重大决策</strong>了。我们做出的每一个选择，从模型大小到注意力机制再到分词器（Tokenizer）的选择，都会创造出各种约束和机遇，直接影响模型的训练和最终的使用。</p> <p>请记住 <strong>“训练指南针”</strong>：在做出任何技术选择之前，我们都需要对 <strong>“为什么”（Why）和“做什么”（What）</strong> 有清晰的认识。我们为什么要训练这个模型？它应该是什么样子？</p> <p>这听起来很显而易见，但正如我们在“训练指南针”中所解释的，在这里保持<strong>深思熟虑</strong>将塑造我们的决策，并避免我们在无尽的实验空间中迷失方向。我们的目标是构建一个<strong>英语 SOTA</strong> 模型吗？<strong>长上下文</strong>是我们的首要任务吗？还是我们试图验证一种<strong>新的架构</strong>？虽然所有这些情况下的训练循环可能看起来相似，但我们运行的实验以及我们接受的<strong>权衡取舍</strong>将截然不同。<strong>尽早回答这个问题</strong>有助于我们决定如何平衡我们在数据和架构工作之间的时间分配，以及在开始运行之前，要在每个方面创新多少。</p> <p>因此，让我们以身作则，回顾一下指导 SmolLM3 设计的目标。我们想要一个<strong>强大的设备端应用模型</strong>，同时具备<strong>有竞争力的多语言性能、可靠的数学和编码能力，以及强大的长上下文处理能力。</strong>正如我们前面提到的，这引导我们选择了<strong>一个 30 亿参数的密集型模型</strong>：它足够大以提供强大的能力，但又足够小以舒适地安装在手机上。考虑到边缘设备的内存限制和我们的项目时间线（大约 3 个月），我们选择了<strong>密集型 Transformer</strong>，而不是 MoE 或混合模型。</p> <p>我们从 SmolLM2 获得了一个小规模（17 亿参数）的英语训练工作“配方”，但扩大规模意味着<strong>重新验证一切</strong>，并解决<strong>多语言和扩展上下文长度</strong>等新挑战。这就是<strong>明确的目标如何塑造我们的方法</strong>的一个清晰例子。例如，在 SmolLM2 中，我们在预训练结束时努力扩展上下文长度，因此对于 SmolLM3，我们从一开始就做出了架构选择——比如使用 <strong>NoPE</strong> 和<strong>文档内掩码 (intra-document masking)</strong>（后续会详细介绍）——以最大化我们成功的几率，而事实证明这种方法奏效了。</p> <p>一旦我们的目标明确，我们就可以开始做出将目标变为现实的技术决策。在本章中，我们将系统地探讨这些核心决策：<strong>架构、数据和超参数</strong>。请将此视为我们的<strong>战略规划阶段</strong>，把这些基础工作做好，将使我们在真正的训练马拉松中避免代价高昂的错误。</p> <h3 id="41-架构选择">4.1 架构选择</h3> <p>如果你观察最近的模型，比如 Qwen3、Gemma3 或 DeepSeek v3，你会发现尽管它们存在差异，但它们都共享着同一个基础——<strong>2017 年引入的 Transformer 架构</strong>。多年来发生变化的不是基本结构，而是对其核心组件的<strong>精炼和改进</strong>。无论你是构建<strong>密集型模型（Dense Model）</strong>、<strong>专家混合模型（Mixture of Experts, MoE）</strong>还是<strong>混合架构（Hybrid Architecture）</strong>，你都在使用这些相同的构建块。</p> <p>这些改进源于各大团队对更优性能的追求，以及对特定挑战的攻克：<strong>推理时的内存限制、大规模训练时的不稳定性</strong>，或<strong>处理更长上下文的需求</strong>。一些修改，比如从<strong>多头注意力（MHA）</strong>转向计算效率更高的注意力变体，如<strong>分组查询注意力（GQA）</strong>，现已被广泛采纳。而其他修改，比如不同的<strong>位置编码方案</strong>，仍在争论之中。最终，今天的实验将凝结成明天的<strong>基线架构</strong>。</p> <p>那么，现代 LLM 今天实际在使用什么呢？让我们看看领先模型所趋同的关键点。不幸的是，并非所有模型都公开了其训练细节，但我们从 DeepSeek、OLMo、Kimi 和 SmolLM 等模型家族获得的透明度，足以让我们一窥当前的技术格局：</p> <table> <thead> <tr> <th style="text-align: left">模型</th> <th style="text-align: left">架构</th> <th style="text-align: left">参数量</th> <th style="text-align: left">训练 Token</th> <th style="text-align: left">注意力机制</th> <th style="text-align: left">上下文长度（最终）</th> <th style="text-align: left">位置编码</th> <th style="text-align: left">精度</th> <th style="text-align: left">初始化 (Std)</th> <th style="text-align: left">优化器</th> <th style="text-align: left">最大学习率</th> <th style="text-align: left">学习率调度</th> <th style="text-align: left">Warmup 步数</th> <th style="text-align: left">Batch Size</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">DeepSeek LLM 7B</td> <td style="text-align: left">Dense</td> <td style="text-align: left">7B</td> <td style="text-align: left">2T</td> <td style="text-align: left">GQA</td> <td style="text-align: left">4K</td> <td style="text-align: left">RoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">0.006</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$4.2 \times 10^{-4}$</td> <td style="text-align: left">Multi-Step</td> <td style="text-align: left">2K</td> <td style="text-align: left">9.4M</td> </tr> <tr> <td style="text-align: left">DeepSeek LLM 67B</td> <td style="text-align: left">Dense</td> <td style="text-align: left">67B</td> <td style="text-align: left">2T</td> <td style="text-align: left">GQA</td> <td style="text-align: left">4K</td> <td style="text-align: left">RoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">0.006</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$3.2 \times 10^{-4}$</td> <td style="text-align: left">Multi-Step</td> <td style="text-align: left">2K</td> <td style="text-align: left">18.9M</td> </tr> <tr> <td style="text-align: left">DeepSeek V2</td> <td style="text-align: left">MoE</td> <td style="text-align: left">236B (21B 激活)</td> <td style="text-align: left">8.1T</td> <td style="text-align: left">MLA</td> <td style="text-align: left">128K</td> <td style="text-align: left">Partial RoPE</td> <td style="text-align: left">-</td> <td style="text-align: left">0.006</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$2.4 \times 10^{-4}$</td> <td style="text-align: left">Multi-Step</td> <td style="text-align: left">2K</td> <td style="text-align: left">9.4M -&gt; 37.7M (Warmup 225B)</td> </tr> <tr> <td style="text-align: left">DeepSeek V3</td> <td style="text-align: left">MoE</td> <td style="text-align: left">671B (37B 激活)</td> <td style="text-align: left">14.8T</td> <td style="text-align: left">MLA</td> <td style="text-align: left">129K</td> <td style="text-align: left">Partial RoPE</td> <td style="text-align: left">FP8</td> <td style="text-align: left">0.006</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$2.2 \times 10^{-4}$</td> <td style="text-align: left">Multi-Step + Cosine</td> <td style="text-align: left">2K</td> <td style="text-align: left">12.6M -&gt; 62.9M (Warmup 469B)</td> </tr> <tr> <td style="text-align: left">MiniMax-01</td> <td style="text-align: left">MoE + Hybrid</td> <td style="text-align: left">456B (45.9B 激活)</td> <td style="text-align: left">11.4T</td> <td style="text-align: left">Linear + GQA</td> <td style="text-align: left">4M</td> <td style="text-align: left">Partial RoPE</td> <td style="text-align: left">-</td> <td style="text-align: left">Xavier Init + Deepnorm</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$2 \times 10^{-4}$</td> <td style="text-align: left">Multi-Step</td> <td style="text-align: left">500</td> <td style="text-align: left">16M -&gt; 32M -&gt; 64M -&gt; 128M</td> </tr> <tr> <td style="text-align: left">Kimi K2</td> <td style="text-align: left">MoE</td> <td style="text-align: left">1T (32B 激活)</td> <td style="text-align: left">15.5T</td> <td style="text-align: left">MLA</td> <td style="text-align: left">128K</td> <td style="text-align: left">Partial RoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">可能是 0.006</td> <td style="text-align: left">MuonClip</td> <td style="text-align: left">$2 \times 10^{-4}$</td> <td style="text-align: left">WSD</td> <td style="text-align: left">500</td> <td style="text-align: left">67M</td> </tr> <tr> <td style="text-align: left">OLMo 2 7B</td> <td style="text-align: left">Dense</td> <td style="text-align: left">7B</td> <td style="text-align: left">5T</td> <td style="text-align: left">MHA</td> <td style="text-align: left">4K</td> <td style="text-align: left">RoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">0.02</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$3 \times 10^{-4}$</td> <td style="text-align: left">Cosine</td> <td style="text-align: left">2K</td> <td style="text-align: left">4.2M</td> </tr> <tr> <td style="text-align: left">SmolLM3</td> <td style="text-align: left">Dense</td> <td style="text-align: left">3B</td> <td style="text-align: left">11T</td> <td style="text-align: left">GQA</td> <td style="text-align: left">128K</td> <td style="text-align: left">NoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">0.02</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$2 \times 10^{-4}$</td> <td style="text-align: left">WSD</td> <td style="text-align: left">2K</td> <td style="text-align: left">2.3M</td> </tr> </tbody> </table> <p>如果你还不理解其中的一些术语，比如 <strong>MLA、NoPE</strong> 或 <strong>WSD</strong>，请不用担心。我们将在本节中解释每一个。现在，你只需要注意其中的多样性：<strong>不同的注意力机制</strong>（MHA, GQA, MLA）、<strong>位置编码</strong>（RoPE, NoPE, partial RoPE）以及<strong>学习率调度</strong>（Cosine, Multi-Step, WSD）。</p> <p>看到如此长的架构选择清单，你可能会感到不知所措，不知道从何处开始。与大多数类似情况一样，我们将<strong>循序渐进</strong>，逐步建立所有必要的专业知识。我们将首先关注<strong>最简单的基础架构（密集型模型）</strong>，并详细研究每个架构方面。稍后，我们将深入探讨 <strong>MoE 和混合模型</strong>，并讨论何时使用它们是一个好的选择。最后，我们将探索<strong>分词器（Tokenizer）</strong>——一个经常被忽视和低估的组件。我们应该使用现有的分词器还是自己训练？我们又如何评估我们的分词器是否优秀？</p> <p>但现在，让我们从每个 LLM 的核心开始：<strong>注意力机制（Attention Mechanism）</strong>。</p> <h4 id="411-注意力机制attention">4.1.1 注意力机制（Attention）</h4> <p>Transformer 架构周围最活跃的研究领域之一就是注意力机制。虽然在预训练期间<strong>前馈层（feedforward layers）</strong>占据了大部分计算，但<strong>注意力机制</strong>在<strong>推理时</strong>成为了主要的瓶颈（尤其是在长上下文场景中）。在这种情况下，它会推高计算成本，并且 <strong>KV 缓存（KV cache）</strong>会迅速消耗 GPU 内存，从而降低吞吐量。让我们快速回顾一下主要的注意力机制，以及它们如何在<strong>容量</strong>和<strong>速度</strong>之间进行权衡。</p> <h5 id="4111-我的注意力需要多少个注意力头">4.1.1.1 我的注意力需要多少个注意力头？</h5> <p><strong>多头注意力（Multi-Head Attention, MHA）</strong> 是最初的 Transformer 引入的标准注意力机制。其核心思想是，你有 <strong>N 个注意力头</strong>，每个头独立地执行相同的检索任务：将隐藏状态转换为查询（Queries）、键（Keys）和值（Values），然后使用当前的查询来匹配键，检索出最相关的 Token，最后转发与匹配 Token 相关联的值。在推理时，我们不需要重新计算过去 Token 的 KV 值，可以直接重用它们。存储过去 KV 值的内存被称为 <strong>KV-Cache</strong>。随着上下文窗口的增长，这个缓存会迅速成为推理的瓶颈，并消耗 GPU 内存的很大一部分。以下是一个简单的计算，用于估算 Llama 3 架构在 MHA 和 8192 序列长度下的 KV-Cache 内存：</p> <p>\(s_{KV} = 2 \times n_{bytes} \times seq \times n_{layers} \times n_{heads} \times dim_{heads}\) \(= 2 \times 2 \times 8192 \times 32 \times 32 \times 128 \approx 4 \text{ GB (Llama3 8B)}\) \(= 2 \times 2 \times 8192 \times 80 \times 64 \times 128 \approx 20 \text{ GB (Llama3 70B)}\)</p> <p><em>（注意：最前面的因子 2 来自于同时存储键和值缓存。）</em></p> <p>正如你所见，缓存大小与序列长度<strong>线性</strong>增加，但上下文窗口却以<strong>指数级</strong>增长，现在已经达到了数百万 Token。因此，提高缓存的效率将使推理时的上下文扩展变得更容易。</p> <p>一个自然而然的问题是：我们真的需要为<strong>每个头</strong>都计算新的 KV 值吗？可能不需要。<strong>多查询注意力（Multi-Query Attention, MQA）</strong> (Shazeer, 2019) 和<strong>分组查询注意力（Grouped Query Attention, GQA）</strong> (Ainslie et al., 2023) 都解决了这个问题。</p> <p>最简单的情况是<strong>在所有头之间共享 KV 值</strong>，从而将 KV 缓存的大小除以 $\text{n}_{heads}$。例如，对于 Llama 3 70B，这可以带来 64 倍的减少！这就是 <strong>MQA</strong> 的思想，并被 StarCoder 等一些模型用作 MHA 的替代方案。</p> <p>然而，我们可能会因此<strong>牺牲一些我们不愿意放弃的注意力容量</strong>。因此，我们可以考虑<strong>中间立场</strong>：<strong>在分组的头之间共享 KV 值</strong>，例如 4 个头共享相同的 KV 值。这就是 <strong>GQA</strong> 的方法，它在 MQA 和 MHA 之间取得了平衡。</p> <p>最近，DeepSeek-v2（并在 v3 中使用）引入了 <strong>多潜变量注意力（Multi-Latent Attention, MLA）</strong> (DeepSeek-AI et al., 2024)，它使用了一种不同的策略来压缩缓存：<strong>它不减少 KV 值的数量，而是减少它们的尺寸</strong>，简单地存储一个<strong>潜变量（latent variable）</strong>，该变量可以在运行时解压缩为 KV 值。通过这种方法，他们成功地将缓存压缩到相当于 <strong>GQA 具有 2.25 个组</strong>的等效值，同时提供了比 MHA <strong>更强大的性能</strong>！为了让它与 RoPE 配合工作，需要一个小小的调整，即增加一个额外的微小潜向量。在 DeepSeek-v2 中，他们选择了 $4 \times \text{dim}<em>{head}$ 作为主潜变量，和 $1/2 \times \text{dim}</em>{head}$ 用于 RoPE 部分，因此总共是 $4.5 \times \text{dim}_{head}$，它同时用于 K 和 V，从而消除了最前面乘数的 2。</p> <p>下表比较了我们刚才讨论的注意力机制。为了简化，我们比较了<strong>每个 Token 使用的参数量</strong>。如果你想计算总内存，只需乘以每个参数的字节数（通常为 2）和序列长度即可：</p> <table> <thead> <tr> <th style="text-align: left">注意力机制 (Attention Mechanism)</th> <th style="text-align: left">每个 Token 的 KV-Cache 参数量</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>MHA (多头注意力)</strong></td> <td style="text-align: left">$= 2 \times n_{heads} \times n_{layers} \times dim_{head}$</td> </tr> <tr> <td style="text-align: left"><strong>MQA (多查询注意力)</strong></td> <td style="text-align: left">$= 2 \times 1 \times n_{layers} \times dim_{head}$</td> </tr> <tr> <td style="text-align: left"><strong>GQA (分组查询注意力)</strong></td> <td style="text-align: left">$= 2 \times g \times n_{layers} \times dim_{head}$ （通常 $g=2, 4, 8$）</td> </tr> <tr> <td style="text-align: left"><strong>MLA (多潜变量注意力)</strong></td> <td style="text-align: left">$= 4.5 \times n_{layers} \times dim_{head}$</td> </tr> </tbody> </table> <p>现在，让我们看看这些注意力机制在实际实验中的表现如何！</p> <h5 id="4112-消融实验---gqa-胜过-mha">4.1.1.2 消融实验 - GQA 胜过 MHA</h5> <p>在这里，我们比较了不同的注意力机制。我们的基线模型使用了 <strong>32 个查询头（Query Heads）</strong> 和 <strong>8 个 KV 头（KV Heads）</strong>，这对应于 <strong>GQA（分组查询注意力）</strong>，其比率为 32/8=4。如果我们使用 MHA，或者如果我们使用更少的 KV 头和更高的 GQA 比率，性能将如何变化？</p> <p>请注意，更改 KV 头的数量会影响参数量，特别是对于 MHA 的情况。为了保持一致性，我们<strong>调整了 MHA 运行的层数</strong>，因为它否则会有超过 1 亿参数的差异；对于其他设置，我们保持了默认的 16 层。</p> <table> <thead> <tr> <th style="text-align: left">注意力类型</th> <th style="text-align: left">查询头（Query Heads）</th> <th style="text-align: left">KV 头（KV Heads）</th> <th style="text-align: left">层数</th> <th style="text-align: left">参数量</th> <th style="text-align: left">备注</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>MQA</strong></td> <td style="text-align: left">32</td> <td style="text-align: left">1</td> <td style="text-align: left">16</td> <td style="text-align: left">1.21B</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left"><strong>GQA (比率 16)</strong></td> <td style="text-align: left">32</td> <td style="text-align: left">2</td> <td style="text-align: left">16</td> <td style="text-align: left">1.21B</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left"><strong>GQA (比率 8)</strong></td> <td style="text-align: left">32</td> <td style="text-align: left">4</td> <td style="text-align: left">16</td> <td style="text-align: left">1.22B</td> <td style="text-align: left">我们的基线</td> </tr> <tr> <td style="text-align: left"><strong>GQA (比率 4)</strong></td> <td style="text-align: left">32</td> <td style="text-align: left">8</td> <td style="text-align: left">16</td> <td style="text-align: left">1.24B</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left"><strong>GQA (比率 2)</strong></td> <td style="text-align: left">32</td> <td style="text-align: left">16</td> <td style="text-align: left">15</td> <td style="text-align: left">1.22B</td> <td style="text-align: left"><strong>减少了层数</strong></td> </tr> <tr> <td style="text-align: left"><strong>MHA</strong></td> <td style="text-align: left">32</td> <td style="text-align: left">32</td> <td style="text-align: left">14</td> <td style="text-align: left">1.20B</td> <td style="text-align: left"><strong>减少了层数</strong></td> </tr> <tr> <td style="text-align: left">GQA (比率 2)</td> <td style="text-align: left">32</td> <td style="text-align: left">16</td> <td style="text-align: left">16</td> <td style="text-align: left">1.27B</td> <td style="text-align: left"><strong>参数量过大 - 未进行消融</strong></td> </tr> <tr> <td style="text-align: left">MHA</td> <td style="text-align: left">32</td> <td style="text-align: left">32</td> <td style="text-align: left">16</td> <td style="text-align: left">1.34B</td> <td style="text-align: left"><strong>参数量过大 - 未进行消融</strong></td> </tr> </tbody> </table> <p>因此，我们比较了 MHA、MQA 和 4 种 GQA 设置（比率分别为 2、4、8、16）。</p> <p>观察消融结果，我们发现 <strong>MQA 和 GQA（16 个分组）</strong>（分别只留下 1 个和 2 个 KV 头）的性能显著<strong>低于 MHA</strong>。另一方面，<strong>GQA 配置（2、4、8 个分组）的性能大致与 MHA 持平</strong>。</p> <p>这个结果在 Loss 曲线和下游评估中都保持一致。我们在 HellaSwag、MMLU 和 ARC 等基准测试中清楚地观察到了这一点，而 OpenBookQA 和 WinoGrande 等基准则显示出少许噪音。</p> <p>基于这些消融实验，<strong>GQA 是 MHA 的一个可靠替代品</strong>。它在保持性能的同时，在推理时更加高效。一些最新的模型采用了 <strong>MLA</strong> 来实现更进一步的 KV 缓存压缩，尽管它尚未被广泛采用。由于在进行消融实验时 MLA 尚未在 nanotron 中实现，我们没有对其进行消融。</p> <p><strong>对于 SmolLM3，我们最终使用了 GQA，分组数量为 4。</strong></p> <p>除了注意力架构本身，我们在训练中使用的<strong>注意力模式（Attention Pattern）</strong>也很重要。接下来，让我们看看<strong>注意力掩码（Attention Masking）</strong>。</p> <h5 id="4113-文档掩码document-masking">4.1.1.3 文档掩码（Document Masking）</h5> <p>我们如何在训练序列中应用注意力，直接影响着计算效率和模型性能。这就引出了 <strong>文档掩码（Document Masking）</strong> 以及更广泛的问题：我们在数据加载器（dataloader）中如何构建训练样本？</p> <p>在预训练期间，我们使用<strong>固定长度的序列</strong>进行训练，但我们的文档长度是<strong>可变</strong>的。一篇研究论文可能有 10k 个 Token，而一个简短的代码片段可能只有几百个 Token。我们如何将可变长度的文档放入固定长度的训练序列中呢？</p> <p>将较短的文档填充（Padding）到目标长度会浪费算力在无意义的填充 Token 上。相反，我们使用<strong>打包（Packing）</strong>：将文档与<strong>序列结束（End-of-Sequence, EOS）Token</strong> 一起打乱并连接起来，然后将结果分割成与序列大小匹配的固定长度块。</p> <p>实际操作看起来是这样的：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>File 1: "Recipe for granola bars..." (400 tokens) &lt;EOS&gt;
File 2: "def hello_world()..." (300 tokens) &lt;EOS&gt;
File 3: "Climate change impacts..." (1000 tokens) &lt;EOS&gt;
File 4: "import numpy as np..." (3000 tokens) &lt;EOS&gt;
...

After concatenation and chunking into 4k sequences:
Sequence 1: [File 1] + [File 2] + [File 3] + [partial File 4]
Sequence 2: [rest of File 4] + [File 5] + [File 6] + ...
</code></pre></div></div> <p>如果一个文件足够长能填满我们的 4k 上下文，那么一个训练序列可能只包含一个完整文件。但在大多数情况下，文件都很短，所以序列包含了多个<strong>随机连接</strong>在一起的文件。</p> <p>在标准的 <strong>因果掩码（Causal Masking）</strong> 下，一个 Token 可以关注打包序列中的所有先前 Token。在上面的例子中，文件 4 中那个 Python 函数里的 Token 可以关注燕麦棒食谱、气候变化文章以及其他碰巧打包在一起的内容。</p> <p>让我们快速看看一个典型的 4k 预训练上下文会包含什么。一项快速分析显示，在 CommonCrawl 和 GitHub 中，<strong>绝大多数（约 80-90%）的文件都短于 2k Token</strong>。</p> <p>下方的图表检查了本文中使用的较新数据集的 Token 分布：</p> <p>这意味着，在一个 2k 或 4k 的训练序列和标准因果掩码下，<strong>绝大多数 Token 将浪费算力去关注那些被打包在一起的、不相关的文档内容。</strong></p> <p>除了计算效率低下之外，Zhao et al. (2024) 发现这种方法引入了来自不相关内容的<strong>噪音</strong>，可能<strong>降低性能</strong>。他们建议使用<strong>文档内掩码（intra-document masking）</strong>：我们修改注意力掩码，使得 Token <strong>只能关注同一文档内的先前 Token</strong>。下方可视化图（<em>注：原文指代的图表</em>）展示了这种差异。</p> <p>Zhu et al. (2025) 在 SkyLadder 中也发现了文档内掩码的类似益处，但提供了另一种解释。他们发现<strong>较短的上下文长度对训练更有利</strong>，而文档内掩码有效地<strong>降低了平均上下文长度</strong>。</p> <p>Llama 3 (Grattafiori et al., 2024) 也使用文档内掩码进行训练，他们发现在短上下文预训练期间影响有限，但对于<strong>长上下文扩展</strong>来说益处显著，因为在那种情况下注意力开销变得更加重要。此外，ProLong 论文 (Gao et al., 2025) 表明，在持续预训练中利用文档掩码来扩展 Llama 3 8B 的上下文，对<strong>长上下文和短上下文的基准都有益处</strong>。</p> <p>我们决定在我们的 1B 基线模型上进行一项消融实验，测试文档掩码是否会影响短上下文性能。你可以在这里找到配置。</p> <p>结果显示，与标准因果掩码相比，Loss 曲线和下游评估得分<strong>完全相同</strong>（如下方图表所示）。我们唯一观察到的一个微小改进是在 <strong>PIQA</strong> 上。</p> <p>要在 nanotron 中启用文档掩码，只需在模型配置中将以下标志设置为 <code class="language-plaintext highlighter-rouge">true</code>：</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">model_config</span><span class="pi">:</span>
  <span class="na">_attn_implementation</span><span class="pi">:</span> <span class="s">flash_attention_2</span>
  <span class="na">_fused_rms_norm</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">_fused_rotary_emb</span><span class="pi">:</span> <span class="kc">true</span>
<span class="pi">-</span> <span class="na">_use_doc_masking</span><span class="pi">:</span> <span class="kc">false</span>
<span class="na">+ _use_doc_masking</span><span class="pi">:</span> <span class="kc">true</span>  <span class="c1"># 启用文档内掩码</span>
</code></pre></div></div> <p>与 Llama 3 类似，我们没有在短上下文任务上观察到明显的性能影响，除了 PIQA 的微小改进。然而，文档掩码在扩展到长序列时变得<strong>至关重要</strong>，因为它可以<strong>加快训练速度</strong>。这对于我们的长上下文扩展尤其重要，我们将序列从 4k 扩展到 64k Token（详情请见“训练马拉松”章节）。因此，我们在 SmolLM3 的整个训练过程中都采用了它。</p> <p>在本节中，我们讨论了注意力如何处理序列。现在，让我们看看 Transformer 中的另一个主要参数块：<strong>嵌入层（Embeddings）</strong>。</p> <p>好的，这是关于“嵌入层共享”和“消融实验”部分的翻译和知乎风格的润色：</p> <h4 id="412-嵌入层共享embedding-sharing">4.1.2 嵌入层共享（Embedding Sharing）</h4> <p>如果你观察我们基线消融模型的配置，你会发现与标准 Transformer 不同的地方之一是，它通过 <strong><code class="language-plaintext highlighter-rouge">tie_word_embeddings</code></strong> 标志启用了<strong>嵌入层共享</strong>。</p> <p>LLM 有两个嵌入组件：</p> <ol> <li><strong>输入嵌入（Input Embeddings）：</strong> 作为 Token 到向量的查找表（大小为 $\text{vocab_size} \times \text{hidden_dim}$）。</li> <li><strong>输出嵌入（Output Embeddings）：</strong> 最后一个线性层，将隐藏状态映射到词汇表 Logits（大小为 $\text{hidden_dim} \times \text{vocab_size}$）。</li> </ol> <p>在经典情况下，如果这两个矩阵是分开的，总嵌入参数量为 $2 \times \text{vocab_size} \times \text{hidden_dim}$。因此，在小型语言模型中，嵌入层可以占据总参数量的很大一部分，尤其是在词汇表大小很大的情况下。这使得<strong>嵌入层共享</strong>（在输出层重用输入嵌入）成为小型模型的一种自然优化。</p> <p>大型模型通常不使用这种技术，因为嵌入层只占其参数预算的一小部分。例如，Llama 3.2 8B 中不共享的总嵌入参数仅占 13%，而在 Llama 3.1 70B 中仅占 3%。</p> <h5 id="4121-消融实验---绑定嵌入的模型可媲美参数量更大的非绑定变体">4.1.2.1 消融实验 - 绑定嵌入的模型可媲美参数量更大的非绑定变体</h5> <p>现在我们将评估<strong>嵌入层共享</strong>对我们消融模型的影响。我们借鉴了 <strong>MobileLLM</strong> 在 <strong>125M 规模</strong>上对该技术的全面消融实验所获得的洞察，该实验表明：共享嵌入在<strong>参数量减少 11.8%</strong> 的同时，<strong>准确率退化极小</strong>。</p> <p>由于非绑定嵌入会将我们的参数量从 1.2B 增加到 1.46B，我们将训练另一个具有非绑定参数但<strong>层数更少</strong>的模型，使其参数量与基线的 1.2B 相匹配。我们将比较三个模型：</p> <ol> <li><strong>基线模型 (1.2B)：</strong> 绑定嵌入（16 层）。</li> <li><strong>非绑定-减少层数模型 (1.2B)：</strong> 非绑定嵌入，但层数更少（12 层）以保持相同的参数预算。</li> <li><strong>非绑定-相同层数模型 (1.46B)：</strong> 非绑定嵌入，层数与基线相同（16 层），作为额外的参考点。</li> </ol> <p>Loss 和评估结果表明，我们的 <strong>1.2B 绑定嵌入基线模型</strong>，在所有基准测试（WinoGrande 除外）上，都实现了与 <strong>1.46B 非绑定等效模型</strong>相当的性能，尽管其参数少了 <strong>18%</strong>。</p> <p>而<strong>非绑定嵌入且层数减少的 1.2B 模型</strong>（12 层对比 16 层）<strong>性能不如前两者</strong>，表现出更高的 Loss 和更低的下游评估分数。这表明，在参数预算相等的情况下，<strong>增加模型深度</strong>比<strong>解绑嵌入层</strong>带来了更大的益处。</p> <p>基于这些结果，我们为 <strong>SmolLM3 3B 模型</strong>保留了<strong>绑定嵌入（Tied Embeddings）</strong>。</p> <p>至此，我们探索了嵌入层共享策略及其权衡。但仅靠嵌入层本身并不能捕获序列中 Token 的顺序；提供这些信息是<strong>位置编码（Positional Encodings）</strong>的作用。在下一节中，我们将探讨位置编码策略是如何演变的，从标准的 RoPE 到像 <strong>NoPE (No Position Embedding)</strong> 这样更新颖的方法，后者能更有效地进行长上下文建模。</p> <h4 id="413-位置编码与长上下文positional-encodings--long-context">4.1.3 位置编码与长上下文（Positional Encodings &amp; Long Context）</h4> <p>当 Transformer 处理文本时，它们面临一个根本性的挑战：它们天生对词序没有感知，因为它们通过并行注意力操作同时处理整个序列。这使得训练高效，但产生了一个问题。在没有明确的位置信息的情况下，从模型的角度来看，“亚当击败了穆恩”和“穆恩击败了亚当”看起来是相似的。</p> <p>解决方案是<strong>位置嵌入（Positional Embeddings）</strong>：一种数学编码，赋予序列中的每个 Token 一个独特的“地址”。但是，随着我们不断将上下文推向<strong>更长</strong>——从早期 BERT 的 512 个 Token 到今天的百万级 Token 模型——位置编码的选择对于<strong>性能和计算效率</strong>变得越来越关键。</p> <h5 id="4131-位置编码的演变">4.1.3.1 位置编码的演变</h5> <p>早期的 Transformer 使用简单的<strong>绝对位置嵌入（Absolute Position Embeddings, APE）</strong> (Vaswani et al., 2023)，它本质上是一个学习到的查找表，将每个位置 $(1, 2, 3…)$ 映射到一个向量，然后将其添加到 Token 嵌入中。这对于短序列运行良好，但有一个主要限制：模型的最大输入序列长度被限制在它所训练的最大长度内。它们不具备<strong>开箱即用</strong>的泛化到更长序列的能力。</p> <p>该领域转向了<strong>相对位置编码（Relative Position Encodings）</strong>，它捕获的是 Token 之间的<strong>距离</strong>，而不是它们的绝对位置。这在直觉上是合理的：两个词相隔 3 个位置，比它们是处于位置 (5, 8) 还是 (105, 108) 更重要。</p> <p><strong>ALiBi (Attention with Linear Biases)</strong> (Press et al., 2022) 通过 Token 距离修改注意力得分。两个 Token 距离越远，它们的注意力得分就会通过应用于注意力权重的简单线性偏差受到越大的惩罚。</p> <p>但主导近期大型语言模型的技术是<strong>旋转位置嵌入（Rotary Position Embedding, RoPE）</strong> (Su et al., 2023)。</p> <h5 id="4132-rope将位置编码为旋转">4.1.3.2 RoPE：将位置编码为旋转</h5> <p>RoPE 的核心洞察是：将位置信息编码为<strong>高维空间中的旋转角度</strong>。RoPE 不是将位置向量添加到 Token 嵌入中，而是通过<strong>依赖于其绝对位置的角度</strong>来旋转查询（Query）和键（Key）向量。</p> <p>其直觉是，我们将嵌入向量中的每对维度视为圆上的坐标，并根据以下因素确定的角度旋转它们：</p> <ul> <li>Token 在序列中的位置 $p$。</li> <li>我们正在处理的维度对 $k$（不同的维度对以不同的频率旋转，这些频率是基础/参考频率的指数）。</li> </ul> <p><em>(此处省略了原文提供的 RoPE 简化 Python 代码，以保持文章的可读性，读者可以直接查阅原文代码。)</em></p> <p>这个代码可能看起来复杂，所以让我们用一个具体的例子来分解它。考虑句子 “The quick brown fox” 中的“fox”这个词。在我们 1B 基线模型中，每个注意力头都使用一个 64 维的查询/键向量。RoPE 将这个向量分成 32 对：$(x_1, x_2), (x_3, x_4), (x_5, x_6)$，依此类推。我们对对进行操作，因为我们在二维空间中围绕圆旋转。为了简单起见，我们关注第一对 $(x_1, x_2)$。词“fox”出现在句子中的位置 3，因此 RoPE 将旋转第一对维度：</p> \[\text{rotation\_angle} = \text{position} \times \theta_0 = 3 \times \left(1 / 10000^{(0/32)}\right) = 3 \times 1.0 = 3.0 \text{ 弧度} = 172^{\circ}\] <p>我们的基础频率是 10000，但对于第一对维度 ($k=0$)，指数为零，因此基础频率不影响计算（我们将其提高到 0 次方）。</p> <p>现在，当两个 Token 通过注意力相互作用时，奇迹发生了。它们的旋转表示之间的<strong>点积（Dot Product）</strong> 直接通过它们旋转角度之间的<strong>相位差（Phase Difference）</strong> 来编码它们的相对距离（其中 $m$ 和 $n$ 是 Token 的位置）：</p> \[\text{dot\_product}(\text{RoPE}(x, m), \text{RoPE}(y, n)) = \sum_{k} \left[x_k \times y_k \times \cos((m-n) \times \theta_k)\right]\] <p>注意力模式<strong>仅取决于 $(m-n)$</strong>，因此相隔 5 个位置的 Token 将始终具有相同的角度关系，无论它们在序列中的绝对位置如何。因此，模型学习了<strong>基于距离的模式</strong>，这些模式适用于序列中的任何绝对位置，并可以<strong>外推到更长的序列</strong>。</p> <h5 id="4133-如何设置-rope-频率">4.1.3.3 如何设置 RoPE 频率？</h5> <p>在实践中，大多数 LLM 预训练都是从相对较短的上下文长度（2K-4K Token）开始的，使用的 RoPE 基础频率是几万，例如 $10\text{K}$ 或 $50\text{K}$。从一开始就用非常长的序列进行训练，由于注意力机制的<strong>二次方缩放</strong>以及<strong>长上下文数据</strong>（上下文长度超过 4K 的样本）的有限可用性，计算成本会非常高昂。研究还表明，这可能会<strong>损害短上下文性能</strong> (Zhu et al., 2025)。模型通常从学习词语之间的短程相关性开始，因此长序列帮助不大。</p> <p>典型的方法是<strong>用较短的序列完成大部分预训练</strong>，然后进行<strong>持续预训练（Continual Pretraining）</strong>，或在最后几千亿 Token 上使用<strong>更长的序列</strong>。然而，随着序列长度的增长，与 Token 位置成正比的旋转角度也会增长，这可能导致<strong>远处 Token 的注意力得分衰减过快</strong> (Rozière et al., 2024; Xiong et al., 2023)：</p> \[\theta = \text{position} \times 1 / (\text{base}^{(k/(\text{dim}/2))})\] <p>解决方案是，随着序列长度的增加而<strong>增加基础频率</strong>，以防止这种衰减，使用诸如 <strong>ABF</strong> 和 <strong>YaRN</strong> 之类的方法。</p> <ul> <li> <p><strong>RoPE ABF (RoPE with Adjusted Base Frequency)</strong> (Xiong et al., 2023b)：通过<strong>增加 RoPE 公式中的基础频率</strong>来解决长上下文中的注意力衰减问题。这种调整减慢了 Token 位置之间的旋转角度，防止了远处 Token 的注意力得分衰减过快。ABF 可以单阶段应用（直接提升频率）或多阶段应用（随着上下文增长而逐渐增加）。该方法易于实现，并以增加的粒度分布嵌入向量，使模型更容易区分远距离位置。虽然简单有效，但 ABF 对所有维度的统一缩放可能不适用于<strong>极长上下文</strong>。</p> </li> <li> <p><strong>YaRN (Yet another RoPE extensioN)</strong> (Peng et al., 2023)：采取了一种更复杂的方法，通过使用<strong>斜坡或缩放函数</strong>在 RoPE 维度上<strong>不均匀地插值频率</strong>。与 ABF 的统一调整不同，YaRN 对不同的频率分量应用不同的缩放因子，从而优化了扩展的上下文窗口。它包括动态注意力缩放和注意力 Logits 中的温度调整等额外技术，有助于在极大的上下文尺寸下保持性能。YaRN 支持高效的 <strong>“短训练，长测试”策略</strong>，只需要更少的 Token 和更少的微调即可实现稳健的外推。尽管比 ABF 更复杂，但 YaRN 通过提供更平滑的缩放和减轻灾难性的注意力损失，通常为<strong>极长上下文</strong>带来更好的经验性能。它也可以单独在推理中使用，无需任何微调。</p> </li> </ul> <p>这些频率调整方法<strong>减缓了注意力得分衰减效应</strong>，并保持了远程 Token 的贡献。例如，Qwen3 的训练就涉及在将序列长度从 4k 扩展到 32k 上下文时，使用 ABF 将频率从 10k 增加到 1M（该团队随后应用 YaRN 来达到 131k，即 4 倍外推）。</p> <p>请注意，对于最佳值目前没有强烈的共识，通常最好在<strong>上下文扩展阶段</strong>尝试不同的 RoPE 值，以找到最适合你特定设置和评估基准的值。</p> <p>今天大多数主要的模型都使用 <strong>RoPE</strong>：Llama、Qwen、Gemma 等等。该技术已被证明在不同模型大小和架构（密集型、MoE、混合型）中都稳健可靠。</p> <h5 id="4134-混合位置编码方法hybrid-positional-encoding-approaches">4.1.3.4 混合位置编码方法（Hybrid Positional Encoding Approaches）</h5> <p>然而，随着模型推向越来越大的上下文 (Meta AI, 2025; Yang et al., 2025)，即使是 RoPE 也开始遇到性能挑战。当在比 <strong>Needle in the Haystack (NIAH)</strong> (Kamradt, 2023) 更具挑战性的长上下文基准（如 Ruler 和 HELMET (Hsieh et al., 2024; Yen et al., 2025)）上进行评估时，在长上下文扩展期间增加 RoPE 频率的标准方法存在局限性。一些更新的技术被引入来提供帮助。</p> <p>我们以 Transformer 需要位置信息来理解 Token 顺序开始本节，但最近的研究挑战了这一假设。<strong>如果明确的位置编码根本不是必需的呢？</strong></p> <p><strong>NoPE (No Position Embedding)</strong> (Kazemnejad et al., 2023) 在<strong>没有任何明确位置编码</strong>的情况下训练 Transformer，允许模型通过<strong>因果掩码和注意力模式</strong>隐式学习位置信息。作者表明，与 ALiBi 和 RoPE 相比，这种方法表现出更好的<strong>长度泛化能力</strong>。由于没有明确的位置编码来外推训练长度之外，NoPE 自然可以处理更长的上下文。然而在实践中，与 RoPE 相比，NoPE 模型在<strong>短上下文推理和知识任务上表现较弱</strong> (Yang et al.)。这表明，虽然明确的位置编码可能会限制外推，但它们为训练上下文长度内的任务提供了有用的<strong>归纳偏置（inductive biases）</strong>。</p> <p><strong>RNoPE 混合方法：</strong> 考虑到这些权衡，B. Yang et al. (2025) 提出结合不同的位置编码策略可能很有趣。他们引入了 <strong>RNoPE</strong>，它在整个模型中交替使用 <strong>RoPE 和 NoPE 层</strong>。RoPE 层提供明确的位置信息，并以近因偏置（recency bias）处理局部上下文，而 NoPE 层则改善了跨长距离的信息检索。这项技术最近被用于 <strong>Llama 4、Command A 和 SmolLM3</strong>。</p> <h5 id="4135-消融实验---nope-在短上下文上与-rope-匹配">4.1.3.5 消融实验 - NoPE 在短上下文上与 RoPE 匹配</h5> <p>让我们测试一下混合的 NoPE 方法。我们将比较一个<strong>纯 RoPE 1B 消融基线</strong>、一个<strong>每隔 4 层移除位置编码的 NoPE 变体</strong>，以及<strong>结合 NoPE 和文档掩码</strong>的第三种配置来测试这些技术之间的相互作用。我们的基本问题是：<strong>我们能否在保持强大的短上下文性能的同时，获得长上下文能力？</strong></p> <p>Loss 和评估结果显示，所有三种配置的性能<strong>相似</strong>，这表明 NoPE <strong>保持了强大的短上下文能力</strong>，同时为更好的长上下文处理提供了基础。基于这些结果，我们为 <strong>SmolLM3 采用了 NoPE + 文档掩码的组合</strong>。</p> <p><strong>部分/分数 RoPE（Partial/Fractional RoPE）：</strong> 另一个互补的想法是<strong>只在模型维度的一个子集上应用 RoPE</strong>。与 RNoPE 在整个层面上交替使用 RoPE 和 NoPE 不同，Partial RoPE 在<strong>同一层内混合</strong>它们。最近的模型，如 GLM-4.5 (5 Team et al., 2025) 或 Minimax-01 (MiniMax et al., 2025)，采用了这种策略，但这在 gpt-j (Wang &amp; Komatsuzaki, 2021) 等较旧的模型中也存在。你也会在每个使用 <strong>MLA</strong> 的模型中看到这一点，因为它是拥有合理推理成本的<strong>必备条件</strong>。</p> <h5 id="4136-限制注意力范围以实现长上下文">4.1.3.6 限制注意力范围以实现长上下文</h5> <p>到目前为止，我们探索了如何处理长上下文的位置信息：启用 RoPE、禁用它 (NoPE)、在某些层上部分应用 (RNoPE) 或在某些隐藏维度上应用 (Partial RoPE)，或调整其频率 (ABF, YaRN)。这些方法修改了模型编码位置的方式，以处理比训练期间看到的序列更长的序列。</p> <p>但还有一种<strong>互补的策略</strong>：<strong>我们可以限制哪些 Token 相互关注，而不是调整位置编码。</strong></p> <p>为了理解为什么这很重要，考虑一个用 8 个 Token 序列预训练的模型。在推理时，我们想处理 16 个 Token（超过训练长度）。位置 8-15 超出了模型位置编码的分布范围。虽然像 RoPE ABF 这样的技术通过调整位置频率来解决这个问题，但<strong>注意力范围方法</strong>采取了不同的方法：它们<strong>策略性地限制</strong>哪些 Token 可以相互关注，将注意力模式保持在熟悉的范围内，同时仍然处理整个序列。这降低了计算成本和内存需求。</p> <p>下方的图表比较了处理我们的 16 个 Token 序列（预训练窗口为 8）的五种策略： </p> <ul> <li> <p><strong>分块注意力（Chunked Attention）</strong> 将序列分成固定大小的块，其中 Token <strong>只能在自己的块内关注</strong>。在我们的示例中，16 个 Token 被分成两个 8 个 Token 的块（0 到 7 和 8 到 15），每个 Token 只能看到其块内的其他 Token。注意 Token 8 到 15 根本不能关注到前面的块。这创建了在块边界重置的<strong>隔离注意力窗口</strong>。Llama 4 (Meta AI, 2025) 在 RoPE 层（四分之三的解码器层）中使用了 8192 个 Token 块的分块注意力，而 NoPE 层则保持对完整上下文的访问。这通过限制每层的 KV 缓存大小来减少内存需求，尽管它意味着 Token 不能关注到前面的块，这可能会影响某些长上下文任务。</p> </li> <li> <p><strong>滑动窗口注意力（Sliding Window Attention, SWA）</strong>，由 Mistral 7B (Child et al., 2019; Jiang et al., 2023) 推广，基于<strong>最近的 Token 最相关</strong>的直觉，采用了不同的方法。每个 Token 只关注<strong>最近的 N 个 Token</strong>，而不是硬性的块边界。在图表中，每个 Token 最多可以看到前面 8 个位置，创建了一个在序列中连续移动的<strong>滑动窗口</strong>。注意 Token 15 可以关注位置 8 到 15，而 Token 10 关注位置 3 到 10。窗口向前滑动，在整个序列中保持局部上下文，没有分块的人为障碍。Gemma 3 将 SWA 与完整注意力在交替层中结合使用，类似于混合位置编码方法混合不同策略的方式。</p> </li> <li> <p><strong>双块注意力（Dual Chunk Attention, DCA）</strong> (An et al., 2024) 是一种<strong>免训练</strong>的方法，它扩展了分块注意力，同时保持了跨块的信息流。在我们的示例中，我们使用块大小 $s=4$，将 16 个 Token 分为 4 个块（沿对角线可视化 $4 \times 4$ 方格）。DCA 结合了三种机制：(1) <strong>块内注意力</strong>，Token 在其块内正常关注（对角线模式）。(2) <strong>块间注意力</strong>，查询使用位置索引 $c-1=7$ 来关注前面的块，创建相对位置上限为 7。(3) <strong>连续块注意力</strong>，具有局部窗口 $w=3$，保留相邻块之间的局部性。这使得所有相对位置都保持在训练分布内（0 到 7），同时保持跨块边界的平滑过渡。DCA 使 Qwen 2.5 等模型能够在推理时支持高达 <strong>100 万 Token</strong> 的超长上下文窗口，而无需在百万 Token 序列上进行持续训练。</p> </li> </ul> <h5 id="4137-注意力汇聚attention-sinks">4.1.3.7 注意力汇聚（Attention Sinks）</h5> <p>在具有长上下文的 Transformer 模型中，出现了一种有趣的现象：模型会为序列中的<strong>起始 Token 分配异常高的注意力分数</strong>，即使这些 Token 在语义上并不重要。这种行为被称为<strong>注意力汇聚（Attention Sinks）</strong> (Xiao et al.)。这些起始 Token 充当了注意力分布的<strong>稳定机制</strong>，起到了注意力可以积累的“汇聚点”作用。</p> <p>实际的洞察是：当上下文长度超过缓存大小时，<strong>仅保留起始几个 Token 的 KV 缓存</strong>以及<strong>最近 Token 的滑动窗口</strong>，可以在很大程度上<strong>恢复性能</strong>。这种简单的修改使模型能够处理更长的序列，而无需微调或性能下降。</p> <p>现代的实现以不同的方式利用注意力汇聚。最初的研究建议在预训练期间<strong>添加一个专用的占位符 Token</strong> 作为明确的注意力汇聚点。最近，像 <strong>gpt-oss</strong> 这样的模型将注意力汇聚实现为<strong>学习到的“每头偏差 Logits”（learned per-head bias logits）</strong>，将其附加到注意力分数上，而不是作为输入序列中的实际 Token。这种方法在不修改分词输入的情况下达到了相同的稳定效果。</p> <p>有趣的是，gpt-oss 还在注意力层本身使用了<strong>偏差单元（bias units）</strong>，这是自 GPT-2 以来就很少见的设计选择。虽然这些偏差单元通常被认为对于标准注意力操作是多余的（Dehghani et al. 的实证结果显示对测试 Loss 的影响很小），但它们可以服务于<strong>实现注意力汇聚的专门功能</strong>。</p> <p>核心洞察是：无论是作为<strong>特殊 Token、学习到的偏差</strong>还是<strong>每头 Logits</strong> 实现，注意力汇聚都为长上下文场景中的注意力分布提供了一个<strong>稳定的“锚点”</strong>，允许模型存储关于<strong>整个序列的通用有用信息</strong>，即使上下文任意增长。</p> <p>至此，我们已经涵盖了注意力的核心组件：平衡内存和计算的不同头部配置（MHA、GQA、MLA），帮助模型理解 Token 顺序的位置编码策略（RoPE、NoPE 及其变体），以及使长上下文变得可处理的注意力范围技术（滑动窗口、分块和注意力汇聚）。我们还研究了嵌入层应如何配置和初始化。这些架构选择定义了你的模型如何处理和表示序列。</p> <p>但拥有正确的架构只是成功的一半。即使是精心设计的模型，也可能遭受训练不稳定的困扰，尤其是在大规模训练时。让我们来看看有助于保持训练稳定的技术。</p> <h4 id="414-提高稳定性improving-stability">4.1.4 提高稳定性（Improving Stability）</h4> <p>现在，让我们转向 LLM 预训练中最大的挑战之一：<strong>不稳定性（Instabilities）</strong>。这些问题通常表现为 Loss 尖刺或训练 Loss 的突然跳跃，在大规模训练时尤为常见。</p> <p>虽然我们将在“训练马拉松”部分深入探讨不同类型的尖刺以及如何处理它们（深入研究浮点精度、优化器和学习率），但某些架构和训练技术也可以帮助我们减少不稳定性。因此，让我们花点时间在这里研究它们。</p> <p>我们将介绍最近大规模训练运行中（例如，Olmo2 (OLMo et al., 2025) 和 Qwen3 (A. Yang, Li, et al., 2025)）用于提高稳定性的几种简单技术：<strong>Z-Loss、从嵌入层中移除权重衰减</strong>和 <strong>QK-norm</strong>。</p> <h5 id="4141-z-loss">4.1.4.1 Z-loss</h5> <p><strong>Z-loss</strong> (Chowdhery et al., 2022) 是一种正则化技术，它通过<strong>在损失函数中添加一个惩罚项</strong>来防止最终输出的 Logits 变得过大。这种正则化促使 Softmax 的分母（即 $Z$ 项）保持在一个合理的范围内，这有助于在训练过程中保持<strong>数值稳定性</strong>。</p> <p>下方在我们的 1B 模型上进行的消融结果（<em>注：指代原文图表</em>）显示，添加 Z-loss 不会影响训练 Loss 或下游性能。对于 SmolLM3，我们最终没有使用它，因为在开始训练时，我们的 Z-loss 实现引入了一些<strong>我们没有优化的训练开销</strong>。</p> <h5 id="4142-从嵌入层中移除权重衰减">4.1.4.2 从嵌入层中移除权重衰减</h5> <p>权重衰减（Weight Decay）通常作为一种正则化技术应用于所有模型参数，但 OLMo et al. (2025) 发现，<strong>将嵌入层排除在权重衰减之外可以提高训练稳定性</strong>。</p> <p>其推理是：权重衰减会导致嵌入层范数在训练过程中逐渐减小，这可能导致早期层中的<strong>梯度变大</strong>，因为层归一化（Layer Normalization）的雅可比矩阵与输入范数成<strong>反比</strong> (Takase et al., 2025)。</p> <p>我们通过训练三种配置来测试这种方法：</p> <ol> <li><strong>基线模型：</strong> 使用标准权重衰减。</li> <li><strong>变体模型：</strong> 嵌入层无权重衰减。</li> <li><strong>组合模型：</strong> 结合我们所有已采纳的更改（嵌入层无权重衰减 + NoPE + 文档掩码），以确保技术之间没有负面相互作用。</li> </ol> <p>Loss 曲线和评估结果在所有三种配置中<strong>几乎相同</strong>。因此，我们在 SmolLM3 训练中<strong>采用了所有这 3 项更改</strong>。</p> <h5 id="4143-qk-norm">4.1.4.3 QK-norm</h5> <p><strong>QK-norm</strong> (Dehghani et al., 2023) 在计算注意力之前，<strong>对查询（Query）和键（Key）向量同时应用层归一化（Layer Normalization）</strong>。这项技术有助于防止注意力 Logits 变得过大，并被许多最近的模型用于提高稳定性。</p> <p>然而，B. Yang et al. (2025) 发现 <strong>QK-norm 会损害长上下文任务</strong>。他们的分析显示，QK-norm 导致相关 Token（即“针”）上的注意力质量降低，而<strong>不相关上下文上的注意力质量增高</strong>。他们认为发生这种情况是因为归一化操作消除了<code class="language-plaintext highlighter-rouge">query-key</code>点积中的<strong>幅度信息（magnitude information）</strong>，这使得注意力 Logits 的幅度更接近。</p> <p>由于这个原因，我们<strong>没有在 SmolLM3 中使用 QK-norm</strong>。此外，作为一个小的 30 亿参数模型，与那些 QK-norm 已被证明最有益处的更大模型相比，它面临的训练不稳定性风险也较小。</p> <h4 id="415-其他核心组件other-core-components">4.1.5 其他核心组件（Other Core Components）</h4> <p>除了我们已经涵盖的组件之外，还有一些其他值得一提的架构决策，以求完整性。</p> <ul> <li> <p><strong>参数初始化（Initialization）：</strong> 现代模型通常使用<strong>截断正态分布初始化</strong>（均值=0，标准差 $\text{std}=0.02$ 或 $\text{std}=0.006$），或者像 <strong>$\mu\text{P}$</strong> (G. Yang &amp; Hu, 2022) 这样的初始化方案，例如 Cohere 的 Command A (Cohere et al., 2025)。这也可以是另一个消融实验的主题。</p> </li> <li> <p><strong>激活函数（Activation Functions）：</strong> <strong>SwiGLU</strong> 已成为现代 LLM 中的事实标准（除了使用 GeGLU 的 Gemma2 和使用 $\text{relu}^2$ 的 Nvidia (Nvidia et al., 2024; NVIDIA et al., 2025)），取代了像 ReLU 或 GELU 这样较旧的选择。</p> </li> <li> <p><strong>架构布局（Architectural Layout）：</strong> 从更宏观的层面看，架构布局的选择也对模型的行为起着作用。尽管<strong>总参数数量</strong>在很大程度上决定了语言模型的容量，但这些参数如何分布在<strong>深度（depth）和宽度（width）</strong>上也至关重要。Petty et al. (2024) 发现，在语言建模和组合性任务上，<strong>更深的模型</strong>比参数量相等的<strong>更宽的模型</strong>表现更优，直到这种益处达到饱和。<strong>“深而薄”</strong>的策略在 MobileLLM 的子十亿参数 LLM 消融实验中表现良好 (Z. Liu et al., 2024)，而<strong>更宽的模型</strong>由于具有更大的并行性，往往能提供更快的推理速度。现代架构以不同的方式体现了这种权衡取舍，正如本文所指出的。</p> </li> </ul> <p>至此，我们已经涵盖了值得为你的训练运行进行优化的<strong>密集型 Transformer 架构</strong>的<strong>最重要方面</strong>。</p> <p>然而，最近也出现了涉及模型整体的其他架构干预，即 <strong>MoE（专家混合）模型</strong>和<strong>混合（Hybrid）模型</strong>。让我们来看看它们能提供什么，从 MoE 开始。</p> <h4 id="416-走向稀疏专家混合模型moe">4.1.6 走向稀疏：专家混合模型（MoE）</h4> <p>专家混合模型（MoE）的直觉是：我们不需要为了每一个 Token 的预测都用到整个模型，这类似于我们的大脑会根据手头的任务激活不同的区域（例如视觉或运动皮层）。对于一个 LLM 来说，这意味着模型在执行翻译任务时，那些学习了代码语法的组件就不需要被使用。如果能做好这一点，就意味着我们可以节省大量的计算资源，因为在推理时我们只需要运行<strong>部分模型</strong>。</p> <p>在技术层面上，MoE 的目标很简单：<strong>增加总参数量，同时不增加每个 Token 的“活跃”参数量</strong>。简化来说，<strong>总参数量</strong>影响模型的总体学习容量，而<strong>活跃参数量</strong>决定了训练成本和推理速度。这就是为什么现在许多前沿系统（例如 DeepSeek V3、K2，以及 Gemini、Grok 等闭源模型）都在使用 MoE 架构。如果你是第一次接触 MoE，不用担心，其机制并不复杂。让我们从标准的密集型架构开始，看看 MoE 所需的必要改变。在 MoE 中，我们将单个 MLP 替换为多个 MLP（<strong>“专家 Experts”</strong>），并在这些 MLP 之前添加一个<strong>可学习的路由器（Router）</strong>。对于每个 Token，路由器会选择一小部分专家来执行计算。<strong>总参数量</strong>和<strong>活跃参数量</strong>的区别就源于此：模型拥有很多专家，但任何给定的 Token 只使用其中少数几个。</p> <p>设计 MoE 层会引出几个核心问题：</p> <ol> <li><strong>专家形态与稀疏度：</strong> 你应该使用<strong>许多小型专家</strong>还是<strong>少数大型专家</strong>？每个 Token 应该有多少专家是<strong>活跃的</strong>？你总共需要多少专家（即<strong>稀疏度</strong>或“top-k”）？是否应该让某些专家成为 <strong>通用型（universal）</strong> 专家而始终保持活跃？</li> <li><strong>利用率与专业化：</strong> 如何选择路由的专家，并确保它们被充分利用（避免闲置容量），同时鼓励它们实现<strong>专业化</strong>？在实践中，这是一个 <strong>负载均衡（load-balancing）</strong> 问题，对训练和推理效率有着重要影响。</li> </ol> <p>在这里，我们专注于一个目标：<strong>给定固定的计算预算，我们如何选择一个能将 Loss 降到最低的 MoE 配置？</strong> 这是一个不同于纯系统效率（吞吐量/延迟）的问题，我们稍后会再讨论后者。本节的大部分内容遵循蚂蚁集团 MoE 扩展定律论文 (Tian et al., 2025) 中的分析。我们将使用他们提出的<strong>效率杠杆（Efficiency Leverage, EL）</strong>的概念。简单来说，EL 衡量了你需要多少<strong>密集型计算</strong>才能匹配 MoE 设计所达到的 Loss，衡量单位是 <strong>FLOPs</strong>。<strong>更高的 EL</strong> 意味着与密集型训练相比，该 MoE 配置每单位计算能带来更多的 Loss 改进。让我们仔细看看如何设置 MoE 的稀疏度来提高效率杠杆。</p> <h5 id="4161-稀疏度激活比率sparsity--activation-ratio">4.1.6.1 稀疏度/激活比率（Sparsity / Activation Ratio）</h5> <p>在本节中，我们想找出哪种 MoE 设置是最佳的。渐进地来看，很容易看出两个极端情况都不是理想的设置：</p> <ul> <li>一方面，始终激活所有专家会使我们回到<strong>密集型设置</strong>，即所有参数始终被使用。</li> <li>另一方面，如果活跃参数非常少（极端情况下只激活 1 个参数），显然不足以解决任务，即使是在一个狭窄的领域。</li> </ul> <p>因此，我们显然需要找到一个<strong>中间点</strong>。在我们深入寻找最佳设置之前，定义两个量是有用的：<strong>激活比率（activation ratio）</strong>及其倒数<strong>稀疏度（sparsity）</strong>：</p> \[\text{激活比率} = \frac{\text{激活的专家数量}}{\text{总专家数量}}\] \[\text{稀疏度} = \frac{\text{总专家数量}}{\text{激活的专家数量}} = \frac{1}{\text{激活比率}}\] <p>从计算的角度来看，成本仅由<strong>活跃参数</strong>驱动。如果你保持活跃专家的数量（和大小）固定，并增加专家的总数，你的推理/训练 FLOPs 预算大致保持不变，但你增加了模型的容量，因此只要训练时间足够长，模型通常会变得更好。</p> <p>如果你对最近的 MoE 论文进行调查，会发现一些有趣的经验性结论：<strong>在固定活跃专家的数量和大小的情况下，增加专家的总数（即降低激活比率/增加稀疏度）可以改善 Loss，但当稀疏度变得非常高时，回报会递减。</strong></p> <p>两个例子：</p> <ul> <li><strong>Kimi K2</strong> (K. Team et al., 2025)：显示了两种效应：更高的稀疏度提高了性能，但随着稀疏度的增长，收益会逐渐减少。</li> <li><strong>蚂蚁集团</strong> (Tian et al., 2025)：与 K2 得出相同结论，并额外指出<strong>稀疏度越高的 MoE 从增加计算中获得的益处越多</strong>。</li> </ul> <p>下面是一些 MoE 模型的稀疏度表格：</p> <table> <thead> <tr> <th style="text-align: left">模型</th> <th style="text-align: left">总专家数量</th> <th style="text-align: left">每个 Token 激活数量（含共享）</th> <th style="text-align: left">稀疏度</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Mixtral-8x7B</td> <td style="text-align: left">8</td> <td style="text-align: left">2</td> <td style="text-align: left">4.0</td> </tr> <tr> <td style="text-align: left">Grok-1</td> <td style="text-align: left">8</td> <td style="text-align: left">2</td> <td style="text-align: left">4.0</td> </tr> <tr> <td style="text-align: left">Grok-2</td> <td style="text-align: left">8</td> <td style="text-align: left">2</td> <td style="text-align: left">4.0</td> </tr> <tr> <td style="text-align: left">OLMoE-1B-7B-0924</td> <td style="text-align: left">64</td> <td style="text-align: left">8</td> <td style="text-align: left">8.0</td> </tr> <tr> <td style="text-align: left">gpt-oss 20b</td> <td style="text-align: left">32</td> <td style="text-align: left">4</td> <td style="text-align: left">8.0</td> </tr> <tr> <td style="text-align: left">Step-3</td> <td style="text-align: left">48 路由 + 1 共享 = 49</td> <td style="text-align: left">3 路由 + 1 共享 = 4</td> <td style="text-align: left">12.25</td> </tr> <tr> <td style="text-align: left">GLM-4.5-Air</td> <td style="text-align: left">128 路由 + 1 共享 = 129</td> <td style="text-align: left">8 路由 + 1 共享 = 9</td> <td style="text-align: left">14.3</td> </tr> <tr> <td style="text-align: left">Qwen3-30B-A3B</td> <td style="text-align: left">128</td> <td style="text-align: left">8</td> <td style="text-align: left">16.0</td> </tr> <tr> <td style="text-align: left">Qwen3-235B-A22B</td> <td style="text-align: left">128</td> <td style="text-align: left">8</td> <td style="text-align: left">16.0</td> </tr> <tr> <td style="text-align: left">GLM-4.5</td> <td style="text-align: left">160 路由 + 1 共享 = 161</td> <td style="text-align: left">8 路由 + 1 共享 = 9</td> <td style="text-align: left">17.8</td> </tr> <tr> <td style="text-align: left">DeepSeek-V2</td> <td style="text-align: left">160 路由 + 2 共享 = 162</td> <td style="text-align: left">6 路由 + 2 共享 = 8</td> <td style="text-align: left">20.25</td> </tr> <tr> <td style="text-align: left">DeepSeek-V3</td> <td style="text-align: left">256 路由 + 1 共享 = 257</td> <td style="text-align: left">8 路由 + 1 共享 = 9</td> <td style="text-align: left">28.6</td> </tr> <tr> <td style="text-align: left">gpt-oss 120b</td> <td style="text-align: left">128</td> <td style="text-align: left">4</td> <td style="text-align: left">32.0</td> </tr> <tr> <td style="text-align: left">Kimi K2</td> <td style="text-align: left">384 路由 + 1 共享 = 385</td> <td style="text-align: left">8 路由 + 1 共享 = 9</td> <td style="text-align: left">42.8</td> </tr> <tr> <td style="text-align: left">Qwen3-Next-80B-A3B-Instruct</td> <td style="text-align: left">512 路由 + 1 共享 = 513</td> <td style="text-align: left">10 总激活 + 1 共享 = 11</td> <td style="text-align: left">46.6</td> </tr> </tbody> </table> <p>最近的趋势很明显：<strong>MoE 模型正变得越来越稀疏</strong>。尽管如此，最佳稀疏度仍然取决于硬件和端到端效率。例如，Step-3 旨在达到峰值效率，并有意不将稀疏度最大化，以适应其特定的硬件和带宽约束，而 gpt-oss-20b 的稀疏度较低则是由于设备上的内存限制（被动专家仍然占用一些内存）。</p> <h5 id="4162-粒度granularity">4.1.6.2 粒度（Granularity）</h5> <p>除了稀疏度之外，我们还需要决定<strong>每个专家应该有多大</strong>。这由蚂蚁集团引入的 <strong>粒度（Granularity）</strong> 来捕获。让我们明确这个术语的含义。术语在不同论文中有所不同，有些使用略有不同的公式。在这里，我们将使用与我们引用的图表相匹配的定义：</p> \[G = \alpha \times \frac{d_{model}}{d_{expert}}\] <p><strong>更高的粒度值</strong>对应于拥有 <strong>更多具有更小维度（$d_{expert}$）的专家</strong>（给定固定的参数数量）。该指标是 <strong>专家维度（$d_{expert}$）</strong> 与 <strong>模型维度（$d_{model}$）</strong> 之间比值的倒数（乘上归一化系数 $\alpha$）。</p> <p>在密集型模型中，一个常用的经验法则是将 MLP 的维度设置为 $d_{intermediate} = 4 \times d_{model}$。如果 $\alpha=4$（像 Krajewski et al. (2024)），你可以大致将粒度视为<strong>匹配密集型 MLP 宽度所需的专家数量</strong>（$4 \times d_{model} = d_{intermediate} = G \times d_{expert}$）。</p> <p>这种解释只是一个粗略的启发式：现代 MoE 设计通常分配比单个密集型 MLP 大得多的总容量，因此一对一匹配在实践中会失效。蚂蚁团队的设置选择了 $\alpha=2$，这只是一种不同的归一化选择。为了保持一致性，我们将采用这种约定并坚持下去。</p> <p>以下是一些 MoE 版本的不同粒度值表格：</p> <table> <thead> <tr> <th style="text-align: left">模型</th> <th style="text-align: left">$d_{model}$</th> <th style="text-align: left">$d_{expert}$</th> <th style="text-align: left">$G = 2 \times d_{model} / d_{expert}$</th> <th style="text-align: left">年份</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Mixtral-8x7B</td> <td style="text-align: left">4,096</td> <td style="text-align: left">14,336</td> <td style="text-align: left">0.571</td> <td style="text-align: left">2023</td> </tr> <tr> <td style="text-align: left">gpt-oss-120b</td> <td style="text-align: left">2,880</td> <td style="text-align: left">2,880</td> <td style="text-align: left">0.5</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">gpt-oss-20b</td> <td style="text-align: left">2,880</td> <td style="text-align: left">2,880</td> <td style="text-align: left">0.5</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">Grok 2</td> <td style="text-align: left">8,192</td> <td style="text-align: left">16,384</td> <td style="text-align: left">1.0</td> <td style="text-align: left">2024</td> </tr> <tr> <td style="text-align: left">StepFun Step-3</td> <td style="text-align: left">7,168</td> <td style="text-align: left">5,120</td> <td style="text-align: left">2.8</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">OLMoE-1B-7B</td> <td style="text-align: left">2,048</td> <td style="text-align: left">1,024</td> <td style="text-align: left">4.0</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">Qwen3-30B-A3B</td> <td style="text-align: left">2,048</td> <td style="text-align: left">768</td> <td style="text-align: left">5.3</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">Qwen3-235B-A22B</td> <td style="text-align: left">4,096</td> <td style="text-align: left">1,536</td> <td style="text-align: left">5.3</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">GLM-4.5-Air</td> <td style="text-align: left">4,096</td> <td style="text-align: left">1,408</td> <td style="text-align: left">5.8</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">DeepSeek V2</td> <td style="text-align: left">5,120</td> <td style="text-align: left">1,536</td> <td style="text-align: left">6.6</td> <td style="text-align: left">2024</td> </tr> <tr> <td style="text-align: left">GLM-4.5</td> <td style="text-align: left">5,120</td> <td style="text-align: left">1,536</td> <td style="text-align: left">6.6</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">Kimi K2</td> <td style="text-align: left">7,168</td> <td style="text-align: left">2,048</td> <td style="text-align: left">7.0</td> <td style="text-align: left">2025</td> </tr> <tr> <td style="text-align: left">DeepSeek V3</td> <td style="text-align: left">7,168</td> <td style="text-align: left">2,048</td> <td style="text-align: left">7.0</td> <td style="text-align: left">2024</td> </tr> <tr> <td style="text-align: left">Qwen3-Next-80B-A3B</td> <td style="text-align: left">2,048</td> <td style="text-align: left">512</td> <td style="text-align: left">8.0</td> <td style="text-align: left">2025</td> </tr> </tbody> </table> <p>让我们谈谈<strong>粒度如何影响行为</strong>（来自蚂蚁集团的论文）：</p> <p>粒度看起来<strong>不是 EL 的主要驱动因素</strong>——它有帮助，尤其是当粒度超过 2 时，但它不是决定 Loss 的主导因素。不过，存在一个<strong>最佳点</strong>：提高粒度到一定程度会有帮助，然后收益就会趋于平稳。因此，粒度是一个有用的调整旋钮，最近发布的趋势明显倾向于更高的值，但<strong>不应该孤立地进行优化</strong>。</p> <p>另一种广泛用于改进 MoE 的方法是 <strong>共享专家（Shared Experts）</strong> 的概念。让我们来看看！</p> <h5 id="4163-共享专家shared-experts">4.1.6.3 共享专家（Shared Experts）</h5> <p><strong>共享专家</strong>设置将<strong>每个 Token</strong> 路由到<strong>一组始终处于激活状态</strong>的专家。这些共享专家吸收数据中<strong>基本、重复的模式</strong>，以便其余的专家可以更积极地进行<strong>专业化</strong>。</p> <p>在实践中，你通常不需要很多共享专家；模型设计者通常选择<strong>一个</strong>，最多<strong>两个</strong>。随着粒度的增加（例如，从 Qwen3 风格的设置转向更接近 Qwen3-Next 的设置），共享专家往往变得更有用。从下方的图表来看，总体影响是<strong>适度的</strong>，它不会戏剧性地改变 EL。一个简单的经验法则是<strong>只使用一个共享专家</strong>，这与 DeepSeek V3、K2 和 Qwen3-Next 等模型的选择相符，并且倾向于在不增加不必要复杂性的情况下最大化效率。</p> <p>那么，共享专家就是<strong>所有 Token 都会始终路由通过</strong>的专家。那其他专家呢？我们如何学习何时路由到每个专家，并确保我们不会只使用少数几个专家？接下来我们将讨论<strong>负载均衡</strong>，它正是解决这个问题的。</p> <h5 id="4164-负载均衡load-balancing">4.1.6.4 负载均衡（Load Balancing）</h5> <p><strong>负载均衡是 MoE 中的关键环节。</strong> 如果设置不当，它可能会破坏所有其他设计选择。通过以下示例，我们可以看到为什么糟糕的负载均衡会给我们带来很多痛苦。</p> <p>考虑一个非常简单的分布式训练设置，我们有 4 块 GPU，并将模型的 4 个专家均匀地分布在这些 GPU 上。如果路由崩溃，所有 Token 都被路由到专家 1，这意味着<strong>只有 1/4 的 GPU 被利用</strong>，这对训练和推理效率来说非常糟糕。除此之外，这也意味着模型的<strong>有效学习容量</strong>也降低了，因为并非所有专家都被激活。</p> <p>为了解决这个问题，我们可以<strong>向路由器添加一个额外的损失项</strong>。你可以在下方看到标准的 <strong>基于辅助损失（auxiliary loss–based）</strong> 的负载均衡 (LBL)：</p> \[\text{Loss}_{LBL} = \alpha \times \sum_{i} f_i \times P_i\] <p>这个简单的公式只使用了三个因子：</p> <ul> <li>系数 $\alpha$ 决定了损失的强度。</li> <li>$f_i$ 是<strong>流量分数（traffic fraction）</strong>，即流经专家 $i$ 的 Token 分数。</li> <li>$P_i$ 是<strong>概率质量（probability mass）</strong>，简单地是流经该专家的 Token 概率总和。</li> </ul> <p>两者都是必需的：$f_i$ 对应于<strong>实际的均衡</strong>，而 $P_i$ 是<strong>平滑且可微分的</strong>，允许梯度流动。如果我们实现了完美的负载均衡，我们会得到 $f_i = P_i = 1/N_r$。然而，我们需要小心如何调整 $\alpha$，因为 $\alpha$ 值太小，我们对路由的引导不够；而如果 $\alpha$ 太大，路由的<strong>均匀性</strong>就变得比主要的语言模型损失更重要。</p> <p>一个关键的细节是<strong>计算路由统计信息的范围</strong>：$f_i$ 和 $P_i$ 是<strong>按局部批次</strong>（每个 Worker 的 Mini-Batch）计算，还是<strong>按全局</strong>（跨 Worker/设备聚合）计算？Qwen 团队的分析 (Qiu et al., 2025) 表明，当每个局部批次中没有足够的 Token 多样性时，局部计算可能会<strong>损害专家专业化</strong>（路由健康状况的良好代理）和<strong>整体模型性能</strong>。专家专业化是一种现象，即一个或多个专家在特定领域被激活得比其他专家更频繁。换句话说，如果一个局部批次很窄，它的路由统计数据就会嘈杂/有偏差，不会带来好的均衡。这表明，<strong>只要可行，我们就应该使用全局统计信息</strong>（或至少是跨设备的聚合）。值得注意的是，在那篇论文发表时，许多框架——包括 Megatron——默认都是局部计算这些统计信息的。</p> <p>下方的图表（<em>注：指代原文图表</em>）来自 Qwen 的论文，说明了 <strong>Mini-Batch 与全局批次聚合</strong>之间的差异及其对性能和专业化的影响。</p> <p>通常，围绕 MoE 进行架构选择的消融实验是棘手的，因为它涉及许多方面的相互作用。例如，共享专家的有效性可能取决于模型的<strong>粒度</strong>。因此，值得花一些时间来确保你有一套好的实验，才能真正获得你正在寻找的洞察！</p> <p>我们现在已经涵盖了 MoE 的基础知识，但仍有更多内容有待发现。以下是一些非详尽的、可供进一步研究的项目清单：</p> <ul> <li>零计算专家、MoE 层重新缩放和训练监控（来自 LongCat-Flash 论文）。</li> <li>正交损失负载均衡（如 ERNIE 4.5 中所示）。</li> <li>在训练过程中<strong>调度负载均衡系数</strong>。</li> <li>架构/优化与 MoE 的相互作用，例如： <ul> <li>优化器排名是否会因 MoE 而改变。</li> <li>如何将 $\mu\text{P}$ 应用于 MoE。</li> <li>如何为 MoE 调整学习率（因为它们在每个批次中看到的 Token 数量不同）。</li> </ul> </li> <li>起始处的密集层数量。</li> <li>…更多。</li> </ul> <p>我们把进一步深入探索的重任留给你们这些求知若渴的读者，现在我们将转向最后一个主要的架构选择：<strong>混合模型（Hybrid Models）</strong>！</p> <h2 id="参考">参考</h2> <p><a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook">原文</a></p>]]></content><author><name></name></author><category term="llm"/><summary type="html"><![CDATA[1. 前言]]></summary></entry><entry><title type="html">MNN模型支持：Qwen3-VL</title><link href="https://zhaode.wang/blog/2025/qwen3vl/" rel="alternate" type="text/html" title="MNN模型支持：Qwen3-VL"/><published>2025-10-24T00:00:00+00:00</published><updated>2025-10-24T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/qwen3vl</id><content type="html" xml:base="https://zhaode.wang/blog/2025/qwen3vl/"><![CDATA[<h2 id="qwen3-vl-架构解析">Qwen3-VL 架构解析</h2> <p>在我们深入技术细节之前，首先从推理引擎的视角，解析一下 Qwen3-VL 的架构创新。Qwen3-VL 作为 Qwen 系列迄今最强大的视觉语言模型，其卓越性能背后是模型结构的深度优化，而这些优化也为 MNN 这样的推理引擎带来了新的挑战。</p> <p><strong>1. DeepStack：从“单向连接”到“多层融合”</strong> 传统的视觉语言模型通常采用 Vision Encoder -&gt; LLM 的串行结构，视觉特征在输入语言模型后便不再与视觉模块交互。而 Qwen3-VL 引入的 <strong>DeepStack</strong> 机制（如下图所示）打破了这一常规。它将 Vision Transformer (ViT) 不同层级的特征图（<code class="language-plaintext highlighter-rouge">feature maps</code>）提取出来，直接注入到语言模型解码器（LLM Decoder）的对应层。</p> <ul> <li><strong>对推理引擎的挑战：</strong> 这种结构意味着计算图不再是简单的线性序列。MNN 需要处理 Vision Encoder 的多个输出张量，并在语言模型的推理过程中，于指定的网络层精确地将这些 <code class="language-plaintext highlighter-rouge">deepstack</code> 张量作为额外输入进行融合。这要求引擎具备灵活的图执行能力和高效的内存管理，以处理这种“旁路”输入，避免不必要的延迟。</li> </ul> <p><strong>2. 位置编码革新：<code class="language-plaintext highlighter-rouge">pos_embeds</code> 与 Interleaved MRoPE</strong> Qwen3-VL 为了增强对长视频和高分辨率图像的空间与时间理解能力，对位置编码进行了革新，例如采用了 <strong>Interleaved MRoPE</strong>。同时，其 Vision Encoder 引入了一个新的动态位置嵌入 <code class="language-plaintext highlighter-rouge">pos_embeds</code>。这个 <code class="language-plaintext highlighter-rouge">pos_embeds</code> 通过双线性插值动态生成，以适应任意分辨率的输入图像。</p> <ul> <li><strong>对推理引擎的挑战：</strong> 动态插值的计算过程包含大量循环和条件判断，这些操作难以直接转换为高效的静态计算图（ONNX）。若强行在图内实现，会引入大量低效算子，严重影响推理性能。因此，如何将这部分动态计算逻辑剥离出主计算图，在运行时（Runtime）高效实现，同时保证模型精度，成为了适配的关键。</li> </ul> <p><strong>3. 架构多样性：Dense 与 MoE 的统一</strong> Qwen3-VL 同时提供了常规的 Dense 模型和更高效的 MoE (Mixture-of-Experts) 变体。然而，<code class="language-plaintext highlighter-rouge">Qwen3-VL-MoE</code> 的专家层（Experts）在官方实现上与标准的 <code class="language-plaintext highlighter-rouge">Qwen3-MoE</code> 结构存在差异，这给模型的统一转换和部署带来了障碍。</p> <ul> <li><strong>对推理引擎的挑战：</strong> 为了避免为两种 MoE 模型开发两套独立的推理后端，我们需要在模型转换阶段进行“适配”。目标是生成一种标准化的 ONNX 结构，让 MNN 的 MoE 推理逻辑可以无差别地处理，这考验了我们在模型前端处理上的灵活性和工程能力。</li> </ul> <p><img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg" alt="Qwen3-VL 模型架构图"/></p> <p>将如此强大的模型引入 MNN，适配上述这些先进特性，是我们这次工作的核心。下面，我们将详细介绍针对这三大挑战的具体解决方案。</p> <h2 id="适配动态位置编码">适配动态位置编码</h2> <p><strong>解决方案：</strong> 我们将 <code class="language-plaintext highlighter-rouge">pos_embeds</code> 的计算拆解，把复杂的索引和权重插值计算移到计算图外（由 MNN C++ Runtime 在运行时处理），仅将纯粹的张量运算保留在 ONNX 模型内。</p> <p><strong>Python 端修改：</strong></p> <p>VisionEncoder 的 <code class="language-plaintext highlighter-rouge">forward</code> 函数不再自己计算插值，而是直接接收预先计算好的 <code class="language-plaintext highlighter-rouge">idx_tensor</code> 和 <code class="language-plaintext highlighter-rouge">weight_tensor</code>。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># VisionEncoder 的 forward 函数修改
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">flatten_patches</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">idx_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">):</span>
    <span class="c1"># ...
</span>    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patch_embed</span><span class="p">(</span><span class="n">flatten_patches</span><span class="p">)</span>

    <span class="c1"># 使用预先计算的索引和权重来获取 pos_embeds
</span>    <span class="n">pos_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_embed</span><span class="p">(</span><span class="n">idx_tensor</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_tensor</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pos_embeds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">pos_embeds</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">pos_embeds</span>
    <span class="c1"># ... 后续 transformer block 计算
</span>    <span class="k">return</span> <span class="n">image_embeds</span><span class="p">,</span> <span class="n">deepstack_feature</span>
</code></pre></div></div> <p><strong>C++ 端实现：</strong></p> <p>在 C++ 运行时，我们复现了插值逻辑，根据输入的图像尺寸动态生成 <code class="language-plaintext highlighter-rouge">idx_tensor</code> 和 <code class="language-plaintext highlighter-rouge">weight_tensor</code>，并作为新的输入传给 MNN 模型。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">isQwen3VL</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 根据图像尺寸 grid_h, grid_w 计算插值所需的索引和权重</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_patches</span> <span class="o">=</span> <span class="n">grid_h</span> <span class="o">*</span> <span class="n">grid_w</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">idx_tensor</span> <span class="o">=</span> <span class="n">Express</span><span class="o">::</span><span class="n">_Input</span><span class="p">({</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">},</span> <span class="p">...);</span>
    <span class="k">auto</span> <span class="n">weight_tensor</span> <span class="o">=</span> <span class="n">Express</span><span class="o">::</span><span class="n">_Input</span><span class="p">({</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">},</span> <span class="p">...);</span>
    <span class="c1">// ... 循环计算每个 patch 的4个插值点索引和权重 ...</span>
    <span class="c1">// ... Reshape 和 Permute 操作以匹配模型输入维度 ...</span>

    <span class="c1">// 将计算好的张量作为额外输入</span>
    <span class="n">moduleInputs</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">idx_tensor</span><span class="p">);</span>
    <span class="n">moduleInputs</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="适配-deepstack-特征">适配 DeepStack 特征</h2> <p><strong>解决方案：</strong> 我们为语言模型的 <code class="language-plaintext highlighter-rouge">forward</code> 函数增加了 <code class="language-plaintext highlighter-rouge">deepstack_embeds</code> 输入，并在 C++ 端实现了特征的正确收集与传递。</p> <p><strong>Python 端修改：</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># LLM 的 forward 函数修改
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...,</span> <span class="n">deepstack_embeds</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">)):</span>
        <span class="c1"># ... transformer block 计算 ...
</span>        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">kv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](...)</span>

        <span class="c1"># 在指定层注入 deepstack 特征
</span>        <span class="k">if</span> <span class="n">deepstack_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">deepstack_embeds</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">hidden_states</span> <span class="o">+=</span> <span class="n">deepstack_embeds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="bp">...</span>
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">...</span>
</code></pre></div></div> <p><strong>C++ 端实现：</strong></p> <p>修改 <code class="language-plaintext highlighter-rouge">embedding</code> 函数，在处理输入序列时，正确地收集 <code class="language-plaintext highlighter-rouge">deepstack</code> 特征，并将其作为语言模型的一个额外输入。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 在 embedding 函数中处理多模态输入</span>
<span class="n">VARP</span> <span class="n">Omni</span><span class="o">::</span><span class="n">embedding</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span> <span class="n">input_ids</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">VARP</span><span class="o">&gt;</span> <span class="n">deepstacks</span><span class="p">;</span>
    <span class="kt">bool</span> <span class="n">hasDeepStack</span> <span class="o">=</span> <span class="o">!</span><span class="n">mDeepStackEmbeddings</span><span class="p">.</span><span class="n">empty</span><span class="p">();</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">id</span> <span class="o">:</span> <span class="n">input_ids</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">id</span> <span class="o">==</span> <span class="n">mVisionPad</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 遇到图像占位符</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">hasDeepStack</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">deepstacks</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">mDeepStackEmbeddings</span><span class="p">[</span><span class="n">vision_idx</span><span class="p">]);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="c1">// 将收集到的 deepstack 特征拼接成一个 tensor</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">hasDeepStack</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">mExtraArgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Express</span><span class="o">::</span><span class="n">_Concat</span><span class="p">(</span><span class="n">deepstacks</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="p">...;</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="moe-模型导出">MoE 模型导出</h2> <p><strong>解决方案：</strong> 在导出前动态重构专家层使其与之前的Qwen3-MoE架构保持一致。我们在模型加载阶段实现了一个适配器，它能检测到 <code class="language-plaintext highlighter-rouge">Qwen3-VL-MoE</code> 的特殊结构，并动态地将其重构为标准的 <code class="language-plaintext highlighter-rouge">ModuleList</code> 形式。</p> <p><strong>适配逻辑精简如下：</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 在 Mlp 模块的初始化函数中进行适配
</span><span class="k">class</span> <span class="nc">Mlp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...):</span>
        <span class="c1"># ...
</span>        <span class="n">is_qwen3_vl_moe</span> <span class="o">=</span> <span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">experts</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_qwen3_vl_moe</span><span class="p">:</span>
            <span class="n">original_experts</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">experts</span>
            <span class="n">new_experts_list</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_experts</span><span class="p">):</span>
                <span class="c1"># 1. 实例化一个标准的 Expert 模块
</span>                <span class="n">expert_mlp</span> <span class="o">=</span> <span class="nc">Qwen3Expert</span><span class="p">(...)</span>
                <span class="c1"># 2. 从原始打包的权重中切片并赋值
</span>                <span class="n">expert_mlp</span><span class="p">.</span><span class="n">gate_up_proj_linear</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">...</span>
                <span class="n">expert_mlp</span><span class="p">.</span><span class="n">down_proj_linear</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">...</span>
                <span class="n">new_experts_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">expert_mlp</span><span class="p">)</span>
            <span class="c1"># 3. 用重构后的标准 ModuleList 替换原有 experts
</span>            <span class="n">self</span><span class="p">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">new_experts_list</span>
</code></pre></div></div> <p>通过在 Python 端进行这次“预处理”，导出的 ONNX 模型拥有了完全一致的 MoE 结构，极大地简化了 MNN 在 C++ 端的推理实现。</p> <h2 id="模型下载">模型下载</h2> <p>我们已经将转换好的 MNN 模型上传至社区，欢迎下载体验：</p> <ul> <li><strong>ModelScope:</strong> <a href="https://modelscope.cn/collections/Qwen3-VL-MNN-f4da0cedb82847">https://modelscope.cn/collections/Qwen3-VL-MNN-f4da0cedb82847</a></li> <li><strong>Hugging Face:</strong> <a href="https://huggingface.co/collections/taobao-mnn/qwen3-vl-mnn">https://huggingface.co/collections/taobao-mnn/qwen3-vl-mnn</a></li> </ul>]]></content><author><name></name></author><category term="MNN"/><category term="llm"/><summary type="html"><![CDATA[Qwen3-VL 架构解析]]></summary></entry><entry><title type="html">一图读懂Qwen</title><link href="https://zhaode.wang/blog/2025/qwenfamily/" rel="alternate" type="text/html" title="一图读懂Qwen"/><published>2025-09-25T00:00:00+00:00</published><updated>2025-09-25T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/qwenfamily</id><content type="html" xml:base="https://zhaode.wang/blog/2025/qwenfamily/"><![CDATA[<p>随着最近举办的云栖大会，“通义千问”系列模型的密集发布引发了不少讨论。 网络上流传的表情包所调侃的，从最初的几个核心模型，到现在“全家桶”式的发布，让不少人直呼“跟不上了”。</p> <p><img src="/assets/img/qwenfamily/gptqwen.png" alt="gpt-qwen"/></p> <p>不过调侃归调侃，通义千问命名相比GPT的“阴间”命名风格还是很“阳间”的。为了帮助大家更清晰地梳理“Qwen家族”的全貌，我制作了一张<strong>Qwen全系列模型汇总及发布时间线图</strong>。</p> <p>从这张图中，你可以直观地看到：</p> <ul> <li><strong>迭代速度快</strong>：通义千问在短时间内完成了多次重要更新，快速覆盖了从小型到超大规模（如参数超过万亿的Qwen3-Max）的完整模型尺寸梯度。</li> <li><strong>家族成员多</strong>：除了基础的语言模型，Qwen还衍生出了众多针对特定任务和场景的“专家模型”，例如强化数学和逻辑推理能力的模型、专注于代码生成的Coder系列以及具备音视频理解能力的多模态模型。</li> <li><strong>技术路线广</strong>：Qwen系列不仅有传统的密集（Dense）模型，还积极探索了混合专家（MoE）架构，旨在实现性能与效率的平衡。</li> </ul> <p><img src="/assets/img/qwenfamily/qwenfamily.png" alt="gpt-qwen"/></p> <p><em>标题图由Nano Banana生成，网页由Qwen Coder模型辅助生成，<a href="https://zhaode.wang/llm/qwenfamily">图片原始网页</a></em></p>]]></content><author><name></name></author><category term="llm"/><summary type="html"><![CDATA[随着最近举办的云栖大会，“通义千问”系列模型的密集发布引发了不少讨论。 网络上流传的表情包所调侃的，从最初的几个核心模型，到现在“全家桶”式的发布，让不少人直呼“跟不上了”。]]></summary></entry><entry><title type="html">端侧LLM硬件系列（二）：内存容量</title><link href="https://zhaode.wang/blog/2025/device-llm-memory-capacity/" rel="alternate" type="text/html" title="端侧LLM硬件系列（二）：内存容量"/><published>2025-09-22T00:00:00+00:00</published><updated>2025-09-22T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/device-llm-memory-capacity</id><content type="html" xml:base="https://zhaode.wang/blog/2025/device-llm-memory-capacity/"><![CDATA[<p>随着iPhone 17 Pro系列将运行内存（RAM）从8GB升级至12GB，一个明确的信号已经发出：为了迎接真正的端侧AI时代，移动设备正在着手应对一个新的、也是更基础的硬件瓶颈——内存容量。</p> <p>这次升级并非为了常规的多任务处理，而是为了给“Apple Intelligence”这类日益复杂的端侧大模型提供必要的运行空间。你或许也曾遇到过类似情况：功能强大的AI应用突然闪退，或在处理稍长文档时无响应。这背后的原因，往往不是芯片算力（TOPS）不足，而是手机的物理内存已经耗尽。</p> <p>内存容量是决定端侧AI模型的入场券，这篇文章将深入分析LLM的内存占用构成，探讨各类优化技术，并最终评估在2025年，多大的内存才足以支撑一个流畅的“口袋里的AI大脑”。</p> <h2 id="llm内存占用的构成三大核心部分的量化分析"><strong>LLM内存占用的构成：三大核心部分的量化分析</strong></h2> <p>要理解内存瓶颈，首先需要精确拆解LLM在运行时到底消耗了什么。其内存占用主要由三个动态和静态的部分构成，每一部分都有其详细的计算规则。</p> <h3 id="1-模型权重-model-weights"><strong>1. 模型权重 (Model Weights)</strong></h3> <p>这是模型最基础、最主要的内存开销，可以理解为模型的“知识库”。其大小由参数量和数据精度（Precision）决定。</p> <ul> <li><strong>数据精度与字节数:</strong> <ul> <li><strong>FP32 (单精度浮点):</strong> 每参数占4字节。</li> <li><strong>FP16/BF16 (半精度浮点):</strong> 每参数占2字节，是当前推理最常用的格式。</li> <li><strong>INT8 (8位整型):</strong> 每参数占1字节（量化后）。</li> <li><strong>INT4 (4位整型):</strong> 每参数占0.5字节（量化后）。</li> </ul> </li> <li><strong>计算公式:</strong> <blockquote> <p><strong>内存占用 (GB) ≈ 参数量 (十亿) × 单个参数大小 (Bytes)</strong></p> </blockquote> </li> </ul> <p>基于此公式，我们可以清晰地看到不同规模模型在加载时对内存的静态需求：</p> <table> <thead> <tr> <th style="text-align: left">模型规模（参数）</th> <th style="text-align: left">FP16 (2 Bytes/Param)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>7B (70亿)</strong></td> <td style="text-align: left">≈ 14 GB</td> </tr> <tr> <td style="text-align: left"><strong>13B (130亿)</strong></td> <td style="text-align: left">≈ 26 GB</td> </tr> </tbody> </table> <p><em>注：为方便估算，此表采用1GB = 10^9 Bytes的近似值。</em></p> <p>这个静态值直接决定了模型能否被加载进手机的“生死线”。从表中可见，一个未经优化的7B模型，仅权重部分就超过了市面上绝大多数手机的内存上限。</p> <h3 id="2-激活值-activations"><strong>2. 激活值 (Activations)</strong></h3> <p>在模型进行前向计算（即“思考”过程）时，每一层网络都会产生临时的中间数据。这部分内存占用是动态的，在处理长文本的Prefill（预填充）阶段会达到峰值。</p> <ul> <li><strong>经验估算法:</strong> 精确计算激活值内存较为复杂，因为它与具体模型架构和推理引擎实现紧密相关。在工程实践中，一个实用的快速估算法则是，<strong>激活值的峰值内存约等于模型权重（FP16）的25%</strong>。对于一个14GB的7B模型，其激活值峰值约为3.5GB。</li> </ul> <h3 id="3-kv-cache"><strong>3. KV Cache</strong></h3> <p>这是在Decoding（解码/逐字生成）阶段为了加速计算而设计的缓存机制。它存储了注意力机制已经计算过的键（Key）和值（Value），避免重复计算。</p> <ul> <li> <p><strong>核心特点:</strong> KV Cache的大小与上下文长度（Sequence Length）成正比，是长对话或长文总结场景下最主要的内存增长点。</p> </li> <li> <p><strong>精确计算公式:</strong></p> <blockquote> <p><strong>KV Cache内存 (Bytes) ≈ 2 × 上下文长度 × 模型层数 × 注意力头个数 × 注意力维度 × 精度(Bytes)</strong></p> </blockquote> <p>不同模型的具体架构参数不同，导致KV Cache大小差异巨大。以一个采用了GQA结构优化的现代模型，如<strong>Qwen2-7B</strong>（拥有28个Transformer层，K/V注意力头为4个，每个头的维度为128）为例，在FP16精度下，处理4096个Token的上下文，其KV Cache占用约为：</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">2 × 4096 × 28 × 4 × 128 × 2 Bytes ≈ 235 MB</code></p> </blockquote> </li> </ul> <h3 id="综合内存占用分析"><strong>综合内存占用分析</strong></h3> <p>现在，我们将一个典型的、未经优化的7B模型的FP16内存占用进行加总，看看它的峰值需求有多庞大：</p> <table> <thead> <tr> <th style="text-align: left">内存组成部分</th> <th style="text-align: left">FP16 (未优化)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>权重内存 (静态)</strong></td> <td style="text-align: left">≈ 14.0 GB</td> </tr> <tr> <td style="text-align: left"><strong>激活值内存 (动态峰值, 估算)</strong></td> <td style="text-align: left">≈ 3.5 GB</td> </tr> <tr> <td style="text-align: left"><strong>KV Cache (4K tokens, 典型值)</strong></td> <td style="text-align: left">≈ 2.1 GB</td> </tr> <tr> <td style="text-align: left"><strong>预估总内存峰值</strong></td> <td style="text-align: left"><strong>≈ 19.6 GB</strong></td> </tr> </tbody> </table> <p><em>注：KV Cache大小因模型结构而异，此处采用一个典型非GQA优化模型的数值以展示问题的严重性。</em></p> <p>结论已经非常清晰：一个未经优化的7B模型，其近20GB的峰值内存需求，在移动端是绝对无法满足的物理限制。这使得下一章将要讨论的内存优化技术，不再是“可选项”，而是让端侧大模型成为现实的“必需品”。</p> <hr/> <h2 id="内存优化技术在有限容量下实现可能"><strong>内存优化技术：在有限容量下实现可能</strong></h2> <p>在物理内存有限的前提下，软件和算法层面的优化是让大模型在端侧运行的关键。这些技术主要围绕内存占用的三个核心部分展开。</p> <h3 id="1-权重量化-weight-quantization"><strong>1. 权重量化 (Weight Quantization)</strong></h3> <p>这是最核心、效果最显著的模型压缩技术。其原理是将高精度（如FP16）的浮点数权重，转换为低精度（如INT8或INT4）的整数，从而大幅减少模型的存储体积。</p> <ul> <li><strong>技术细节：</strong> 为保证精度，现代量化方案（如GPTQ/AWQ）普遍采用<strong>分组量化 (Grouped Quantization)</strong>。即将权重分为多个小组（例如每32个或64个为一组），并为每个小组计算独立的量化参数（<strong>Scale</strong>，即比例尺）。</li> <li><strong>有效比特率 (bpw):</strong> 这也意味着量化后的实际成本会略高于其标称位数。例如，对一个INT4量化模型，如果每32个权重共享一个FP16（16 bit）的Scale，则每个权重的平均占用为 <code class="language-plaintext highlighter-rouge">(32 * 4 + 16) / 32 = 4.5 bpw</code>。</li> <li><strong>优化结果：</strong> 基于4.5 bpw计算，一个原本需要14GB的7B模型，其权重体积可以被压缩至约 <strong>3.7 GB</strong>，使其具备了在移动设备上加载的可能性。</li> </ul> <h3 id="2-激活值优化技术"><strong>2. 激活值优化技术</strong></h3> <p>主要用于降低处理长序列时产生的瞬时内存峰值，核心思路是“以时间换空间”。</p> <ul> <li><strong>激活重计算 (Activation Recomputation):</strong> 不在内存中保留所有中间层的激活值，而是在需要时通过前向计算从上一个“检查点”重新推导。</li> <li><strong>分块预填充 (Chunk Prefill):</strong> 将长输入切分成小块，逐块进行计算并填充KV Cache。每处理完一小块，其对应的激活值内存即可释放，从而避免了巨大的瞬时内存开销。</li> </ul> <h3 id="3-kv-cache-优化技术"><strong>3. KV Cache 优化技术</strong></h3> <p>旨在降低长上下文场景下的内存占用。</p> <ul> <li><strong>KV Cache量化:</strong> 同样可以将KV Cache中的数据从FP16量化至INT8，直接将其内存占用降低50%。</li> <li><strong>模型结构优化 (GQA):</strong> 分组查询注意力（Grouped-Query Attention, GQA）是目前主流的优化结构。它通过让多组“注意力头”共享同一套K和V缓存，在大幅降低KV Cache内存占用的同时，实现了性能和效果的最佳平衡。</li> </ul> <h3 id="4-混合存储方案"><strong>4. 混合存储方案</strong></h3> <p>当上述优化仍无法满足需求时，最后的手段是利用速度较慢但容量巨大的闪存。</p> <ul> <li><strong>内存卸载 (Offloading):</strong> 将当前不活跃的模型层（比如<code class="language-plaintext highlighter-rouge">Embedding</code>层）或较早的KV Cache从RAM中转移到闪存，需要时再加载回来。这是一种以牺牲响应速度（延迟）为代价，换取更大有效容量的终极方案。</li> </ul> <hr/> <h3 id="优化成效分析全新的内存占用"><strong>优化成效分析：全新的内存占用</strong></h3> <p>经过上述一系列组合拳式的优化，我们现在可以重新计算同一个7B模型在端侧运行时的实际内存占用：</p> <table> <thead> <tr> <th style="text-align: left">内存组成部分</th> <th style="text-align: left">优化后 (4-bit量化 + GQA)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>权重内存 (4.5 bpw)</strong></td> <td style="text-align: left">≈ 3.7 GB</td> </tr> <tr> <td style="text-align: left"><strong>激活值内存 (动态峰值, 估算)</strong></td> <td style="text-align: left">≈ 0.9 GB</td> </tr> <tr> <td style="text-align: left"><strong>KV Cache (4K tokens, Qwen2-7B, INT8)</strong></td> <td style="text-align: left">≈ 0.12 GB</td> </tr> <tr> <td style="text-align: left"><strong>预估总内存峰值</strong></td> <td style="text-align: left"><strong>≈ 4.7 GB</strong></td> </tr> </tbody> </table> <p>从接近20GB到不足5GB——这是一个超过75%的惊人降幅。这标志着，一个强大的70亿参数大模型，在理论上终于获得了进入主流8GB、畅行12GB内存手机的“资格”。然而，理论上的可行性，还需要通过操作系统层面严苛的现实考验。</p> <hr/> <h2 id="操作系统的内存管理机制"><strong>操作系统的内存管理机制</strong></h2> <p>优化后的模型最终仍需在操作系统的管理下运行。Android和iOS的机制差异，直接影响了应用的稳定性。</p> <ul> <li><strong>Android:</strong> 采用<strong>低内存杀手（Low-Memory Killer, LMK）</strong>机制。当系统总内存不足时，会根据进程优先级强制“杀死”后台应用。一个占用数GB内存的AI应用，即便能运行，也非常容易在切换到后台时被LMK清理。</li> <li><strong>iOS:</strong> 对每个应用有严格的内存使用上限。一旦应用消耗的“脏内存”（Dirty Memory，由程序动态分配的内存）超限，就会被系统强制终止（闪退）。 <ul> <li>在iOS中运行LLM类型的任务，需要申请<code class="language-plaintext highlighter-rouge">com.apple.developer.kernel.increased-memory-limit</code>权限，可以提升APP的内存上限。</li> <li>另外一个有效方案是使用<code class="language-plaintext highlighter-rouge">mmap</code>技术加载模型权重。<code class="language-plaintext highlighter-rouge">mmap</code>将只读的权重映射为“干净内存”（Clean Memory），这部分内存iOS并不会计入APP的内存占用，从而避免了因触及“脏内存”上限导致闪退。</li> </ul> </li> </ul> <hr/> <h2 id="结论多大内存的手机才配得上ai大脑的称号"><strong>结论：多大内存的手机，才配得上“AI大脑”的称号？</strong></h2> <p>综合以上分析，我们可以得出一份基于2025年主流设备的AI能力评估矩阵：</p> <h3 id="真实世界ai能力适配矩阵-2025年机型"><strong>真实世界AI能力适配矩阵 (2025年机型)</strong></h3> <table> <thead> <tr> <th style="text-align: left">设备典型代表</th> <th style="text-align: left">物理内存容量</th> <th style="text-align: left">运行7B模型 (4-bit, GQA) 体验评估</th> <th style="text-align: left">运行13B模型 (4-bit, GQA) 体验评估</th> <th style="text-align: left">场景分析与最终评价</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>标准版iPhone 17 / 主流安卓中端机</strong></td> <td style="text-align: left"><strong>8GB</strong></td> <td style="text-align: left"><strong>勉强可用</strong></td> <td style="text-align: left"><strong>几乎不可行</strong></td> <td style="text-align: left"><strong>体验受限</strong>：模型加载后系统可用内存极低，多任务处理能力差，后台易被终止。仅适合轻量级、非连续的AI任务。物理容量是硬伤。</td> </tr> <tr> <td style="text-align: left"><strong>iPhone 17 Pro / 安卓主流高端机</strong></td> <td style="text-align: left"><strong>12GB</strong></td> <td style="text-align: left"><strong>体验流畅</strong></td> <td style="text-align: left"><strong>基本可行</strong></td> <td style="text-align: left"><strong>AI体验基准</strong>：为当前主流的7B模型提供了充足的运行空间和良好的多任务缓冲。是硬件容量和软件优化结合的最佳平衡点，构成了高质量端侧AI体验的基础。</td> </tr> <tr> <td style="text-align: left"><strong>安卓旗舰机</strong></td> <td style="text-align: left"><strong>16GB</strong></td> <td style="text-align: left"><strong>游刃有余</strong></td> <td style="text-align: left"><strong>体验流畅</strong></td> <td style="text-align: left"><strong>面向未来</strong>：不仅能轻松驾驭13B模型，也为更复杂的AI应用（如多模态交互）预留了充足的硬件冗余。是追求极致性能和长期AI体验保障的最佳选择。</td> </tr> <tr> <td style="text-align: left"><strong>安卓顶级旗舰机</strong></td> <td style="text-align: left"><strong>24GB</strong></td> <td style="text-align: left"><strong>性能过剩</strong></td> <td style="text-align: left"><strong>游刃有余</strong></td> <td style="text-align: left"><strong>开发与探索</strong>：远超当前普通用户的需求，主要价值在于为开发者提供了一个不受内存束缚的实验平台，探索端侧AI的未来可能性，可运行<code class="language-plaintext highlighter-rouge">GPT-OSS-20B</code>。</td> </tr> </tbody> </table> <hr/> <h3 id="最终结论"><strong>最终结论</strong></h3> <p>分析指向一个明确的结果：<strong>物理内存容量是端侧AI能力的根基，而软件优化则是在这个根基上实现效率最大化的手段。</strong></p> <ol> <li><strong>8GB内存</strong> 在端侧AI时代已显不足，无法提供稳定、流畅的大模型体验。</li> <li><strong>12GB内存</strong> 正如iPhone 17 Pro所展示的，已成为2025年“AI手机”的有效起步线，是保证高质量体验的基础。</li> <li><strong>16GB内存</strong> 则提供了更强的性能保障和面向未来的扩展性，足以流畅运行下一代百亿参数模型。</li> </ol> <p>最终，决定手机AI体验上限的，不再仅仅是算力（TOPS）数字，而是那个实实在在的物理内存容量。在这场将AI装进口袋的竞赛中，内存，正是那张最关键的“入场券”。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[随着iPhone 17 Pro系列将运行内存（RAM）从8GB升级至12GB，一个明确的信号已经发出：为了迎接真正的端侧AI时代，移动设备正在着手应对一个新的、也是更基础的硬件瓶颈——内存容量。]]></summary></entry><entry><title type="html">Qwen3-Next：下一代MoE模型架构解析</title><link href="https://zhaode.wang/blog/2025/qwen3-next/" rel="alternate" type="text/html" title="Qwen3-Next：下一代MoE模型架构解析"/><published>2025-09-10T00:00:00+00:00</published><updated>2025-09-10T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/qwen3-next</id><content type="html" xml:base="https://zhaode.wang/blog/2025/qwen3-next/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">transformers</code> 库近期合并了 <code class="language-plaintext highlighter-rouge">Qwen3-Next</code> 的 <a href="https://github.com/huggingface/transformers/pull/40771/">PR</a>，正式将其纳入官方生态。这个 PR 提交了 <code class="language-plaintext highlighter-rouge">Qwen3-Next-80B-A3B-Instruct</code> 模型的实现，该模型被介绍为下一代基础模型，针对超长上下文和大规模参数效率进行了深度优化。官方的描述为：</p> <blockquote> <p>The Qwen3-Next series represents our next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency.</p> </blockquote> <p>该模型的四大核心亮点为：</p> <ul> <li><strong>高稀疏度 MoE (High-Sparsity MoE)</strong>: 实现了极低的计算激活比，在保持庞大知识容量的同时，追求极致的推理性能。</li> <li><strong>混合注意力 (Hybrid Attention)</strong>: 融合<strong>门控增量网络 (Gated DeltaNet)</strong> 与<strong>门控注意力 (Gated Attention)</strong>，高效建模不同距离的上下文依赖。</li> <li><strong>多词元预测 (MTP)</strong>: 提升模型性能并为推理加速设计的先进预训练目标。</li> <li><strong>其他优化</strong>: 包括<strong>零中心化的 RMSNorm</strong> 等，旨在增强训练稳定性。</li> </ul> <p>下面我们来深入 PR 内容，逐一解析 Qwen3-Next 的架构创新。</p> <h2 id="一高稀疏度-moe-与共享专家">一、高稀疏度 MoE 与共享专家</h2> <p>Qwen3-Next 的混合专家（MoE）设计有两个关键点：高稀疏度和共享专家。</p> <p><strong>1. 高稀疏度对解码性能的提升</strong></p> <p>高稀疏度是 Qwen3-Next 实现极致性能的核心。以 80B 版本为例，它拥有 800 亿总参数，但在生成每一个 token 时，仅需激活其中的 30 亿参数进行计算。这一激活比远低于当前主流 MoE 模型，是一项关键的架构选择。</p> <p>这并非简单的“节约”，而是对性能的直接赋能。在自回归生成（decode）任务中，模型需要逐词进行前向传播，此时<strong>每一步的计算量 (FLOPs) 直接决定了生成速度</strong>。通过将激活参数降低一个数量级，Qwen3-Next 实现了：</p> <ul> <li><strong>更高的吞吐量</strong>：在处理长上下文（&gt;32K tokens）时，其推理吞吐量可达 Qwen3-32B 的 <strong>10 倍以上</strong>。</li> <li><strong>更快的响应速度</strong>：对于用户而言，这意味着更低的延迟和更流畅的交互体验。</li> </ul> <p>可以说，高稀疏度 MoE 是 Qwen3-Next 在解码性能提升上的核心引擎。</p> <p><strong>2. 共享专家增强稳定性</strong></p> <p>为确保极致稀疏下的稳定性，Qwen3-Next 在 MoE 模块中额外增加了一个<strong>共享专家（Shared Expert）</strong>，形成了一种更稳健的“双轨”设计。</p> <pre><code class="language-mermaid">flowchart TD
    A[输入 Hidden State] --&gt;|所有 Token| B("共享专家 (Shared Expert)");
    A --&gt;|所有 Token| C{"路由 (Router)"};

    subgraph "稀疏专家路径 (Sparse Path)"
        C -- 为每个 Token 选择 Top-K --&gt; D[Expert 1];
        C -- 为每个 Token 选择 Top-K --&gt; E[...];
        C -- 为每个 Token 选择 Top-K --&gt; F[Expert N];
        D &amp; E &amp; F --&gt; G("对 Top-K 专家输出进行加权求和");
    end

    subgraph "共享路径 (Dense Path)"
        B --&gt; H(共享路径输出);
    end

    G --&gt; I{"add"};
    H --&gt; I;
    I --&gt; L[最终输出];

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style L fill:#ccf,stroke:#333,stroke-width:2px
</code></pre> <p>如上图所示，输入 Token 会兵分两路：一路通过路由器选择 Top-K 个稀疏专家进行<strong>专业化</strong>计算；另一路则全部通过一个共享专家进行<strong>通用化</strong>计算。这种设计好比一个会诊流程：共享专家如同经验丰富的“全科医生”，处理基础和通用的语言模式；稀疏专家则像“专科医生”，处理更细分、更专业的知识。<code class="language-plaintext highlighter-rouge">Qwen3NextSparseMoeBlock</code> 的代码清晰地实现了这一并行结构，共享专家的存在为模型提供了一个稳定的计算基座，极大地提升了模型的鲁棒性。</p> <h2 id="二混合注意力与-gateddeltanet-详解">二、混合注意力与 GatedDeltaNet 详解</h2> <p>Qwen3-Next 采用混合注意力架构以高效处理长上下文。它在不同层交替使用 O(N²) 复杂度的标准注意力和 O(N) 复杂度的线性注意力，实现了能力与效率的平衡。</p> <pre><code class="language-mermaid">graph LR
    Input --&gt; Layer_i["Layer i&lt;br/&gt;&lt;b&gt;Gated Full Attention&lt;/b&gt;&lt;br/&gt;(O(N²) 复杂度, 精准捕捉)"];
    Layer_i --&gt; Layer_i_plus_1["Layer i+1&lt;br/&gt;&lt;b&gt;Gated DeltaNet (线性)&lt;/b&gt;&lt;br/&gt;(O(N) 复杂度, 高效长距)"];
    Layer_i_plus_1 --&gt; Output;
</code></pre> <p><strong>GatedDeltaNet 计算机制详解</strong></p> <p>其中，<code class="language-plaintext highlighter-rouge">Qwen3NextGatedDeltaNet</code> 是实现线性注意力的核心模块。它通过一套精密的“输入-卷积-门控-递归-输出”流程，在保持线性复杂度的同时，实现了对长距离依赖的有效建模。</p> <ol> <li> <p><strong>输入映射</strong>: 输入 <code class="language-plaintext highlighter-rouge">hidden_states</code> 被线性层 (<code class="language-plaintext highlighter-rouge">in_proj_qkvz</code>, <code class="language-plaintext highlighter-rouge">in_proj_ba</code>) 投影成一系列中间状态：<code class="language-plaintext highlighter-rouge">query</code>, <code class="language-plaintext highlighter-rouge">key</code>, <code class="language-plaintext highlighter-rouge">value</code> 及用于门控的 <code class="language-plaintext highlighter-rouge">z</code>, <code class="language-plaintext highlighter-rouge">b</code>, <code class="language-plaintext highlighter-rouge">a</code>。</p> </li> <li><strong>因果卷积 (局部信息)</strong>: 拼接后的 <code class="language-plaintext highlighter-rouge">qkv</code> 张量经过一个一维因果卷积，它像一个滑动窗口，用于高效捕捉每个 Token 与其附近邻居之间的局部上下文。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 文件: modeling_qwen3_next.py -&gt; class Qwen3NextGatedDeltaNet
</span><span class="n">mixed_qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">causal_conv1d_fn</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">mixed_qkv</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">conv1d</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="c1"># ...
</span><span class="p">)</span>
</code></pre></div> </div> </li> <li><strong>门控信号生成</strong>: 模型从投影 <code class="language-plaintext highlighter-rouge">a</code> 和 <code class="language-plaintext highlighter-rouge">b</code> 中，学习两个关键的、类似 RNN 的门控信号： <ul> <li><code class="language-plaintext highlighter-rouge">beta</code> (输入门): 通过 <code class="language-plaintext highlighter-rouge">sigmoid</code> 函数，控制有多少新信息（<code class="language-plaintext highlighter-rouge">value</code>）可以被写入“状态”。</li> <li><code class="language-plaintext highlighter-rouge">g</code> (遗忘门): 控制历史信息的衰减率，决定“记忆”能保留多久。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">()</span>
<span class="n">g</span> <span class="o">=</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">A_log</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">exp</span><span class="p">()</span> <span class="o">*</span> <span class="n">F</span><span class="p">.</span><span class="nf">softplus</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">dt_bias</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> </li> <li><strong>门控增量规则 (全局信息)</strong>: 将 <code class="language-plaintext highlighter-rouge">q, k, v</code> 和门控信号 <code class="language-plaintext highlighter-rouge">beta, g</code> 送入核心的递归函数，进行全局信息传递。这一步是实现 O(N) 复杂度的关键。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">core_attn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">chunk_gated_delta_rule</span><span class="p">(</span>
    <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="bp">...</span>
<span class="p">)</span>
</code></pre></div> </div> </li> <li><strong>输出门控</strong>: <code class="language-plaintext highlighter-rouge">GatedDeltaNet</code> 的输出 <code class="language-plaintext highlighter-rouge">core_attn_out</code> 还会被 <code class="language-plaintext highlighter-rouge">z</code> 进行最终的门控调制，然后才传递给下一层。 <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">core_attn_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">core_attn_out</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</code></pre></div> </div> <p>通过这套流程，<code class="language-plaintext highlighter-rouge">GatedDeltaNet</code> 实现了“卷积捕捉局部，递归传递全局”的高效信息处理模式。</p> </li> </ol> <h2 id="三多词元预测-mtp">三、多词元预测 (MTP)</h2> <p>多词元预测 (MTP) 是一种先进的预训练目标，它在预训练和推理阶段都有显著增益。</p> <ul> <li><strong>预训练阶段</strong>: 传统模型在 <code class="language-plaintext highlighter-rouge">t</code> 时刻只预测 <code class="language-plaintext highlighter-rouge">t+1</code> 的词元。MTP 则要求模型在 <code class="language-plaintext highlighter-rouge">t</code> 时刻同时预测 <code class="language-plaintext highlighter-rouge">t+1</code>, <code class="language-plaintext highlighter-rouge">t+2</code>, …, <code class="language-plaintext highlighter-rouge">t+n</code> 多个未来的词元。这有助于模型学习更具前瞻性的语言模式，提升了其对因果关系的理解。</li> <li><strong>推理阶段</strong>: MTP 的能力天然适配<strong>思辨解码 (Speculative Decoding)</strong>。模型可以一次性生成 <code class="language-plaintext highlighter-rouge">n</code> 个候选 token，再由系统并行验证，从而在命中率高的情况下，数倍提升解码速度，是实现高效推理的关键技术之一。</li> </ul> <p><code class="language-plaintext highlighter-rouge">Qwen3NextPreTTrainedModel</code> 类中的 <code class="language-plaintext highlighter-rouge">_keys_to_ignore_on_load_unexpected = [r"^mtp.*"]</code> 这行代码，证实了 MTP 是其预训练阶段的一部分，相关的权重已经融入模型，即使推理代码中没有显式的多头预测结构。</p> <h2 id="四其他架构优化零中心化-rmsnorm">四、其他架构优化：零中心化 RMSNorm</h2> <p>Qwen3-Next 对 RMSNorm 进行了精巧的改进，以增强训练稳定性。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 文件: modeling_qwen3_next.py -&gt; class Qwen3NextRMSNorm
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 核心: 乘以 (1.0 + weight)，而非直接乘 weight
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_norm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">.</span><span class="nf">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>由于 <code class="language-plaintext highlighter-rouge">self.weight</code> 初始化为 0，该层在训练初期近似于一个无参数的纯归一化操作。这个看似微小的改动有助于稳定梯度在网络深层的传播，特别是在训练初期，对避免梯度爆炸或消失问题有积极作用。</p> <h2 id="总结">总结</h2> <p><code class="language-plaintext highlighter-rouge">Qwen3-Next</code> 的设计哲学是在“大而全”和“小而美”之间寻找最佳平衡点。它并非依赖单一技术的颠覆，而是通过一系列精心设计的技术组合：</p> <ul> <li><strong>高稀疏度 MoE 与共享专家</strong>，平衡了计算负载与模型鲁棒性。</li> <li><strong>混合注意力与 GatedDeltaNet</strong>，平衡了对不同距离上下文的建模能力与计算效率。</li> <li><strong>MTP</strong>，同时优化了模型的预训练效果与推理速度。</li> <li><strong>零中心化 RMSNorm</strong> 等细节，提升了训练的稳定性。</li> </ul> <p>这些技术细节的深度融合，最终造就了这款在性能、效率和训练成本上都极具竞争力的下一代大语言模型。</p>]]></content><author><name></name></author><category term="llm"/><summary type="html"><![CDATA[transformers 库近期合并了 Qwen3-Next 的 PR，正式将其纳入官方生态。这个 PR 提交了 Qwen3-Next-80B-A3B-Instruct 模型的实现，该模型被介绍为下一代基础模型，针对超长上下文和大规模参数效率进行了深度优化。官方的描述为：]]></summary></entry><entry><title type="html">MNN模型支持：面壁小钢炮MiniCPM-V-4</title><link href="https://zhaode.wang/blog/2025/minicpm/" rel="alternate" type="text/html" title="MNN模型支持：面壁小钢炮MiniCPM-V-4"/><published>2025-09-05T00:00:00+00:00</published><updated>2025-09-05T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/minicpm</id><content type="html" xml:base="https://zhaode.wang/blog/2025/minicpm/"><![CDATA[<p>面壁智能的MiniCPM模型，自发布以来就被誉为“端侧小钢炮”，以其在端侧设备上出色的多模态能力而闻名。MNN作为一个端侧推理框架，支持目前主流的端侧模型，端侧小钢炮的模型也不例外。这里记录一下MNN对MiniCPM-V-4的支持过程。</p> <h2 id="minicpm-v-4模型介绍">MiniCPM-V-4模型介绍</h2> <p>首先，简单了解一下 <strong>MiniCPM-V-4</strong>：</p> <ul> <li><strong>参数规模</strong>：4.1B，由一个400M的视觉编码器和一个3B的语言模型组成。</li> <li><strong>模型性能</strong>：在权威的OpenCompass评测中得分69.0，表现优于许多同量级模型。在旗舰手机上，可以实现流畅的实时交互（首token延迟&lt;2s，解码速度&gt;17 token/s）。</li> </ul> <p>对这个模型的支持主要工作是对他的视觉处理部分的支持</p> <h2 id="模型导出">模型导出</h2> <p>我们主要针对模型的视觉处理部分（Vision Encoder）进行了三项关键优化。</p> <p><strong>优化策略一：变动态搜索为静态计算的图像切分</strong></p> <p><strong>问题背景</strong>：为了处理高清大图，MiniCPM-V 4.0会智能地将图像切分成多个小块（Slices）。原始实现需要通过一个搜索算法，在运行时动态计算出最佳的切分网格，这个过程在端侧会带来不必要的延迟。</p> <p><strong>我们的解决方案</strong>：我们发现这个搜索过程可以被一个确定性的数学模型替代。通过分析图像的面积、长宽比等几何特征，我们可以用一次前向计算直接得出最优的切分方案。</p> <p><strong>优化后的核心算法逻辑</strong>：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_image_processing_plan</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">original_size</span><span class="p">,</span> <span class="n">max_slice_nums</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">scale_resolution</span><span class="o">=</span><span class="mi">448</span><span class="p">):</span>
    <span class="n">original_height</span><span class="p">,</span> <span class="n">original_width</span> <span class="o">=</span> <span class="n">original_size</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">original_width</span> <span class="o">*</span> <span class="n">original_height</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">scale_resolution</span> <span class="o">*</span> <span class="n">scale_resolution</span><span class="p">)</span>
    <span class="n">multiple</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">ratio</span><span class="p">),</span> <span class="n">max_slice_nums</span><span class="p">)</span>

    <span class="c1"># 智能网格划分算法
</span>    <span class="k">if</span> <span class="n">multiple</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="p">{</span><span class="n">multiple</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">multiple</span><span class="p">,</span> <span class="n">multiple</span> <span class="o">+</span> <span class="mi">1</span><span class="p">}:</span>
            <span class="k">if</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">num</span> <span class="o">&lt;=</span> <span class="n">max_slice_nums</span><span class="p">:</span>
                <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">while</span> <span class="n">m</span> <span class="o">*</span> <span class="n">m</span> <span class="o">&lt;=</span> <span class="n">num</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">num</span> <span class="o">%</span> <span class="n">m</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">candidates</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">num</span> <span class="o">//</span> <span class="n">m</span><span class="p">))</span>
                    <span class="n">m</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># 选择最接近原图长宽比的网格
</span>        <span class="n">log_ratio</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">original_width</span> <span class="o">/</span> <span class="n">original_height</span><span class="p">)</span>
        <span class="n">best_grid</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">g</span><span class="p">:</span> <span class="nf">abs</span><span class="p">(</span><span class="n">log_ratio</span> <span class="o">-</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
</code></pre></div></div> <p><strong>优化效果</strong>：这个改动将一个复杂的动态逻辑，简化为了C++中易于实现的纯数学运算。</p> <p><strong>优化策略二：用一次Permute操作统一几何变换</strong></p> <p><strong>问题背景</strong>：原始的图像切分和重排逻辑，涉及到多次<code class="language-plaintext highlighter-rouge">reshape</code>和<code class="language-plaintext highlighter-rouge">transpose</code>操作。这不仅代码繁琐，而且每次操作都可能触发内存拷贝，效率不高。</p> <p><strong>我们的解决方案</strong>：我们深入分析后发现，切图和重排的本质，都是在更高维度上对张量（Tensor）的数据进行位置重排。因此，这些看似复杂的多步操作，完全可以通过一次精心设计的<code class="language-plaintext highlighter-rouge">reshape</code>和一次<code class="language-plaintext highlighter-rouge">permute</code>（维度置换）操作来完成。</p> <p><strong>实现对比 (示意)</strong>：</p> <ul> <li><strong>原始逻辑</strong>： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 步骤1：切分
</span><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(...)</span>
<span class="c1"># 步骤2：多次重排
</span><span class="n">temp1</span> <span class="o">=</span> <span class="n">patches</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(...)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">temp1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(...).</span><span class="nf">transpose</span><span class="p">(...)</span>
</code></pre></div> </div> </li> <li><strong>优化后逻辑</strong>： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 一步到位
# 先构建一个包含所有维度信息的高维张量
</span><span class="n">high_dim_tensor</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(...)</span>
<span class="c1"># 再通过一次permute完成所有数据的位置交换
</span><span class="n">permuted_tensor</span> <span class="o">=</span> <span class="n">high_dim_tensor</span><span class="p">.</span><span class="nf">permute</span><span class="p">(...)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">permuted_tensor</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(...)</span>
</code></pre></div> </div> <p><strong>优化效果</strong>：这一改动对C++的实现极为友好。原本需要编写复杂循环和索引计算的代码，现在简化为对底层<code class="language-plaintext highlighter-rouge">permute</code>算子的一次调用，代码更简洁，执行效率也更高。</p> </li> </ul> <p><strong>优化策略三：为计算图导出重构推理逻辑</strong></p> <p><strong>问题背景</strong>：为了实现跨平台部署，模型必须能够导出计算图。</p> <p><strong>我们的解决方案</strong>：遵循“动静分离”的原则，我们将所有动态逻辑从模型的核心计算图中剥离出去。</p> <ul> <li><strong>预处理阶段完成填充</strong>：在C++的图像预处理阶段，就将所有输入数据填充（Pad）到固定的最大尺寸。这样，送入模型的张量尺寸永远是静态的。</li> <li><strong>简化和前置掩码计算</strong>：将动态生成注意力掩码（Attention Mask）的逻辑，同样移到模型外部的预处理环节。</li> <li><strong>引入缓存机制</strong>：对于可复用的计算结果，如位置编码，我们增加了缓存，避免在每次推理中重复生成。</li> </ul> <p><strong>优化效果</strong>：经过重构，模型变成了一个完全静态的计算图，可以顺利地导出为ONNX文件，为最终在MNN上的高效运行铺平了道路。</p> <h2 id="模型推理">模型推理</h2> <p>理论层面的优化最终需要通过高效、稳健的代码实现来落地。我们将 MiniCPM 视觉处理的核心逻辑在 MNN 框架内，通过 C++ 进行了全面重构。下面，我们将详细拆解 <code class="language-plaintext highlighter-rouge">minicpmVisionProcess</code> 函数的实现，展示如何将优化思想转化为高性能的推理管线。</p> <h3 id="核心实现reoderimage-变换函数">核心实现：<code class="language-plaintext highlighter-rouge">reoderImage</code> 变换函数</h3> <p>整个流程中最精妙的部分被封装在一个名为 <code class="language-plaintext highlighter-rouge">reoderImage</code> 的 Lambda 函数中。它体现了<strong>优化策略二（用一次 Permute 统一几何变换）</strong>，负责将输入图像高效地缩放、切分并重排为模型所需的 Patch 序列。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">reoderImage</span> <span class="o">=</span> <span class="p">[</span><span class="k">this</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">patchSize</span><span class="p">](</span>
    <span class="n">Express</span><span class="o">::</span><span class="n">VARP</span> <span class="n">img</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">targetSize</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">grid</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span> <span class="n">tgtSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 1. 图像预处理：缩放、归一化、颜色空间转换</span>
    <span class="k">auto</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">MNN</span><span class="o">::</span><span class="n">CV</span><span class="o">::</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">...);</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">Express</span><span class="o">::</span><span class="n">_Convert</span><span class="p">(</span><span class="n">patches</span><span class="p">,</span> <span class="n">NCHW</span><span class="p">);</span>

    <span class="c1">// 2. 高效切片与重排的核心：Reshape -&gt; Permute -&gt; Reshape</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">Express</span><span class="o">::</span><span class="n">_Reshape</span><span class="p">(</span><span class="n">patches</span><span class="p">,</span> <span class="p">{</span>
        <span class="n">channel</span><span class="p">,</span> <span class="n">gridH</span><span class="p">,</span> <span class="n">numPatchesH</span><span class="p">,</span> <span class="n">patchSize</span><span class="p">,</span> <span class="n">gridW</span><span class="p">,</span> <span class="n">numPatchesW</span><span class="p">,</span> <span class="n">patchSize</span>
    <span class="p">});</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">Express</span><span class="o">::</span><span class="n">_Permute</span><span class="p">(</span><span class="n">patches</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">});</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">Express</span><span class="o">::</span><span class="n">_Reshape</span><span class="p">(</span><span class="n">patches</span><span class="p">,</span> <span class="p">{</span>
        <span class="n">gridH</span> <span class="o">*</span> <span class="n">gridW</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">patchSize</span><span class="p">,</span> <span class="n">numPatchesH</span> <span class="o">*</span> <span class="n">numPatchesW</span> <span class="o">*</span> <span class="n">patchSize</span>
    <span class="p">});</span>
    <span class="c1">// ...</span>
    <span class="k">return</span> <span class="n">patches</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div></div> <p>此函数的核心在于，它将复杂的切图逻辑收敛到了一次 <code class="language-plaintext highlighter-rouge">_Permute</code> 原子操作。通过先 <code class="language-plaintext highlighter-rouge">_Reshape</code> 将图像张量提升到包含了网格、Patch、通道等所有几何信息的 7 维，然后一次 <code class="language-plaintext highlighter-rouge">_Permute</code> 就完成了所有数据块的位置交换，最后 <code class="language-plaintext highlighter-rouge">_Reshape</code> 回目标形状。这套操作避免了繁琐的循环和内存拷贝，为 C++ 的高性能实现提供了巨大便利。</p> <h3 id="静态模型输入构建">静态模型输入构建</h3> <p>遵循<strong>优化策略三（动静分离）</strong>的原则，我们在模型外部的 C++ 预处理阶段，准备好了 Vision Encoder 所需的全部四个静态输入张量。</p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">pixel_values</code> (像素值)</strong>: <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">globalImage</span> <span class="o">=</span> <span class="n">reoderImage</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">globalSize</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tgtSize</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">refineImage</span> <span class="o">=</span> <span class="n">reoderImage</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">refineSize</span><span class="p">,</span> <span class="n">sliceGrids</span><span class="p">,</span> <span class="n">tgtSize</span><span class="p">);</span>
<span class="c1">// 对 globalImage 进行 Padding，使其尺寸与 refineImage 对齐</span>
<span class="n">globalImage</span> <span class="o">=</span> <span class="n">_Pad</span><span class="p">(</span><span class="n">globalImage</span><span class="p">,</span> <span class="p">...);</span>
<span class="k">auto</span> <span class="n">pixel_values</span> <span class="o">=</span> <span class="n">_Concat</span><span class="p">({</span><span class="n">globalImage</span><span class="p">,</span> <span class="n">refineImage</span><span class="p">},</span> <span class="mi">0</span><span class="p">);</span>
</code></pre></div> </div> <p>我们分别对全局图像（1x1网格）和高清切片图像调用 <code class="language-plaintext highlighter-rouge">reoderImage</code>。然后，将较小的 <code class="language-plaintext highlighter-rouge">globalImage</code> 填充（Pad）到与 <code class="language-plaintext highlighter-rouge">refineImage</code> 相同的尺寸，最后将它们拼接（Concat）成一个 Batch，送入模型。所有动态尺寸处理都在模型外部完成。</p> </li> <li><strong><code class="language-plaintext highlighter-rouge">position_ids</code> (位置ID)</strong>: <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h_idx</span> <span class="o">&lt;</span> <span class="n">nb_patches_h</span><span class="p">;</span> <span class="o">++</span><span class="n">h_idx</span><span class="p">)</span> <span class="p">{</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">w_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">w_idx</span> <span class="o">&lt;</span> <span class="n">nb_patches_w</span><span class="p">;</span> <span class="o">++</span><span class="n">w_idx</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">long</span> <span class="n">bucket_h</span> <span class="o">=</span> <span class="n">floor</span><span class="p">((</span><span class="n">h_idx</span> <span class="o">/</span> <span class="n">nb_patches_h</span><span class="p">)</span> <span class="o">*</span> <span class="n">patchesPerSide</span><span class="p">);</span>
    <span class="kt">long</span> <span class="n">bucket_w</span> <span class="o">=</span> <span class="n">floor</span><span class="p">((</span><span class="n">w_idx</span> <span class="o">/</span> <span class="n">nb_patches_w</span><span class="p">)</span> <span class="o">*</span> <span class="n">patchesPerSide</span><span class="p">);</span>
    <span class="n">posPtr</span><span class="p">[...]</span> <span class="o">=</span> <span class="n">bucket_h</span> <span class="o">*</span> <span class="n">patchesPerSide</span> <span class="o">+</span> <span class="n">bucket_w</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">}</span>
</code></pre></div> </div> <p>这部分代码为每个 Patch 生成高精度的位置编码。它通过线性插值，将不同分辨率的 Patch 网格，统一映射到一个固定的 <code class="language-plaintext highlighter-rouge">patchesPerSide</code> x <code class="language-plaintext highlighter-rouge">patchesPerSide</code> 虚拟坐标系中，确保模型能准确理解每个 Patch 的相对空间位置。</p> </li> <li><strong><code class="language-plaintext highlighter-rouge">attention_mask</code> (注意力掩码) 和 <code class="language-plaintext highlighter-rouge">tgt_sizes</code> (目标尺寸)</strong>: 我们同时生成 <code class="language-plaintext highlighter-rouge">attention_mask</code> 和 <code class="language-plaintext highlighter-rouge">tgt_sizes</code> 张量。前者用于在注意力计算中屏蔽掉因 Padding 产生的无效数据；后者则向模型传递每个切片的原始 Patch 尺寸，作为计算的元数据。</li> </ol> <h3 id="模型推理与多模态指令生成">模型推理与多模态指令生成</h3> <p>当所有输入张量准备就绪后，调用 MNN 引擎执行 Vision Encoder 的推理。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">imageEmbedding</span> <span class="o">=</span> <span class="n">mVisionModule</span><span class="o">-&gt;</span><span class="n">onForward</span><span class="p">({</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">tgt_sizes</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</code></pre></div></div> <p>推理完成后，我们得到包含所有图像特征的 <code class="language-plaintext highlighter-rouge">imageEmbedding</code>。最后一步，是构建一个语言模型能够理解的“多模态指令”序列。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">imgIds</span><span class="p">;</span>
<span class="c1">// 插入 &lt;image&gt; token 和 64 个 &lt;unk&gt; 占位符</span>
<span class="n">imgIds</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">mVisionStart</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="n">visionLen</span><span class="p">;</span> <span class="n">p</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">imgIds</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">mVisionPad</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">imgIds</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">mVisionEnd</span><span class="p">);</span>

<span class="c1">// 为每个 slice 插入 &lt;slice&gt; token 和 64 个 &lt;unk&gt; 占位符</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">B</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">imgIds</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">visionSliceStart</span><span class="p">);</span>
    <span class="c1">// ... 插入 64 个 &lt;unk&gt; 占位符 ...</span>
    <span class="n">imgIds</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">visionSliceEnd</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">return</span> <span class="n">imgIds</span><span class="p">;</span>
</code></pre></div></div> <p>这段代码使用特殊的 Token ID 来标记全局图像（<code class="language-plaintext highlighter-rouge">&lt;image&gt;</code>）和每个切片（<code class="language-plaintext highlighter-rouge">&lt;slice&gt;</code>）的边界，并在其中填充固定数量（64个）的占位符。这些占位符将在后续步骤中，被 <code class="language-plaintext highlighter-rouge">imageEmbedding</code> 中的实际视觉特征向量所替换，从而完成图文信息的最终融合。</p> <p>通过这套精心设计的 C++ 管线，一张原始图像被高效地转换为了一个结构精密、可供多模态大模型直接处理的输入序列，成功将算法原型产品化。</p> <h2 id="模型下载">模型下载</h2> <p>我们已经将转换好的 MNN 模型上传至社区，欢迎下载体验：</p> <ul> <li><strong>ModelScope:</strong> <a href="https://modelscope.cn/models/MNN/MiniCPM-V-4-MNN">https://modelscope.cn/models/MNN/MiniCPM-V-4-MNN</a></li> <li><strong>Hugging Face:</strong> <a href="https://huggingface.co/taobao-mnn/MiniCPM-V-4-MNN">https://huggingface.co/taobao-mnn/MiniCPM-V-4-MNN</a></li> </ul>]]></content><author><name></name></author><category term="MNN"/><category term="llm"/><summary type="html"><![CDATA[面壁智能的MiniCPM模型，自发布以来就被誉为“端侧小钢炮”，以其在端侧设备上出色的多模态能力而闻名。MNN作为一个端侧推理框架，支持目前主流的端侧模型，端侧小钢炮的模型也不例外。这里记录一下MNN对MiniCPM-V-4的支持过程。]]></summary></entry><entry><title type="html">端侧LLM硬件系列（一）：内存带宽</title><link href="https://zhaode.wang/blog/2025/device-llm-memory-bandwidth/" rel="alternate" type="text/html" title="端侧LLM硬件系列（一）：内存带宽"/><published>2025-09-02T00:00:00+00:00</published><updated>2025-09-02T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/device-llm-memory-bandwidth</id><content type="html" xml:base="https://zhaode.wang/blog/2025/device-llm-memory-bandwidth/"><![CDATA[<p>在AI大模型席卷一切的今天，我们都期待手机成为真正的“口袋里的AI大脑”。但你有没有感觉，手机上的AI助手总是慢半拍？无论是想让它快速总结一篇长文，还是在图片编辑时使用“AI消除”功能，那种等待的延迟感，总在提醒我们“理想与现实的差距”。</p> <p>这个瓶颈，或许不在于芯片厂商们大力宣传的算力（TOPS）数字有多高，而在于一个常常被忽视的参数——内存带宽。</p> <p>这就像拥有了一台超级跑车的引擎（AI计算核心），却只给它配了一根细细的油管，空有一身力气却踩不上油门。</p> <h2 id="为什么说llm的性能主要卡在内存带宽"><strong>为什么说LLM的性能，主要卡在内存带宽？</strong></h2> <p>要理解这个问题，我们首先需要知道，大模型在手机上生成内容，分为两个关键阶段：</p> <ol> <li><strong>Prefill（预填充/提示处理）</strong>：这是模型“读懂问题”的阶段。它会并行处理你输入的所有文字（Prompt），比如“帮我写一首关于星空的诗”。这个阶段计算量大，可以充分利用NPU/GPU的并行计算能力，速度通常很快。</li> <li><strong>Decoding（解码/逐字生成）</strong>：这是模型“思考并回答”的阶段。它会一个字一个字地生成答案，比如“浩瀚的…夜空…”。每生成一个新字，都需要把前面所有的字连同新生成的字一起，再和全部的模型参数进行一次运算。</li> </ol> <p><strong>而瓶颈，恰恰就出在这个决定了用户最终体验的“解码”阶段。</strong></p> <blockquote> <p><strong>一个反直觉的现象：为什么强大的NPU/GPU，在解码时反而可能不如CPU？</strong></p> <p>很多技术测试都发现一个奇怪的现象：在手机上运行LLM时，<strong>Prefill阶段用NPU/GPU处理，速度极快。但到了最关键的Decoding阶段，它们的生成速度经常还不如直接用CPU。</strong></p> <p><strong>原因正是内存带宽的制约，我们可以用一个简单的公式来揭示真相：</strong></p> \[\text{理论解码速度（Tokens/s）} \approx \frac{\text{内存带宽（GB/s）}}{\text{模型单次推理数据量（GB）}}\] <p>这个公式告诉我们，解码速度的上限，完全取决于“数据搬运”的效率。<strong>每生成一个字（Token），都需要将重达数GB的模型参数从系统内存（RAM）完整地“搬运”一遍。</strong></p> <ul> <li><strong>NPU/GPU的“窘境”</strong>：它们算力再强，也得等数据从内存中漫长地传输过来。大部分时间都在“等米下锅”，强大的算力被闲置，自然快不起来。</li> <li><strong>CPU的“智慧”</strong>：CPU核心虽少，但它自带的<strong>巨大高速缓存（Cache）</strong> 相当于一个“厨房里的冰箱”。它可以把最常用的参数暂存起来随用随取，避免了频繁访问“几公里外的大仓库（RAM）”，因此在解码这种“计算量小、数据访问频繁”的任务上反而效率更高。</li> </ul> <p><strong>让我们量化一下这个瓶颈：</strong> 假设一款手机的内存带宽是<strong>64 GB/s</strong>，运行一个需要加载<strong>4GB</strong>参数的7B模型。理论上，它每秒最多只能生成 <code class="language-plaintext highlighter-rouge">64 ÷ 4 = 16</code> 个Token。一个汉字约等于2个Token，这意味着手机每秒最多能生成8个汉字——这个速度已经低于人类的平均阅读速度，用户会明显感觉到“卡顿”。如果想让体验流畅（如32 Tokens/s），内存带宽就需要翻倍到<strong>128 GB/s</strong>。</p> <p><strong>至此，结论已经非常清晰：</strong> 对于决定LLM交互流畅度的解码环节，数据传输效率远比纯粹的计算能力更重要。这就是AI社区将解码称为“内存密集型”（Memory-bound）任务的根本原因。</p> </blockquote> <hr/> <h2 id="拆解带宽瓶颈天花板与破局之路"><strong>拆解带宽瓶颈：“天花板”与“破局之路”</strong></h2> <p>要理解手机AI性能的真正瓶颈，我们必须深入到内存带宽的“引擎室”，看看它由什么构成，又被什么所限制。</p> <p>内存带宽的计算公式看似简单，由两个核心参数相乘决定：</p> \[\text{内存带宽（GB/s）} = \frac{\text{数据传输速率（MT/s）} \times \text{总线宽度（bits）}}{8}\] <ul> <li><strong>数据传输速率 (Data Rate):</strong> 这是我们最常听到的参数，比如LPDDR5X-8533。它就像是数据高速公路上的<strong>“最高限速”</strong>，是各大厂商目前提升带宽最直接、竞争最激烈的战场。</li> <li><strong>内存总线宽度 (Bus Width):</strong> 这可以理解为高速公路的<strong>“车道数量”</strong>。目前，旗舰手机芯片几乎无一例外地被“焊死”在了<strong>64-bit</strong>这个宽度上。</li> </ul> <p>问题来了：既然车道越多，数据并行能力越强，为什么手机芯片不像PC显卡（如RTX 4090拥有384-bit总线）那样，把“路”修得更宽呢？</p> <p>答案是，在手机内部寸土寸金的空间里，拓宽总线这条“物理之路”几乎已经走到了尽头。它面临着三座难以逾越的大山，也就是我们所说的<strong>“物理天花板”</strong>：</p> <ol> <li><strong>空间限制：</strong> 每一bit总线都需要一条独立的、像头发丝一样精密的PCB走线。将64-bit翻倍到128-bit，意味着布线复杂度和面积需求的指数级增长，这在手机主板上是不可想象的。</li> <li><strong>信号干扰：</strong> “车道”越多、越密，彼此间的“信号串扰”就越严重，数据很容易出错。为了保证信号同步，所有走线的长度必须像军人列队般精确对齐，这对设计和制造工艺是巨大的挑战。</li> <li><strong>功耗失控：</strong> 每一条“车道”都需要消耗电能来驱动数据传输。粗暴地加宽总线会直接导致功耗和发热激增，这对于依靠电池续命的手机来说是致命的。</li> </ol> <p>既然横向的“扩路”已触及天花板，聪明的芯片工程师们便另辟蹊径，开启了三条通往未来的<strong>“破局之路”</strong>。</p> <h4 id="破局之路一纵向提速--不断刷新的数据率-lpddr6及后续"><strong>破局之路一：纵向提速 —— 不断刷新的数据率 (LPDDR6及后续)</strong></h4> <p>这是最经典、最确定的演进路线：既然路宽不了，那就让车速更快。从LPDDR4X的4266MT/s，到LPDDR5X的8533MT/s，再到即将登场的<strong>LPDDR6</strong>（起步速率可达<strong>12800MT/s</strong>），这条路一直在坚定地向前延伸。它虽然可靠，但边际效应也开始显现，更高的速率对功耗和信号控制的要求也越来越苛刻。</p> <h4 id="破局之路二空间折叠--用3d封装拓宽总线-先进封装技术"><strong>破局之路二：空间折叠 —— 用3D封装拓宽总线 (先进封装技术)</strong></h4> <p>既然平面上修路已无可能，那就“向上”发展，搭建“立交桥”。<strong>先进的3D封装技术</strong>，允许将内存芯片（DRAM）直接堆叠在SoC芯片的上方，通过成千上万条超短的垂直通道连接。这就像把两个城市直接叠在一起，交通距离无限缩短。</p> <ul> <li><strong>核心优势：</strong> 极短的距离意味着极低的功耗和信号干扰，这使得<strong>在垂直方向上实现128-bit甚至更宽的总线</strong>成为可能。PC领域的HBM（高带宽内存）正是该技术的体现，而移动端正在全力探索如何将其小型化、低功耗化，这被视为未来颠覆带宽格局的关键。</li> </ul> <h4 id="破局之路三修建前置仓--用系统级缓存抄近道-slc"><strong>破局之路三：修建“前置仓” —— 用系统级缓存抄近道 (SLC)</strong></h4> <p>如果每次都去几公里外的大仓库（系统内存）取货太慢，那就在车间旁边建一个“前置仓”（缓存）。<strong>扩大和优化系统级缓存（System Level Cache, SLC）</strong>正是这一思路的体现。这块大容量高速缓存由CPU、GPU、NPU等所有计算核心共享。</p> <ul> <li><strong>工作原理：</strong> 在运行LLM时，芯片可以将最核心的模型参数（如注意力权重）提前加载到SLC这个“前置仓”里。当NPU需要时，直接从“隔壁”高速调取，<strong>彻底避免了访问外部系统内存的漫长等待</strong>。苹果和联发科都是该策略的忠实拥趸，通过不断增大片上缓存，在实际AI应用中获得了远超理论带宽的能效表现。</li> </ul> <hr/> <h2 id="2025年旗舰芯片内存带宽大比拼谁的水管最粗"><strong>2025年旗舰芯片内存带宽大比拼：谁的“水管”最粗？</strong></h2> <p>进入2025年，各家旗舰芯片在内存带宽上展开激烈角逐。基于现有信息和行业预测，我们整理了以下旗舰芯片的内存系统分析：</p> <table> <thead> <tr> <th style="text-align: left">芯片型号</th> <th style="text-align: left">内存标准</th> <th style="text-align: left">理论峰值速率</th> <th style="text-align: left">理论峰值带宽</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>联发科 天玑9400/9500</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">10667 Mbps</td> <td style="text-align: left"><strong>85.3 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>高通 骁龙 8 Elite</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">9600 Mbps</td> <td style="text-align: left"><strong>76.8 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>三星 Exynos 2500</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">9600 Mbps</td> <td style="text-align: left"><strong>76.8 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>高通 骁龙 8 Elite Gen 5</strong></td> <td style="text-align: left">LPDDR6</td> <td style="text-align: left"><strong>最高14400 Mbps</strong></td> <td style="text-align: left"><strong>最高115.2 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>苹果 A19 Pro</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">待定</td> <td style="text-align: left">待定</td> </tr> </tbody> </table> <ul> <li> <p><strong>联发科：激进的LPDDR5X优化者</strong> 联发科一直致力于在LPDDR5X标准下压榨极限。<strong>天玑9400/9500</strong>系列率先将内存速率推高到惊人的<strong>10667Mbps</strong>，一举刷新行业记录，并以<strong>85.3GB/s</strong>的理论带宽领跑。同时，它还通过增大片上缓存（L3/SLC）来减少对主内存的访问，进一步提升了数据处理效率。</p> </li> <li> <p><strong>高通：LPDDR6的先行者</strong> 高通在<strong>骁龙8 Elite</strong>上采用了<strong>9600Mbps</strong>的LPDDR5X，与三星Exynos 2500并驾齐驱。但更引人注目的是，它很可能成为首个在<strong>骁龙8 Elite Gen 5</strong>上商用<strong>LPDDR6</strong>的手机芯片，这将使带宽一举突破100GB/s大关，达到<strong>115.2GB/s</strong>，为未来的端侧大模型奠定坚实基础。</p> </li> <li> <p><strong>苹果：从容的内存容量策略</strong> 尽管苹果的A系列芯片在内存带宽数据上相对封闭，但最新的A19 Pro芯片却做出了一个罕见而关键的升级：将<strong>iPhone 17 Pro</strong>的运行内存从8GB提升至<strong>12GB</strong>。这背后正是为了适配“Apple Intelligence”等AI功能，通过更大的容量来承载更复杂的模型和数据，间接缓解了带宽压力。这也可以看作是对‘破局之路三’中增大缓存、减少主内存访问策略的一种宏观体现。</p> </li> </ul> <hr/> <h2 id="总结"><strong>总结</strong></h2> <p>内存带宽无疑是当前制约手机LLM性能的头号瓶颈，其重要性甚至超过了单纯的算力堆砌。在这场竞赛中，联发科凭借对LPDDR5X的极致优化暂时领跑，而高通则可能通过率先拥抱LPDDR6标准，在未来实现颠覆性突破。</p> <p>然而，决定最终AI体验的，将是一个完整的“供水系统”。无论是继续提升内存速率，还是探索先进封装和更大系统缓存等未来技术，其最终目的都是为了构建一个高效的数据流。除了加粗“水管”（内存带宽），还需要强大的“水泵”（NPU/GPU算力）、足够大的“蓄水池”（内存容量），以及智能的“用水策略”（算法与软件优化）。</p> <p>只有当硬件、软件和AI模型三方协同进化，我们才能真正迎来那个理想的未来：在手机上与强大、流畅、懂你心意的AI进行实时互动。</p>]]></content><author><name></name></author><category term="llm"/><category term="device"/><summary type="html"><![CDATA[在AI大模型席卷一切的今天，我们都期待手机成为真正的“口袋里的AI大脑”。但你有没有感觉，手机上的AI助手总是慢半拍？无论是想让它快速总结一篇长文，还是在图片编辑时使用“AI消除”功能，那种等待的延迟感，总在提醒我们“理想与现实的差距”。]]></summary></entry><entry><title type="html">CoreML踩坑记：慎用Conv1D</title><link href="https://zhaode.wang/blog/2025/coreml-conv1d/" rel="alternate" type="text/html" title="CoreML踩坑记：慎用Conv1D"/><published>2025-08-18T00:00:00+00:00</published><updated>2025-08-18T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/coreml-conv1d</id><content type="html" xml:base="https://zhaode.wang/blog/2025/coreml-conv1d/"><![CDATA[<h4 id="背景">背景</h4> <p>最近在给MNN写CoreML后端，优化<code class="language-plaintext highlighter-rouge">Qwen2.5-Omni</code>的性能。在测试<code class="language-plaintext highlighter-rouge">BigVGAN</code>模型的时候发现结果对不齐，逐层调试后发现错误出现在<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>（一维转置卷积）算子上。</p> <p>因为MNN的后端是在通过线转换计算图的方式得到CoreML模型的，而且这个构图的过程都是我自己从头实现的，所以一般出错都是算子构造问题。但此类问题会出现在同类型的第一个算子后，而这次错误却是从<code class="language-plaintext highlighter-rouge">ups</code>的第二层开始出现的。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ups</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose1d</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">layer_idx</span><span class="p">),</span>
                    <span class="n">config</span><span class="p">.</span><span class="n">upsample_initial_channel</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">layer_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span>
                    <span class="n">kernel_size</span><span class="p">,</span>
                    <span class="n">stride</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">stride</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">upsample_rates</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">upsample_kernel_sizes</span><span class="p">))</span>
    <span class="p">]</span>
</code></pre></div></div> <h4 id="排查过程">排查过程</h4> <p>首先怀疑构图出错。于是我用<code class="language-plaintext highlighter-rouge">coremltools</code>把一个PyTorch的<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>模型转成<code class="language-plaintext highlighter-rouge">.mlmodelc</code>，把<code class="language-plaintext highlighter-rouge">model.mil</code>抠出来当“标准答案”，跟我自己写的图定义对比发现一模一样。</p> <p>这就奇怪了，图的定义没问题，那问题出在哪？</p> <p>没有思路，就将出错的算子单独转换成一个模型进行深入测试，发现了一个奇怪的现象，当我把<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>改成<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>后，结果就对了。</p> <p>理论上他们应该是等价的，但结果却不一样。为了进一步测试，我尝试把<strong>把Bias参数去掉，全设成0</strong>进行对比。</p> <p>结果它俩输出就一样了！这下基本可以确定，问题就出在CoreML执行1D转置卷积时，处理Bias的逻辑有Bug。从错误的输出现象看，Bias没有被正确地加到整个输出通道上，只影响了每个通道的头几个数。</p> <p>然后好奇第一层也有bias为何没问题呢？就把第一层的<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>提取成独立模型，同时改造出对应的<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>版本，结果它俩输出一致。看来这个问题是与算子的尺寸有关的。</p> <p>到这里我估计就不是构图问题了，而是CoreML内部的问题。</p> <h4 id="最终验证">最终验证</h4> <p>为了验证是否是CoreML的问题，我写了个Python脚本来做测试。脚本里建了两个一模一样的PyTorch模型，一个用<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>，一个用<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>，然后用<code class="language-plaintext highlighter-rouge">coremltools</code>转成CoreML模型，对比输出。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">coremltools</span> <span class="k">as</span> <span class="n">ct</span>

<span class="n">COMPUTE_UNIT</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="n">ComputeUnit</span><span class="p">.</span><span class="n">ALL</span>
<span class="c1"># COMPUTE_UNIT = ct.ComputeUnit.CPU_ONLY
# COMPUTE_UNIT = ct.ComputeUnit.CPU_AND_GPU
</span>
<span class="k">class</span> <span class="nc">Deconv1DModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">deconv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose1d</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">deconv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Deconv2DModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">deconv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">deconv</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">convert_and_save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>
    <span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span>
        <span class="n">traced_model</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ct</span><span class="p">.</span><span class="nc">TensorType</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)],</span>
        <span class="n">convert_to</span><span class="o">=</span><span class="sh">"</span><span class="s">mlprogram</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">minimum_deployment_target</span><span class="o">=</span><span class="n">ct</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">iOS18</span><span class="p">,</span>
        <span class="n">compute_units</span><span class="o">=</span><span class="n">COMPUTE_UNIT</span>
    <span class="p">)</span>
    <span class="n">model_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">./</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">.mlpackage</span><span class="sh">'</span>
    <span class="n">mlmodel</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model_path</span>

<span class="k">def</span> <span class="nf">predict_with_coreml</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">MLModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">compute_units</span><span class="o">=</span><span class="n">COMPUTE_UNIT</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">input_tensor</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)}</span>
    <span class="n">output_dict</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
    <span class="n">output_key</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">output_dict</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">output_key</span><span class="p">]</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">model_1d</span> <span class="o">=</span> <span class="nc">Deconv1DModel</span><span class="p">()</span>
    <span class="n">model_2d</span> <span class="o">=</span> <span class="nc">Deconv2DModel</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="mi">600</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">model_2d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model_1d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">model_2d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model_1d</span><span class="p">.</span><span class="n">deconv</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span>
    <span class="n">model_1d</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">model_2d</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">torch_output_1d</span> <span class="o">=</span> <span class="nf">model_1d</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
        <span class="n">torch_output_2d</span> <span class="o">=</span> <span class="nf">model_2d</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
    <span class="n">are_torch_outputs_close</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">torch_output_1d</span><span class="p">,</span> <span class="n">torch_output_2d</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PyTorch中1D和2D模型的输出是否一致? -&gt; </span><span class="si">{</span><span class="n">are_torch_outputs_close</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">are_torch_outputs_close</span><span class="p">,</span> <span class="sh">"</span><span class="s">错误：PyTorch模型不等价，测试无法继续！</span><span class="sh">"</span>

    <span class="c1"># --- CoreML转换与推理 ---
</span>    <span class="n">model_1d_path</span> <span class="o">=</span> <span class="nf">convert_and_save</span><span class="p">(</span><span class="n">model_1d</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="sh">"</span><span class="s">deconv1d_model_specific_data</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model_2d_path</span> <span class="o">=</span> <span class="nf">convert_and_save</span><span class="p">(</span><span class="n">model_2d</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="sh">"</span><span class="s">deconv2d_model_specific_data</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">coreml_output_1d</span> <span class="o">=</span> <span class="nf">predict_with_coreml</span><span class="p">(</span><span class="n">model_1d_path</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span>
    <span class="n">coreml_output_2d</span> <span class="o">=</span> <span class="nf">predict_with_coreml</span><span class="p">(</span><span class="n">model_2d_path</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">conv1d output: </span><span class="si">{</span><span class="n">coreml_output_1d</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">conv2d output: </span><span class="si">{</span><span class="n">coreml_output_2d</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">are_coreml_outputs_close</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">coreml_output_1d</span><span class="p">,</span> <span class="n">coreml_output_2d</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">max_abs_diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">coreml_output_1d</span> <span class="o">-</span> <span class="n">coreml_output_2d</span><span class="p">).</span><span class="nf">max</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">CoreML中1D和2D模型的输出是否一致? -&gt; </span><span class="si">{</span><span class="n">are_coreml_outputs_close</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">两个输出之间的最大绝对差值为: </span><span class="si">{</span><span class="n">max_abs_diff</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>通过切换<code class="language-plaintext highlighter-rouge">coremltools</code>的<code class="language-plaintext highlighter-rouge">compute_units</code>参数，我得到了决定性的证据：</p> <ul> <li><code class="language-plaintext highlighter-rouge">compute_units = ct.ComputeUnit.CPU_ONLY</code>：<strong>结果正确</strong>。</li> <li><code class="language-plaintext highlighter-rouge">compute_units = ct.ComputeUnit.CPU_AND_GPU</code>：<strong>结果正确</strong>。</li> <li><code class="language-plaintext highlighter-rouge">compute_units = ct.ComputeUnit.ALL</code>：<strong>结果错误</strong>！</li> </ul> <p><code class="language-plaintext highlighter-rouge">ALL</code>模式和<code class="language-plaintext highlighter-rouge">CPU_AND_GPU</code>模式唯一的区别就是前者会启用ANE（苹果的神经网络引擎）。这就说明，<strong>Bug的根源在于CoreML在ANE上的具体实现</strong>。只要计算任务被分配到ANE上，这个特定尺寸的<code class="language-plaintext highlighter-rouge">ConvTranspose1d</code>的Bias加法就会出错。</p> <h4 id="解决方案">解决方案</h4> <p>既然定位了Bug，解决方案就有了。</p> <ol> <li><strong>方案A：把Bias加法拆出来</strong>。先算一个不带Bias的<code class="language-plaintext highlighter-rouge">conv_transpose</code>，再手动加一个<code class="language-plaintext highlighter-rouge">add</code>算子。这个方法能解决问题，但多了一步操作，内存要多倒腾一次，可能会影响性能。</li> <li><strong>方案B：把1D伪装成2D</strong>。在1D算子前后，分别插入<code class="language-plaintext highlighter-rouge">expand_dims</code>和<code class="language-plaintext highlighter-rouge">squeeze</code>算子，把数据变成4D，然后调用我们已经验证过没问题的<code class="language-plaintext highlighter-rouge">ConvTranspose2d</code>来计算。</li> </ol> <p>考虑到性能，我最终选择了<strong>方案B</strong>。一个融合算子，内存读写一次，效率高。虽然在MNN的图转换逻辑里要多写几行代码，但这能让CoreML在底层执行一个高效的融合算子。这个维度变换的逻辑被封装在算子转换的内部，对整个计算图的其他部分是透明的，不会影响其他算子的实现。</p> <h4 id="总结">总结</h4> <p>这次踩坑经历耗费了不少时间，总结下来有几点：</p> <ol> <li><strong>CoreML的<code class="language-plaintext highlighter-rouge">Conv1d</code>算子在ANE上可能存在隐蔽的Bug</strong>，当你的模型里有这个算子并且结果不对时，可以优先排查它，并尽量使用<code class="language-plaintext highlighter-rouge">Conv2d</code>。</li> <li><strong>验证问题时，一定要切换<code class="language-plaintext highlighter-rouge">compute_units</code></strong>，对比CPU、GPU、ANE的行为差异，这能帮你快速定位问题是不是硬件相关的。</li> </ol> <p>希望这个排查过程能给遇到类似问题的朋友一点启发。</p>]]></content><author><name></name></author><category term="llm"/><category term="coreml"/><category term="ios"/><summary type="html"><![CDATA[背景]]></summary></entry><entry><title type="html">深入 gpt-oss-20b 架构：MNN 移动端性能实践</title><link href="https://zhaode.wang/blog/2025/gpt-oss/" rel="alternate" type="text/html" title="深入 gpt-oss-20b 架构：MNN 移动端性能实践"/><published>2025-08-08T00:00:00+00:00</published><updated>2025-08-08T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/gpt-oss</id><content type="html" xml:base="https://zhaode.wang/blog/2025/gpt-oss/"><![CDATA[<p>在 OpenAI 开源 gpt-oss-20b 模型之后，MNN 迅速完成了对这个 20B 参数大语言模型的高效适配，成功将其带到移动端。实测结果表明，该模型不仅知识渊博、推理能力不凡，并且在手机上也能实现令人印象深刻的运行速度。</p> <h2 id="智力测试gpt-oss-20b-vs-qwen3-30b-a3b-thinking-2507">智力测试：gpt-oss-20b vs. Qwen3-30B-A3B-Thinking-2507</h2> <p>为了深入探究 gpt-oss-20b 的能力边界，我们将其与同样强大的 Qwen3-30B-A3B-Thinking-2507 模型进行了一场横向对比评测。</p> <h3 id="测试环境说明">测试环境说明</h3> <p>两款模型均使用 <code class="language-plaintext highlighter-rouge">MNN</code> 的 <strong>4-bit HQQ</strong> 方法进行量化，并在 <code class="language-plaintext highlighter-rouge">Mac M3 Pro</code> 上通过 <code class="language-plaintext highlighter-rouge">MNN</code> 框架进行测试。我们采用了各模型官方推荐的采样配置，并使用 <code class="language-plaintext highlighter-rouge">Gemini 1.5 Pro</code> 生成了一系列涵盖数学、物理、逻辑和常识推理的测试问题。</p> <p>测试题目如下：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.11 和 9.9 哪个大?
有一架飞机，停在一个和跑道一样长、一样宽的巨型跑步机上。跑步机的系统被设定为：无论飞机机轮的速度是多少，它都会以完全相同的速度向相反方向&gt;运动，从而抵消机轮的转动。请问，这架飞机能否成功起飞？
你有一个完全密闭、内部是完美真空、并且100%不透光的坚固箱子。你在箱子里的天平上称量这个箱子。然后，你打开了箱子里的一支手电筒。请问，当手&gt;电筒亮着时，天平显示的重量会比之前更重、更轻，还是完全一样？
你有两根不均匀的绳子，每根绳子从一头烧到另一头都正好需要1个小时。所谓不均匀，是指绳子可能前半段10分钟就烧完了，后半段却烧了50分钟。现在，
只用这两根绳子和一个打火机，你如何精确地计时45分钟？
I want to cool down a sealed room, so I leave my refrigerator door open inside it. Will the room's overall temperature eventually rise, lower, or stay the same?
If all the people on Earth gathered in one place and all jumped at the exact same time, what would happen to the Earth? Would its orbit be altered?
</code></pre></div></div> <h3 id="-测试结果一览">📊 测试结果一览</h3> <table> <thead> <tr> <th style="text-align: left">测试项目</th> <th style="text-align: left">gpt-oss-20b</th> <th style="text-align: left">Qwen3-30B-A3B-Thinking-2507</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">数值比较 (9.11 vs 9.9)</td> <td style="text-align: left">✅ <strong>正确</strong> (认为9.9更大)</td> <td style="text-align: left">✅ <strong>正确</strong> (认为9.9更大)</td> </tr> <tr> <td style="text-align: left">物理推理 (跑步机与飞机)</td> <td style="text-align: left">❌ <strong>错误</strong> (认为不会起飞)</td> <td style="text-align: left">✅ <strong>正确</strong> (能够起飞)</td> </tr> <tr> <td style="text-align: left">高级物理 (密闭箱中的光)</td> <td style="text-align: left">✅ <strong>正确</strong> (E=mc²)</td> <td style="text-align: left">❌ <strong>错误</strong> (忽略质能等价)</td> </tr> <tr> <td style="text-align: left">逻辑推理 (烧绳计时45分钟)</td> <td style="text-align: left">✅ <strong>正确</strong> (标准解法)</td> <td style="text-align: left">✅ <strong>正确</strong> (标准解法)</td> </tr> <tr> <td style="text-align: left">热力学 (敞开的冰箱)</td> <td style="text-align: left">✅ <strong>正确</strong> (热力学第二定律)</td> <td style="text-align: left">✅ <strong>正确</strong> (直觉误判)</td> </tr> <tr> <td style="text-align: left">宏观力学 (全球人同时跳)</td> <td style="text-align: left">✅ <strong>正确</strong> (量化分析)</td> <td style="text-align: left">✅ <strong>正确</strong> (定性分析)</td> </tr> </tbody> </table> <p><strong>详细测试结果</strong></p> <ul> <li><a href="https://github.com/wangzhaode/llm-lab/blob/main/gpt-oss/gpt-oss-20b-MNN-answer.txt">gpt-oss-20b-MNN-answer.txt</a></li> <li><a href="https://github.com/wangzhaode/llm-lab/blob/main/gpt-oss/Qwen3-30B-A3B-Thinking-2507-MNN-answer.txt">Qwen3-30B-A3B-Thinking-2507-MNN-answer.txt</a></li> </ul> <h3 id="-关键洞察">🎯 关键洞察</h3> <p>通过这场对决，两个模型展现出截然不同的“智能画像”：</p> <ul> <li> <p><strong>gpt-oss-20b：知识渊博的“检索型专家”</strong> 它在需要精确、深入专业知识的领域表现卓越。例如，在“密闭箱中的光”问题上，它能准确调用质能等价性（E=mc²）这一高级物理知识，给出理论严谨的回答。这表明其庞大的参数量有效地存储了丰富的世界知识。</p> </li> <li> <p><strong>Qwen3-30B-A3B-Thinking-2507：逻辑缜密的“推理型思考者”</strong> 它在需要多步推理、理解相对关系和进行科学计算的场景中更胜一筹。例如，在“跑步机上的飞机”，它能摆脱直觉误导，运用正确的物理原理和量化分析来解决问题。</p> </li> </ul> <p><strong>结论</strong>：gpt-oss-20b 强大的知识储备使其在“是什么”的问题上表现突出，而 Qwen3-30B-A3B-Thinking-2507 则在“为什么”和“如何做”的问题上展现了更强的逻辑推理能力。</p> <h2 id="架构深度解析">架构深度解析</h2> <p>gpt-oss-20b 的独特性能源于其精妙的架构设计。</p> <p>首先，它采用了 <strong>MoE (混合专家)</strong> 架构。模型每层包含32个专家网络，但每个 Token 的前向传播仅激活其中4个（约13%的激活率）。这意味着，尽管其总参数量高达21B，但单次推理的计算量仅相当于一个 <strong>3.6B</strong> 的稠密模型，极大地降低了硬件门槛。同时，专家网络部分采用了 <strong>MXFP4</strong> 量化，将模型尺寸压缩至原来的四分之一。</p> <p>为了应对长文本处理带来的计算挑战，gpt-oss 采用了<strong>交替注意力机制</strong>：一半的注意力层使用全上下文（Full Attention），另一半则使用仅关注前128个 Token 的滑动窗口注意力（Sliding Window Attention）。这种设计将一半注意力计算的复杂度从 O(n²) 优化到了 O(n×w)，是其能够高效处理长序列的关键。</p> <p>除了这些宏观设计，gpt-oss-20b 的 Attention 结构中还蕴含着一些非常有趣的设计。</p> <h3 id="1-attention-sinks滑动窗口的稳定锚">1. Attention Sinks：滑动窗口的“稳定锚”</h3> <p>这是一个在其他模型中非常罕见的设计。我们知道，标准的注意力机制容易在序列开头形成“注意力汇点 (Attention Sinks)”，这些初始 Token 会像锚一样稳定全局注意力。然而，当使用滑动窗口时，这些头部的 Token 会被逐渐移出窗口，导致注意力分布变得不稳定。</p> <p>gpt-oss 通过在序列末尾显式地添加一个可学习的 <code class="language-plaintext highlighter-rouge">sinks</code> 向量来解决这个问题。这个 <code class="language-plaintext highlighter-rouge">sinks</code> 作为一个固定的“锚点”，确保即使在窗口滑动时，注意力也有一个稳定的归宿，从而维持了模型的推理连贯性和效率。</p> <h3 id="2-注意力心跳一项揭示架构特点的发现">2. “注意力心跳”：一项揭示架构特点的发现</h3> <p>gpt-oss-20b 依然保留了 Attention Bias，而许多新模型（如Qwen2系列）已转向 <code class="language-plaintext highlighter-rouge">LayerNorm</code>。通过分析我们发现，gpt-oss 的 Bias 设计得非常克制（仅 Q-Proj 有微小 Bias，K-Proj 全为0），避免了 Qwen2 系列中因 Bias 过大导致的 <code class="language-plaintext highlighter-rouge">Q@K^T</code> 溢出风险。</p> <p>然而，当我们进一步探究其 <code class="language-plaintext highlighter-rouge">Q@K^T</code> 的原始 Logits 分布时，一个极其规律且引人注目的现象浮出水面。</p> <p><img src="/assets/img/gpt-oss/gpt_oss_20b.png" alt=""/></p> <p>如图所示，模型的注意力表现出一种完美交替的“高峰-低谷”模式：</p> <ul> <li><strong>奇数层 (sliding_attention)</strong>，构成了图中所有红色的“<strong>山峰</strong>”。这些层的 <code class="language-plaintext highlighter-rouge">Q@K^T</code> Logits 值异常之高，动辄达到数百甚至上千。</li> <li><strong>偶数层 (full_attention)</strong>，则构成了图中所有蓝色的“<strong>谷底</strong>”。这些层的 <code class="language-plaintext highlighter-rouge">Q@K^T</code> Logits 值则要低得多，通常只有前者的十分之一甚至更少。</li> </ul> <p>这个巨大的、数量级上的差异，并非随机伪影或由偏置项驱动，而是 gpt-oss-20b 在架构层面的一种<strong>深刻的功能特化 (Functional Specialization)</strong>。</p> <p><strong>原因解析：两种注意力层的“能量”分工</strong></p> <p>这个“注意力心跳”的根本原因在于，模型在训练中为两种注意力层学会了生成<strong>能量（即向量范数）完全不同的 Q 和 K 向量</strong>。</p> <ol> <li><strong>Sliding Attention 层：“高增益”的局部信号放大器</strong> <ul> <li><strong>定位</strong>：这些层是“<strong>局部专家</strong>”，视野狭窄。</li> <li><strong>任务</strong>：由于看不见全局，它们必须在有限信息中，对最关键的局部模式做出<strong>毫不含糊、决定性</strong>的判断。</li> <li><strong>实现</strong>：模型通过生成<strong>高范数（高能量）的 Q/K 向量</strong>来实现这一点。当两个高能量向量对齐时，它们的点积会变得非常巨大，就像一个高增益放大器，在进入Softmax前就几乎“内定”了胜者，确保最强的局部信号能被稳定传递。</li> </ul> </li> <li><strong>Full Attention 层：“低增益”的全局信息协调者</strong> <ul> <li><strong>定位</strong>：这些层是“<strong>全局战略家</strong>”，视野广阔。</li> <li><strong>任务</strong>：它们的职责是审慎地评估和协调长距离依赖关系，这需要的是<strong>灵活性</strong>，而非武断。</li> <li><strong>实现</strong>：模型通过生成<strong>范数较小（低能量）的 Q/K 向量</strong>，使得在Softmax前，各个Token的竞争更温和公平。这让模型可以同时融合多个来自不同位置的弱信号，从而发现复杂的全局模式。</li> </ul> </li> </ol> <h3 id="3-短胖-vs-瘦长的架构哲学">3. “短胖” vs “瘦长”的架构哲学</h3> <p>最后，模型间的宏观架构差异也值得玩味：</p> <ul> <li><strong>gpt-oss-20b</strong>: 24层架构，相对“<strong>短胖</strong>”。</li> <li><strong>Qwen系列</strong>: 通常为48层甚至更多，相对“<strong>瘦长</strong>”。</li> </ul> <p>这种差异或许能解释它们的能力倾向：gpt-oss-20b“短胖”的结构让每层拥有更高的参数密度，可能更利于知识的存储和检索；而Qwen“瘦长”的结构通过更多层级的深度非线性变换，可能在逻辑推理和多步思考上更具优势。这也与我们在“智力测试”中观察到的“知识检索型”vs“情境推理型”的差异不谋而合。</p> <h2 id="mnn-适配与性能表现">MNN 适配与性能表现</h2> <p>为了将 gpt-oss-20b 的强大能力带到端侧，MNN 框架针对其独特架构进行了深度适配和优化。</p> <ul> <li><strong>MoE 适配</strong>：为简化导出流程，MNN 将 gpt-oss 的原生 MoE 实现转换为与 Qwen2-MoE 兼容的格式。</li> <li><strong>注意力优化</strong>：初期通过 <code class="language-plaintext highlighter-rouge">attention_mask</code> 实现滑动窗口，未来将持续优化以实现更高的计算效率。</li> </ul> <h3 id="实测性能">实测性能</h3> <table> <thead> <tr> <th style="text-align: left">平台</th> <th style="text-align: left">Prefill 速度</th> <th style="text-align: left">Decode 速度</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>OnePlus (骁龙8 Gen3)</strong></td> <td style="text-align: left">13.68 tokens/s</td> <td style="text-align: left">11.35 tokens/s</td> </tr> <tr> <td style="text-align: left"><strong>Mac M3 Pro</strong></td> <td style="text-align: left">62.72 tokens/s</td> <td style="text-align: left">23.98 tokens/s</td> </tr> </tbody> </table> <p><img src="/assets/img/gpt-oss/gptoss.gif" alt=""/></p> <p><em>移动端运行截图显示了gpt-oss-20b在手机上的流畅表现</em></p> <h3 id="模型下载与快速部署">模型下载与快速部署</h3> <p>下载模型文件后，即可通过 MNN 的标准推理接口轻松调用，移动端可以使用 MNN Chat APP快速体验。</p> <ul> <li><strong>ModelScope</strong>: <a href="https://modelscope.cn/models/MNN/gpt-oss-20b-MNN">gpt-oss-20b-MNN</a></li> <li><strong>HuggingFace</strong>: <a href="https://huggingface.co/MNN/gpt-oss-20b-MNN">gpt-oss-20b-MNN</a></li> <li><strong>MNN Chat APP</strong>: <a href="https://meta.alicdn.com/data/mnn/mnn_chat_0_7_0.apk">apk</a></li> </ul> <h2 id="结语当未来照进现实">结语：当未来照进现实</h2> <p>完成对 gpt-oss-20b 的所有测试和分析后，我心中涌起一阵强烈的感慨。AI 的发展速度如此迅猛，以至于那个两年前初次让我感到震撼、认为遥不可及的 GPT，如今其同等级别的能力，已经安然运行在我的掌心之中。</p> <p>从云端的庞然大物，到手机里的高效智能，这不仅是参数和代码的迁移，更是技术民主化的又一次伟大实践。这让我无比振奋，也让我对未来充满期待——端侧的 AI 能力必将日益强大，一个更加智能、更加个性化的世界正在我们眼前展开。</p>]]></content><author><name></name></author><category term="llm"/><category term="mnn"/><summary type="html"><![CDATA[在 OpenAI 开源 gpt-oss-20b 模型之后，MNN 迅速完成了对这个 20B 参数大语言模型的高效适配，成功将其带到移动端。实测结果表明，该模型不仅知识渊博、推理能力不凡，并且在手机上也能实现令人印象深刻的运行速度。]]></summary></entry><entry><title type="html">混元端侧模型分析</title><link href="https://zhaode.wang/blog/2025/Hunyuan/" rel="alternate" type="text/html" title="混元端侧模型分析"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://zhaode.wang/blog/2025/Hunyuan</id><content type="html" xml:base="https://zhaode.wang/blog/2025/Hunyuan/"><![CDATA[<p>模型之战的下半场，烽火已经烧到了端侧。继阿里Qwen之后，腾讯混元也学着一次性发了覆盖多种大小的端侧模型。这种直接对标的打法，让这场“国产内战”变得非常有看点。</p> <p>我一直比较关注端侧推理，这次正好把两个大厂的同级模型拉出来，硬碰硬地比一下。不扯别的，就看重合评测集里的硬核数据，看看指令微调之后，到底谁才是端侧场景的更优选。</p> <h3 id="性能对比"><strong>性能对比</strong></h3> <p>评测集就选这六个，都是硬骨头，覆盖了数学、科学、代码、指令遵循和工具调用这些核心能力：</p> <ul> <li><strong>AIME’24 &amp; AIME’25:</strong> 数学竞赛，纯看逻辑推理。</li> <li><strong>GPQA-Diamond:</strong> 专业科学难题，看知识和推理。</li> <li><strong>LiveCodeBench:</strong> 真实环境代码能力。</li> <li><strong>IF-Eval:</strong> 能不能听懂人话，按复杂指令办事。</li> <li><strong>BFCL v3:</strong> Agent能力，就是用工具干活的能力。</li> </ul> <p><img src="/assets/img/hunyuan/hunyuan_qwen3.png" alt=""/></p> <h4 id="入门级-05b-vs-06b"><strong>入门级 (0.5B vs 0.6B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-0.5B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-0.6B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>17.2</strong></td> <td style="text-align: left">10.7</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>20.0</strong></td> <td style="text-align: left">15.1</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left">23.3</td> <td style="text-align: left"><strong>27.9</strong></td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">11.1</td> <td style="text-align: left"><strong>12.3</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">49.7</td> <td style="text-align: left"><strong>59.2</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>49.8</strong></td> <td style="text-align: left">46.4</td> </tr> </tbody> </table> <h4 id="主流级-18b-vs-17b"><strong>主流级 (1.8B vs 1.7B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-1.8B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-1.7B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>56.7</strong></td> <td style="text-align: left">48.3</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>53.9</strong></td> <td style="text-align: left">36.8</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left"><strong>47.2</strong></td> <td style="text-align: left">40.1</td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">31.5</td> <td style="text-align: left"><strong>33.2</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">67.6</td> <td style="text-align: left"><strong>72.5</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>58.3</strong></td> <td style="text-align: left">56.6</td> </tr> </tbody> </table> <h4 id="甜点级-4b-vs-4b"><strong>甜点级 (4B vs 4B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-4B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-4B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>78.3</strong></td> <td style="text-align: left">73.8</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>66.5</strong></td> <td style="text-align: left">65.6</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left"><strong>61.1</strong></td> <td style="text-align: left">55.9</td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">49.4</td> <td style="text-align: left"><strong>54.5</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">76.6</td> <td style="text-align: left"><strong>81.9</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>67.9</strong></td> <td style="text-align: left">65.9</td> </tr> </tbody> </table> <h4 id="旗舰级-7b-vs-8b"><strong>旗舰级 (7B vs 8B)</strong></h4> <table> <thead> <tr> <th style="text-align: left"><strong>评测基准</strong></th> <th style="text-align: left"><strong>Hunyuan-7B-Instruct</strong></th> <th style="text-align: left"><strong>Qwen3-8B</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">AIME’24</td> <td style="text-align: left"><strong>81.1</strong></td> <td style="text-align: left">76.0</td> </tr> <tr> <td style="text-align: left">AIME’25</td> <td style="text-align: left"><strong>75.3</strong></td> <td style="text-align: left">67.3</td> </tr> <tr> <td style="text-align: left">GPQA-Diamond</td> <td style="text-align: left">60.1</td> <td style="text-align: left"><strong>62.0</strong></td> </tr> <tr> <td style="text-align: left">LiveCodeBench</td> <td style="text-align: left">57.0</td> <td style="text-align: left"><strong>57.5</strong></td> </tr> <tr> <td style="text-align: left">IF-Eval</td> <td style="text-align: left">79.3</td> <td style="text-align: left"><strong>85.0</strong></td> </tr> <tr> <td style="text-align: left">BFCL v3</td> <td style="text-align: left"><strong>70.8</strong></td> <td style="text-align: left">68.1</td> </tr> </tbody> </table> <h4 id="数据分析"><strong>数据分析</strong></h4> <ul> <li> <p><strong>跑分下来，两边的画像非常清晰。</strong> Hunyuan的进化路径非常明确，就是猛点 <strong>“推理”</strong> 和 <strong>“增强的代理能力”</strong> 这两个技能点。从0.5B到7B，它在AIME（数学）和BFCL（工具调用）上几乎一路领先。Qwen3则更像个“六边形战士”，在IF-Eval（指令遵循）和LiveCodeBench（代码）上表现稳定且强大，通用基础能力很扎实。</p> </li> <li> <p><strong>7B vs 8B的对决最有意思。</strong> 参数量更小的Hunyuan-7B，在最难的数学（AIME）上反超了更大的Qwen3-8B，这说明Hunyuan的参数效率在推理任务上非常高。但Qwen3-8B凭借更大的体量，在知识、代码和指令遵循上还是更强，综合实力依然是标杆。</p> </li> </ul> <h4 id="benchmark之外的亮点"><strong>Benchmark之外的亮点</strong></h4> <p>除了跑分，Hunyuan这次还带了两个Qwen没有的“杀手锏”，而这可能才是决定某些应用场景选型的关键。</p> <ol> <li> <p><strong>超长上下文：原生256K vs 32K</strong> 这可能是最“不讲道理”的优势。Hunyuan原生支持<strong>256K</strong>的上下文窗口，<strong>这直接是Qwen3原生32K的8倍</strong>。这意味着什么？处理超长文档、几十页的财报、把一整本书扔进去当知识库…这些场景Qwen3可能就很难搞，但Hunyuan能直接吃下，而且官方宣称在长文本任务上性能稳定。对于想做长文本分析、RAG增强等应用的开发者来说，这吸引力太大了。</p> </li> <li> <p><strong>为Agent而生的设计</strong> 上面的BFCL跑分已经证明了Hunyuan在工具调用上的领先。这不只是“碰巧”考得好，而是腾讯在设计模型时，就奔着让它当一个能干活的<strong>Agent</strong>去的，而不是一个纯粹的聊天机器人。这一点在τ-Bench和C3-Bench等其他Agent评测中也得到验证，说明它的这个优势是系统性的。</p> </li> </ol> <h4 id="总结一下"><strong>总结一下</strong></h4> <p>所以到底怎么选？结论已经非常清晰了：</p> <ul> <li> <p><strong>选Hunyuan，如果你需要一个“长文本分析专家” + “超级助理(Agent)”</strong> 如果你的应用场景需要处理上万甚至十万字以上的文档，或者需要模型作为核心，去调度各种外部API和工具来完成复杂任务，那Hunyuan是好的选择。它在这两个方向上优势太明显了。</p> </li> <li> <p><strong>选Qwen3，如果你想要一个“可靠的全能执行者” + “编程好帮手”</strong> 如果你的需求是构建一个能准确理解用户各种指令的通用聊天助手，或是在代码生成、调试等开发场景中寻求稳定可靠的帮助，那基础更全面、指令遵循能力更强的Qwen3依然是那个不会错的选择。</p> </li> </ul> <h3 id="个人思考混合推理是条好走的路吗"><strong>个人思考：混合推理，是条好走的路吗？</strong></h3> <p>这里有个很有意思的细节。</p> <p>Hunyuan这次也提了“快慢思考”混合推理，这和之前Qwen3的做法很像，想让一个模型满足不同场景下对速度和精度的要求。</p> <p>但耐人寻味的是，Qwen3最新的版本反而把这条路给放弃了，直接把<strong>Thinking</strong>和<strong>Instruct</strong>拆成了两个独立模型。</p> <p>所以，“一体两用”的混合推理，到底是不是一条好走的路？</p> <p>Qwen3这波操作，看起来就像是踩过坑之后做的决定。这其实也印证了我自己做微调和部署时的感觉：这种混合模式听起来很美，但在实际部署、量化和持续优化上，远不如两个权责清晰的独立模型来得直接和高效。一个模型要同时兼顾两个差异巨大的优化目标，往往意味着两边都做不到极致。</p> <p>Hunyuan现在还在坚持这条路，后续的效果和社区反馈会怎么样，值得我们持续观察。</p> <h3 id="压轴好戏mnn支持">压轴好戏：MNN支持</h3> <p>MNN第一时间给出了Hunyuan系列模型的支持，安装MNN Chat可以直接在手机上体验Hunyuan的模型。</p> <p><img src="/assets/img/hunyuan/mnnchat.png" alt=""/></p> <h4 id="app--模型下载链接">APP &amp; 模型下载链接：</h4> <ul> <li><a href="https://play.google.com/store/apps/details?id=com.alibaba.mnnllm.android.release">MNN Chat</a></li> <li><a href="https://modelscope.cn/collections/Hunyuan-MNN-f088ac18d5db4a">ModelScope</a></li> <li><a href="https://huggingface.co/collections/taobao-mnn/hunyuan-mnn-68907bf27f4073975a8a8df6">HuggingFace</a></li> </ul>]]></content><author><name></name></author><category term="llm"/><category term="mnn"/><summary type="html"><![CDATA[模型之战的下半场，烽火已经烧到了端侧。继阿里Qwen之后，腾讯混元也学着一次性发了覆盖多种大小的端侧模型。这种直接对标的打法，让这场“国产内战”变得非常有看点。]]></summary></entry></feed>