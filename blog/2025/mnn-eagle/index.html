<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MNN支持Eagle3 | Zhaode's blog </title> <meta name="author" content="Zhaode Wang"> <meta name="description" content="Expert insights on On-Device AI, LLM Optimization, and High-Performance Computing. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, blog, MNN, Edge AI, Model Compression, LLM, On-Device AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons|Inter:300,400,500,700|Merriweather:300,400,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhaode.wang/blog/2025/mnn-eagle/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhaode's blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MNN支持Eagle3</h1> <p class="post-meta"> Created on November 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>大语言模型（LLM）端侧推理的性能指标主要是看Prefill和Decode的性能，其中Prefill能够反应首字延迟，Decode能够反应生成速度。在端侧环境下，使用GPU或者NPU能够提升Prefill的速度，降低首字延迟。但是对于Decode速度，这些加速硬件都没有效果。因为Decode过程受限于内存带宽，而端侧设备由于功耗和芯片面积的限制，内存带宽比较受限。因此，提升Decode速度最有效的方法就是投机解码，而 Eagle3 是目前最前沿的方案之一。本文将分享 MNN 适配与优化 Eagle3 的完整实践记录。</p> <h2 id="推理">推理</h2> <p><a href="https://github.com/SafeAILab/EAGLE" rel="external nofollow noopener" target="_blank">Eagle3</a> 会训练出一个约等于模型一层 + 32K大小的<code class="language-plaintext highlighter-rouge">lm_head</code>作为草稿模型，在生成过程中会让草稿模型生成出多个草稿Token。然后让主模型“一次性”验证这些草稿，而不是像传统自回归那样逐个token生成。减少了对主模型的调用次数，从而实现了推理加速。</p> <p>整个推理流程可以分解为以下几个关键步骤：</p> <h4 id="第一步初始化与上下文处理"><strong>第一步：初始化与上下文处理</strong></h4> <ol> <li> <strong>输入处理</strong>：当用户输入一个提示（Prompt）时，主模型会像标准推理一样，对整个提示进行编码和处理，生成初始的键/值缓存（KV Cache）。这是后续所有生成工作的基础。</li> <li> <strong>生成第一个Token</strong>：目标模型（Qwen2）基于初始的KV Cache，生成第一个Token。这个步骤是无法“推测”的，必须由主模型完成。</li> </ol> <h4 id="第二步eagle3-生成草稿token"><strong>第二步：Eagle3 生成草稿Token</strong></h4> <ol> <li> <strong>获取上下文</strong>：Eagle3模型接收目标模型生成的最新一个Token作为输入，并获得主模型的3层中间变量（hidden_states）。</li> <li> <strong>并行生成草稿</strong>：Eagle3模型利用其轻量的结构，以自回归的方式快速、并行地生成K个候选Token（草稿）。 <ul> <li> <strong>草稿树结构</strong>：在这个过程中，Eagle3内部会维护一个“草稿树”。每个节点代表一个生成的草稿Token。这种树状结构是为了在更复杂的推测解码中处理多个候选序列。</li> </ul> </li> </ol> <h4 id="第三步目标模型批量验证"><strong>第三步：目标模型批量验证</strong></h4> <ol> <li> <strong>准备输入</strong>：将Eagle3生成的K个草稿Token与前文的真实Token拼接在一起，形成一个完整的序列。</li> <li> <strong>一次性验证</strong>：主模型接收这个拼接后的序列，并进行一次前向传播（Forward）。由于输入的序列长度为 <code class="language-plaintext highlighter-rouge">(1 + K)</code>，Qwen2会一次性地计算出这 <code class="language-plaintext highlighter-rouge">(1 + K)</code> 个位置的概率分布。</li> <li> <strong>路径选择</strong>：接下来，系统会逐一比对Eagle3生成的草稿Token和主模型验证后的结果，如果验证一致则接受否则拒绝；从验证接受的草稿树中接受路径最长的分支作为最终的输出。</li> </ol> <h4 id="第四步接受或拒绝并开启新一轮循环"><strong>第四步：接受或拒绝并开启新一轮循环</strong></h4> <ol> <li> <strong>更新输出和KV Cache</strong>：将未被接受的token的KV Cache删除掉，并将接受token的KV Cache按照顺序排列。</li> <li> <strong>循环</strong>：回到<strong>第二步</strong>，让草稿模型基于当前最新的Token，再次生成新的草稿，重复整个“推测-验证”的循环，直到生成满足长度要求或遇到终止符。</li> </ol> <p>这个流程有效地将多次主模型的推理，替换为“多次小模型推理 + 一次主模型验证”，总的访存大小小于单纯的主模型推理。</p> <p>比如假设模型的参数是2B, 草稿模型大小为0.1B，我们单次生成3个草稿且平均能够接受1个，则访存量如下：</p> <ol> <li>标准解码：<strong>2B * 2 = 4B</strong> </li> <li>投机解码：，则总访存大小为<strong>2B + 0.1B * 3 = 2.3B</strong> </li> </ol> <p>投机解码的访存量为标准解码的<strong>57.5%</strong>，有效降低了带宽需求，提升模型Decode速度。</p> <p>Eagle3的优势在于其在比较小的参数情况下能够做到比较高的接受率，因此我们选择使用Eagle3。</p> <h2 id="mnn中的eagle3推理">MNN中的Eagle3推理</h2> <p>为了在MNN C++推理引擎中高效地实现Eagle3的推测性解码（Speculative Decoding）算法，我们设计并实现了一套完整的端到端流程。我们将复杂的草稿Token生成与管理的逻辑，<strong>抽象成了一个独立的、可复用的 <code class="language-plaintext highlighter-rouge">TokenTree</code> 类</strong>，极大地简化了主流程的复杂度，并增强了代码的可维护性。</p> <h4 id="核心设计tokentree-类">核心设计：<code class="language-plaintext highlighter-rouge">TokenTree</code> 类</h4> <blockquote> <p><a href="https://github.com/alibaba/MNN/blob/master/transformers/llm/engine/src/speculative_decoding/tokentree.hpp" rel="external nofollow noopener" target="_blank">tokentree.hpp</a></p> </blockquote> <p>在推测性解码中，草稿模型需要生成一个包含多条候选序列的“树”。管理这棵树的生长、剪枝和信息提取是整个流程中最复杂的部分。为了解决这个问题，我们创建了 <code class="language-plaintext highlighter-rouge">TokenTree</code> 类，它封装了以下所有核心功能：</p> <ol> <li> <strong>节点管理</strong>：内部使用 <code class="language-plaintext highlighter-rouge">TokenTreeNode</code> 结构体来表示树中的每一个Token，清晰地存储了Token ID、对数概率、深度以及父子关系。</li> <li> <strong>树的初始化与生长</strong>： <ul> <li> <code class="language-plaintext highlighter-rouge">init()</code>: 接收第一轮Top-K预测结果，初始化树的第一层节点。</li> <li> <code class="language-plaintext highlighter-rouge">grow()</code>: 在每一轮迭代中，接收当前所有叶子节点的Top-K预测结果，生成新的候选节点，并根据累积概率进行排序和剪枝，仅保留最优的K个候选者作为新的叶子节点。</li> </ul> </li> <li> <strong>注意力掩码（Attention Mask）管理</strong>：在树生长的过程中，<code class="language-plaintext highlighter-rouge">TokenTree</code> 会同步地、高效地计算和更新下一轮迭代所需的注意力掩码，确保每个Token只能注意到其路径上的祖先节点。</li> <li> <strong>最终输出打包</strong>： <ul> <li> <code class="language-plaintext highlighter-rouge">finalize()</code>: 当树生长到预设深度后，该方法会整理所有候选的草稿Token，并生成主模型进行批量验证所需的所有输入，包括： <ul> <li> <code class="language-plaintext highlighter-rouge">draftTokens</code>: 扁平化后的草稿Token序列。</li> <li> <code class="language-plaintext highlighter-rouge">positionIds</code>: 每个草稿Token在序列中的正确位置ID。</li> <li> <code class="language-plaintext highlighter-rouge">attentionMask</code>: 供主模型使用的、符合树状结构的注意力掩码。</li> <li> <code class="language-plaintext highlighter-rouge">retrieveIndices</code>: 用于从主模型输出中快速回溯和验证最佳路径的索引列表。</li> </ul> </li> </ul> </li> </ol> <h4 id="eaglegeneration清晰的主流程"> <code class="language-plaintext highlighter-rouge">EagleGeneration</code>：清晰的主流程</h4> <blockquote> <p><a href="https://github.com/alibaba/MNN/blob/master/transformers/llm/engine/src/speculative_decoding/eagle.cpp" rel="external nofollow noopener" target="_blank">eagle.cpp</a></p> </blockquote> <p>通过将复杂的树状结构管理逻辑剥离到 <code class="language-plaintext highlighter-rouge">TokenTree</code> 中，主流程类 <code class="language-plaintext highlighter-rouge">EagleGeneration</code> 的职责变得非常清晰和线性：</p> <ol> <li> <strong>调用草稿模型</strong>：执行Eagle模型的前向计算，获取logits。</li> <li> <strong>驱动TokenTree</strong>：将logits结果喂给 <code class="language-plaintext highlighter-rouge">TokenTree</code> 实例，调用其 <code class="language-plaintext highlighter-rouge">grow()</code> 方法来发展草稿。</li> <li> <strong>获取验证数据</strong>：当草稿生成完毕，调用 <code class="language-plaintext highlighter-rouge">TokenTree</code> 的 <code class="language-plaintext highlighter-rouge">finalize()</code> 方法，一键获取打包好的、用于主模型验证的所有输入张量（Tensors）。</li> <li> <strong>执行主模型验证</strong>：调用主模型进行一次高效的批量前向推理。</li> <li> <strong>结果解析与循环</strong>：根据验证结果，确定接受的Token数量，并开启下一轮的草稿生成循环。</li> </ol> <h2 id="训练">训练</h2> <h3 id="eagle3-训练特点">Eagle3 训练特点</h3> <p>Eagle3的出色效果其实源自于其训练过程的设计。在Eagle3之前，像EAGLE这样的草稿模型存在一个普遍问题：</p> <ol> <li> <strong>间接的预测目标</strong>：模型被训练去预测主模型的<strong>中间层特征 (feature)</strong>，而非直接预测最终的词元 (token)。这是一个额外且不必要的约束，限制了模型的表达能力。</li> <li> <strong>训练与推理的不一致性</strong>：在训练时，模型每一步的输入都是来自目标模型的“完美”特征；但在推理时，模型却必须依赖自己上一步生成的、“不完美”的预测结果作为输入。这种不一致导致<strong>误差会像滚雪球一样累积</strong>，使得草稿序列在第二个、第三个词元之后的质量急剧下降，严重影响了最终的加速效果。</li> </ol> <p>Eagle3通过以下两项紧密结合的改进，彻底解决了上述问题：</p> <p><strong>1. 摆脱特征预测约束，融合多层级特征</strong></p> <p>首先，EAGLE-3完全摒弃了预测中间特征的旧范式，直接以<strong>预测最终的Token分布</strong>为目标。更重要的是，它不再仅仅依赖目标模型的顶层特征，而是创新性地融合了来自<strong>低、中、高不同层级的特征信息</strong>。通过一个简单的全连接层（FC）进行信息压缩，为草稿模型提供了远比以往更丰富、更全面的语义输入。</p> <p><strong>2. 引入“训练时测试” (TTT)，解决误差累积</strong></p> <p>这是Eagle3训练方法中最核心的创新。其思想是在训练阶段就<strong>完整地模拟推理时的多步生成过程</strong>：</p> <ul> <li> <strong>闭环反馈</strong>：当草稿模型在训练中生成第一个预测输出后，该输出会<strong>立即被用作下一轮预测的输入</strong>，而不是使用来自目标模型的“标准答案”。</li> <li> <strong>模拟真实环境</strong>：这个过程会模拟性地持续多步，并动态调整注意力掩码（Attention Mask），让模型在训练中就“演练”如何基于自己的预测进行连贯的思考和生成。</li> </ul> <p>这种“自产自销”的闭环训练方式，迫使模型学会了如何处理和修正自身预测中可能存在的微小偏差。它从根本上解决了训练与推理不一致的问题，使得训练出的草稿模型变得异常鲁棒。</p> <p><strong>总结而言</strong>，通过“训练时测试”机制，我们训练的中文Eagle3模型不仅学习目标模型的输出，更重要的是学会了如何在其自身的输出序列上进行稳定、高质量的连续生成。这使得模型在实际推理中能够产出更长、接受率更高的草稿序列，从而为实现业界领先的端侧推理加速比奠定了坚实的基础。</p> <p>好的，这是一个补充和润色后的段落，可以无缝地放入你的技术文章中。</p> <h3 id="训练框架选择specforge">训练框架选择：SpecForge</h3> <p>在启动Eagle3的训练任务时，选择一个高效、灵活且易于扩展的训练框架至关重要。</p> <p>虽然Eagle官方提供了原始的训练实现代码，为我们理解其核心算法提供了宝贵的参考，但在实际工程应用中，我们发现它存在一些局限性。例如，其原生支持的模型种类有限，当我们需要将其应用于像Qwen2这样的新模型时，往往需要投入大量的精力进行代码适配和修改，这在一定程度上拖慢了研发和迭代的速度。</p> <p>为了解决这些挑战，我们最终选择了由SGLang团队精心打造的 <strong><a href="https://github.com/sgl-project/SpecForge" rel="external nofollow noopener" target="_blank">SpecForge</a></strong> 框架。SpecForge是一个专为推测性解码（Speculative Decoding）草稿模型设计的训练库，它完美地契合了我们的项目需求：</p> <ol> <li> <p><strong>广泛的模型支持</strong>：SpecForge拥有开箱即用的特性，无缝支持包括Llama系列、Qwen系列、GPT-OSS在内的众多主流开源大模型架构。这意味着我们可以将精力完全集中在模型训练和数据优化上，而不是花费在繁琐的底层代码适配工作上。</p> </li> <li> <p><strong>卓越的易用性</strong>：该框架提供了高度封装和用户友好的API。它将复杂的“训练时测试 (TTT)”逻辑、动态注意力掩码管理等核心环节进行了优雅的抽象。开发者只需通过简单的配置，就能启动一个完整的Eagle3训练流程，极大地降低了上手门槛。另外，它还支持在AMD的显卡上进行训练，这让我在AMD环境下可以轻松上手。</p> </li> <li> <p><strong>社区活跃与维护</strong>：作为SGLang生态的一部分，SpecForge拥有活跃的社区支持和持续的更新维护，确保了框架的稳定性和前沿性。</p> </li> </ol> <h3 id="训练数据构造eaglechat">训练数据构造：EagleChat</h3> <p>在基于SpecForge框架对Qwen3系列进行Eagle3训练的初期，我们主要使用了社区通用的 <code class="language-plaintext highlighter-rouge">ShareGPT</code> 和 <code class="language-plaintext highlighter-rouge">UltraChat</code> 数据集。然而，在实际测试中，我们发现这种通用的数据配比在特定场景下存在明显的短板，主要体现在：</p> <ol> <li> <strong>中文场景“水土不服”</strong>：原有数据集以英文为主，导致草稿模型在中文语境下的接受率极低，几乎无法提供加速。</li> <li> <strong>接受率低</strong>：即使在英文环境下测试，接受率也远低于官方给出的Llama模型。</li> </ol> <p>为了解决上述问题，我们构建了一个高质量的中英双语对话数据集——<strong>EagleChat</strong>，并对其进行了详尽的消融实验。</p> <h4 id="数据集构成混合与平衡">数据集构成：混合与平衡</h4> <p>EagleChat 旨在成为一个能显著增强 LLM（特别是像 EAGLE 这样的草稿模型）综合对话能力的优质语料库。我们采用混合策略，将 <code class="language-plaintext highlighter-rouge">smoltalk-chinese</code> 引入到现有的 <code class="language-plaintext highlighter-rouge">ShareGPT</code> 和 <code class="language-plaintext highlighter-rouge">UltraChat</code> 中，不仅补足了中文能力，还强化了思维链（CoT）推理能力。</p> <p>最终构建的数据集包含超过 <strong>100万</strong> 条高质量对话数据，具体分布如下：</p> <table> <thead> <tr> <th style="text-align: left">数据来源 (Source Dataset)</th> <th style="text-align: left">对话数量 (Number of Conversations)</th> <th style="text-align: left">作用与特点</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>ShareGPT</strong></td> <td style="text-align: left">120,675</td> <td style="text-align: left">高质量的用户真实交互数据，覆盖多领域</td> </tr> <tr> <td style="text-align: left"><strong>UltraChat</strong></td> <td style="text-align: left">207,865</td> <td style="text-align: left">大规模合成对话数据，增强通用指令遵循能力</td> </tr> <tr> <td style="text-align: left"><strong>smoltalk-chinese</strong></td> <td style="text-align: left">710,564</td> <td style="text-align: left"> <strong>核心增量</strong>，专注于中文思维链与推理，大幅提升中文及逻辑能力</td> </tr> <tr> <td style="text-align: left"><strong>Total</strong></td> <td style="text-align: left"><strong>1,039,104</strong></td> <td style="text-align: left"><strong>EagleChat 总量</strong></td> </tr> </tbody> </table> <p>所有数据均经过统一格式化清洗和随机打乱，以保证训练的稳定性。</p> <h4 id="效果验证用数据说话">效果验证：用数据说话</h4> <p>为了验证 EagleChat 的有效性，我们使用 <code class="language-plaintext highlighter-rouge">Qwen3-4B-Instruct</code> 作为基座模型，分别使用 EagleChat、ShareGPT 和 UltraChat 训练了三个版本的 Eagle3 草稿模型，并在多个权威基准测试上对比了它们的<strong>平均接受长度（Acceptance Length, <code class="language-plaintext highlighter-rouge">acc_length</code>）</strong>。</p> <blockquote> <p><strong>注</strong>：<code class="language-plaintext highlighter-rouge">acc_length</code> 越高，意味着推理加速效果越好。</p> </blockquote> <p><strong>核心实验结果如下表所示：</strong></p> <table> <thead> <tr> <th style="text-align: left">Benchmark</th> <th style="text-align: left">任务类型</th> <th style="text-align: left">EagleChat (Ours)</th> <th style="text-align: left">ShareGPT</th> <th style="text-align: left">UltraChat</th> <th style="text-align: left"><strong>提升幅度 (%)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"> <strong>C-Eval</strong> (Draft=8)</td> <td style="text-align: left">中文综合</td> <td style="text-align: left"><strong>2.03</strong></td> <td style="text-align: left">1.17</td> <td style="text-align: left">1.08</td> <td style="text-align: left"> <strong>+73.38%</strong> 🚀</td> </tr> <tr> <td style="text-align: left"> <strong>CMMLU</strong> (Draft=8)</td> <td style="text-align: left">中文综合</td> <td style="text-align: left"><strong>2.02</strong></td> <td style="text-align: left">1.18</td> <td style="text-align: left">1.08</td> <td style="text-align: left"> <strong>+71.65%</strong> 🚀</td> </tr> <tr> <td style="text-align: left"> <strong>Math500</strong> (Draft=8)</td> <td style="text-align: left">数学推理</td> <td style="text-align: left"><strong>2.46</strong></td> <td style="text-align: left">1.95</td> <td style="text-align: left">1.70</td> <td style="text-align: left"><strong>+26.14%</strong></td> </tr> <tr> <td style="text-align: left"> <strong>GSM8K</strong> (Draft=8)</td> <td style="text-align: left">数学推理</td> <td style="text-align: left"><strong>2.38</strong></td> <td style="text-align: left">1.93</td> <td style="text-align: left">1.81</td> <td style="text-align: left"><strong>+23.35%</strong></td> </tr> <tr> <td style="text-align: left"> <strong>HumanEval</strong> (Draft=8)</td> <td style="text-align: left">代码生成</td> <td style="text-align: left"><strong>2.49</strong></td> <td style="text-align: left">2.10</td> <td style="text-align: left">1.94</td> <td style="text-align: left"><strong>+18.61%</strong></td> </tr> <tr> <td style="text-align: left"> <strong>MT-Bench</strong> (Draft=8)</td> <td style="text-align: left">通用能力</td> <td style="text-align: left"><strong>1.90</strong></td> <td style="text-align: left">1.65</td> <td style="text-align: left">1.61</td> <td style="text-align: left"><strong>+15.05%</strong></td> </tr> </tbody> </table> <p><em>(Draft=N 表示主模型每次验证 N 个 token)</em></p> <p><strong>结果分析：</strong></p> <ol> <li> <strong>中文能力爆发式增长</strong>：在 C-Eval 和 CMMLU 等中文基准测试中，EagleChat 训练的模型相比 Baseline 取得了 <strong>超过 60%-70%</strong> 的巨大提升。这直接证明了引入 <code class="language-plaintext highlighter-rouge">smoltalk-chinese</code> 对于解决“中文失语”问题的决定性作用。</li> <li> <strong>逻辑推理显著增强</strong>：在 GSM8K 和 Math500 等数学任务上，接受长度提升了约 <strong>25%</strong>。这意味着草稿模型不仅学会了说话，还学会了模仿主模型的推理步骤。</li> <li> <strong>全面优于基线</strong>：即使在通用的 MT-Bench 和代码任务上，EagleChat 也保持了 15% 左右的稳定优势。</li> </ol> <p><strong>可视化对比（Draft Tokens = 4 vs 8）：</strong></p> <p><img src="/assets/img/eagle/draft4.png" alt="alt text"> <img src="/assets/img/eagle/draft8.png" alt="alt text"></p> <p>实验数据可以看出：<strong>高质量、混合配比的 EagleChat 数据集，是训练高性能、支持中文 Eagle3 模型的关键所在。</strong></p> <h4 id="工程适配改进-specforge">工程适配：改进 SpecForge</h4> <p>在训练过程中发现了SpecForge中存在一些问题并对其进行修复和改善，并将修改贡献回了 SpecForge 社区。</p> <ol> <li>在AMD环境训练时 triton kernel的 num_warps 设置错误 <a href="https://github.com/sgl-project/SpecForge/pull/259" rel="external nofollow noopener" target="_blank">PR #259</a> </li> <li>增加EagleChat数据集，并修复中文数据乱码问题 <a href="https://github.com/sgl-project/SpecForge/pull/266" rel="external nofollow noopener" target="_blank">PR #266</a>；</li> <li>增加中文的benchmark，以测试在中文数据集上的接受率 <a href="https://github.com/sgl-project/SpecForge/pull/267" rel="external nofollow noopener" target="_blank">PR #267</a> </li> </ol> <h2 id="测试">测试</h2> <p>将训练好的模型部署到真实的端侧设备上，是我们整个工作的最后一步，也是检验成果的关键环节。在测试过程中我们发现，理论上的最优配置与实际应用场景之间存在差异，尤其是在算力受限的设备上，对草稿数量的调优至关重要。</p> <h3 id="验证开销的权衡">验证开销的权衡</h3> <p>在服务端或拥有强大GPU的设备上，一次性验证多个草稿词元（Token）的开销相对较小。然而，在端侧设备，尤其是CPU上运行时，我们发现<strong>验证（Verification）本身并非没有开销</strong>。</p> <p>当草稿树设置得过宽或过深（即一次性生成大量草稿词元）时，主模型进行批量验证的计算量会相应增大。如果这些草稿的接受率不够高，那么这种“大批量验证”的开销甚至可能会抵消掉推测性解码带来的收益，导致<strong>整体加速比不升反降</strong>。</p> <h4 id="最佳草稿数量配置">最佳草稿数量配置</h4> <p>经过多轮实验，我们发现在端侧CPU这类算力受限的场景下，并非草稿数量越多越好。更少的草稿数量意味着更低的验证开销和更高的平均接受率，反而能带来更稳定的加速效果。</p> <p>我们推荐的配置是单次生成<strong>3个或7个</strong>草稿词元（Draft Tokens）。这样，连同上下文的第一个词元，主模型单次仅需验证4个或8个词元。这个数量在当前主流的端侧硬件上达到了<strong>草稿生成开销、批量验证开销与高接受率</strong>之间的最佳平衡。</p> <h4 id="加速效果实测">加速效果实测</h4> <p>在采用上述优化配置后，我们在端侧设备上取得了显著的加速效果。</p> <ul> <li> <strong>场景表现差异</strong>： <ul> <li>在<strong>英文和编程类</strong>任务中，由于其语言结构相对固定，草稿模型的接受率最高。</li> <li> <strong>中文场景</strong>的接受率略低，这与中文语言的复杂性和灵活性有关，但依然表现出色。</li> </ul> </li> <li> <strong>量化加速结果</strong>： <ul> <li> <strong>平均加速效果</strong>：综合各类任务，解码（Decode）阶段的平均速度提升了约 <strong>50%</strong>。</li> <li> <strong>峰值加速效果</strong>：在接受率较高的最佳场景下（如生成结构化代码或固定格式的英文文本），加速比可高达 <strong>100%</strong>，即实现了<strong>推理速度的翻倍</strong>。</li> </ul> </li> </ul> <p>这一结果证明，经过精心的训练和参数调优，Eagle3草稿模型能够在端侧设备上带来稳定且可观的性能提升。</p> <p><img src="/assets/img/eagle/eagle.png" alt="alt text"></p> <h2 id="搞定以及开箱即用的模型">搞定！以及“开箱即用”的模型</h2> <p>好了，以上就是MNN支持Eagle3的全过程。</p> <p>我们从重构 C++ 代码开始，给模型搭配了一套中英混合的<code class="language-plaintext highlighter-rouge">EagleChat</code> 数据集，同时给 SpecForge 社区“添砖加瓦”修了 Bug，最后在端侧设备上简单测试了一下性能，总算把能踩的坑都踩了一遍。</p> <p>我们把最终训练好的“成品”——也就是适配了 Qwen3 的 Eagle3 模型——都放在下面了，欢迎大家下载试玩，或者给我们提 Issue 一起交流。</p> <ul> <li> <p><strong>Hugging Face (抱抱脸)传送门:</strong> <a href="https://huggingface.co/collections/taobao-mnn/eagle3" rel="external nofollow noopener" target="_blank">https://huggingface.co/collections/taobao-mnn/eagle3</a></p> </li> <li> <p><strong>魔搭 (ModelScope) 传送门:</strong> <a href="https://modelscope.cn/collections/Eagle3-408e79eec8b441" rel="external nofollow noopener" target="_blank">https://modelscope.cn/collections/Eagle3-408e79eec8b441</a></p> </li> </ul> <p>就这样，祝大家玩得开心！</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/super-weight/">LLM Super Weight 实测：剪枝降智与量化思考</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/llm-train/">LLM训练实战手册</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwen3vl/">MNN模型支持：Qwen3-VL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwenfamily/">一图读懂Qwen</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/device-llm-memory-capacity/">端侧LLM硬件系列（二）：内存容量</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhaode Wang. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>