<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 端侧LLM硬件系列（二）：内存容量 | Zhaode's blog </title> <meta name="author" content="Zhaode Wang"> <meta name="description" content="A blog about technology and life. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, blog"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhaode.wang/blog/2025/device-llm-memory-capacity/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhaode's blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">端侧LLM硬件系列（二）：内存容量</h1> <p class="post-meta"> Created on September 22, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>随着iPhone 17 Pro系列将运行内存（RAM）从8GB升级至12GB，一个明确的信号已经发出：为了迎接真正的端侧AI时代，移动设备正在着手应对一个新的、也是更基础的硬件瓶颈——内存容量。</p> <p>这次升级并非为了常规的多任务处理，而是为了给“Apple Intelligence”这类日益复杂的端侧大模型提供必要的运行空间。你或许也曾遇到过类似情况：功能强大的AI应用突然闪退，或在处理稍长文档时无响应。这背后的原因，往往不是芯片算力（TOPS）不足，而是手机的物理内存已经耗尽。</p> <p>内存容量是决定端侧AI模型的入场券，这篇文章将深入分析LLM的内存占用构成，探讨各类优化技术，并最终评估在2025年，多大的内存才足以支撑一个流畅的“口袋里的AI大脑”。</p> <h2 id="llm内存占用的构成三大核心部分的量化分析"><strong>LLM内存占用的构成：三大核心部分的量化分析</strong></h2> <p>要理解内存瓶颈，首先需要精确拆解LLM在运行时到底消耗了什么。其内存占用主要由三个动态和静态的部分构成，每一部分都有其详细的计算规则。</p> <h3 id="1-模型权重-model-weights"><strong>1. 模型权重 (Model Weights)</strong></h3> <p>这是模型最基础、最主要的内存开销，可以理解为模型的“知识库”。其大小由参数量和数据精度（Precision）决定。</p> <ul> <li> <strong>数据精度与字节数:</strong> <ul> <li> <strong>FP32 (单精度浮点):</strong> 每参数占4字节。</li> <li> <strong>FP16/BF16 (半精度浮点):</strong> 每参数占2字节，是当前推理最常用的格式。</li> <li> <strong>INT8 (8位整型):</strong> 每参数占1字节（量化后）。</li> <li> <strong>INT4 (4位整型):</strong> 每参数占0.5字节（量化后）。</li> </ul> </li> <li> <strong>计算公式:</strong> <blockquote> <p><strong>内存占用 (GB) ≈ 参数量 (十亿) × 单个参数大小 (Bytes)</strong></p> </blockquote> </li> </ul> <p>基于此公式，我们可以清晰地看到不同规模模型在加载时对内存的静态需求：</p> <table> <thead> <tr> <th style="text-align: left">模型规模（参数）</th> <th style="text-align: left">FP16 (2 Bytes/Param)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>7B (70亿)</strong></td> <td style="text-align: left">≈ 14 GB</td> </tr> <tr> <td style="text-align: left"><strong>13B (130亿)</strong></td> <td style="text-align: left">≈ 26 GB</td> </tr> </tbody> </table> <p><em>注：为方便估算，此表采用1GB = 10^9 Bytes的近似值。</em></p> <p>这个静态值直接决定了模型能否被加载进手机的“生死线”。从表中可见，一个未经优化的7B模型，仅权重部分就超过了市面上绝大多数手机的内存上限。</p> <h3 id="2-激活值-activations"><strong>2. 激活值 (Activations)</strong></h3> <p>在模型进行前向计算（即“思考”过程）时，每一层网络都会产生临时的中间数据。这部分内存占用是动态的，在处理长文本的Prefill（预填充）阶段会达到峰值。</p> <ul> <li> <strong>经验估算法:</strong> 精确计算激活值内存较为复杂，因为它与具体模型架构和推理引擎实现紧密相关。在工程实践中，一个实用的快速估算法则是，<strong>激活值的峰值内存约等于模型权重（FP16）的25%</strong>。对于一个14GB的7B模型，其激活值峰值约为3.5GB。</li> </ul> <h3 id="3-kv-cache"><strong>3. KV Cache</strong></h3> <p>这是在Decoding（解码/逐字生成）阶段为了加速计算而设计的缓存机制。它存储了注意力机制已经计算过的键（Key）和值（Value），避免重复计算。</p> <ul> <li> <p><strong>核心特点:</strong> KV Cache的大小与上下文长度（Sequence Length）成正比，是长对话或长文总结场景下最主要的内存增长点。</p> </li> <li> <p><strong>精确计算公式:</strong></p> <blockquote> <p><strong>KV Cache内存 (Bytes) ≈ 2 × 上下文长度 × 模型层数 × 注意力头个数 × 注意力维度 × 精度(Bytes)</strong></p> </blockquote> <p>不同模型的具体架构参数不同，导致KV Cache大小差异巨大。以一个采用了GQA结构优化的现代模型，如<strong>Qwen2-7B</strong>（拥有28个Transformer层，K/V注意力头为4个，每个头的维度为128）为例，在FP16精度下，处理4096个Token的上下文，其KV Cache占用约为：</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">2 × 4096 × 28 × 4 × 128 × 2 Bytes ≈ 235 MB</code></p> </blockquote> </li> </ul> <h3 id="综合内存占用分析"><strong>综合内存占用分析</strong></h3> <p>现在，我们将一个典型的、未经优化的7B模型的FP16内存占用进行加总，看看它的峰值需求有多庞大：</p> <table> <thead> <tr> <th style="text-align: left">内存组成部分</th> <th style="text-align: left">FP16 (未优化)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>权重内存 (静态)</strong></td> <td style="text-align: left">≈ 14.0 GB</td> </tr> <tr> <td style="text-align: left"><strong>激活值内存 (动态峰值, 估算)</strong></td> <td style="text-align: left">≈ 3.5 GB</td> </tr> <tr> <td style="text-align: left"><strong>KV Cache (4K tokens, 典型值)</strong></td> <td style="text-align: left">≈ 2.1 GB</td> </tr> <tr> <td style="text-align: left"><strong>预估总内存峰值</strong></td> <td style="text-align: left"><strong>≈ 19.6 GB</strong></td> </tr> </tbody> </table> <p><em>注：KV Cache大小因模型结构而异，此处采用一个典型非GQA优化模型的数值以展示问题的严重性。</em></p> <p>结论已经非常清晰：一个未经优化的7B模型，其近20GB的峰值内存需求，在移动端是绝对无法满足的物理限制。这使得下一章将要讨论的内存优化技术，不再是“可选项”，而是让端侧大模型成为现实的“必需品”。</p> <hr> <h2 id="内存优化技术在有限容量下实现可能"><strong>内存优化技术：在有限容量下实现可能</strong></h2> <p>在物理内存有限的前提下，软件和算法层面的优化是让大模型在端侧运行的关键。这些技术主要围绕内存占用的三个核心部分展开。</p> <h3 id="1-权重量化-weight-quantization"><strong>1. 权重量化 (Weight Quantization)</strong></h3> <p>这是最核心、效果最显著的模型压缩技术。其原理是将高精度（如FP16）的浮点数权重，转换为低精度（如INT8或INT4）的整数，从而大幅减少模型的存储体积。</p> <ul> <li> <strong>技术细节：</strong> 为保证精度，现代量化方案（如GPTQ/AWQ）普遍采用<strong>分组量化 (Grouped Quantization)</strong>。即将权重分为多个小组（例如每32个或64个为一组），并为每个小组计算独立的量化参数（<strong>Scale</strong>，即比例尺）。</li> <li> <strong>有效比特率 (bpw):</strong> 这也意味着量化后的实际成本会略高于其标称位数。例如，对一个INT4量化模型，如果每32个权重共享一个FP16（16 bit）的Scale，则每个权重的平均占用为 <code class="language-plaintext highlighter-rouge">(32 * 4 + 16) / 32 = 4.5 bpw</code>。</li> <li> <strong>优化结果：</strong> 基于4.5 bpw计算，一个原本需要14GB的7B模型，其权重体积可以被压缩至约 <strong>3.7 GB</strong>，使其具备了在移动设备上加载的可能性。</li> </ul> <h3 id="2-激活值优化技术"><strong>2. 激活值优化技术</strong></h3> <p>主要用于降低处理长序列时产生的瞬时内存峰值，核心思路是“以时间换空间”。</p> <ul> <li> <strong>激活重计算 (Activation Recomputation):</strong> 不在内存中保留所有中间层的激活值，而是在需要时通过前向计算从上一个“检查点”重新推导。</li> <li> <strong>分块预填充 (Chunk Prefill):</strong> 将长输入切分成小块，逐块进行计算并填充KV Cache。每处理完一小块，其对应的激活值内存即可释放，从而避免了巨大的瞬时内存开销。</li> </ul> <h3 id="3-kv-cache-优化技术"><strong>3. KV Cache 优化技术</strong></h3> <p>旨在降低长上下文场景下的内存占用。</p> <ul> <li> <strong>KV Cache量化:</strong> 同样可以将KV Cache中的数据从FP16量化至INT8，直接将其内存占用降低50%。</li> <li> <strong>模型结构优化 (GQA):</strong> 分组查询注意力（Grouped-Query Attention, GQA）是目前主流的优化结构。它通过让多组“注意力头”共享同一套K和V缓存，在大幅降低KV Cache内存占用的同时，实现了性能和效果的最佳平衡。</li> </ul> <h3 id="4-混合存储方案"><strong>4. 混合存储方案</strong></h3> <p>当上述优化仍无法满足需求时，最后的手段是利用速度较慢但容量巨大的闪存。</p> <ul> <li> <strong>内存卸载 (Offloading):</strong> 将当前不活跃的模型层（比如<code class="language-plaintext highlighter-rouge">Embedding</code>层）或较早的KV Cache从RAM中转移到闪存，需要时再加载回来。这是一种以牺牲响应速度（延迟）为代价，换取更大有效容量的终极方案。</li> </ul> <hr> <h3 id="优化成效分析全新的内存占用"><strong>优化成效分析：全新的内存占用</strong></h3> <p>经过上述一系列组合拳式的优化，我们现在可以重新计算同一个7B模型在端侧运行时的实际内存占用：</p> <table> <thead> <tr> <th style="text-align: left">内存组成部分</th> <th style="text-align: left">优化后 (4-bit量化 + GQA)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>权重内存 (4.5 bpw)</strong></td> <td style="text-align: left">≈ 3.7 GB</td> </tr> <tr> <td style="text-align: left"><strong>激活值内存 (动态峰值, 估算)</strong></td> <td style="text-align: left">≈ 0.9 GB</td> </tr> <tr> <td style="text-align: left"><strong>KV Cache (4K tokens, Qwen2-7B, INT8)</strong></td> <td style="text-align: left">≈ 0.12 GB</td> </tr> <tr> <td style="text-align: left"><strong>预估总内存峰值</strong></td> <td style="text-align: left"><strong>≈ 4.7 GB</strong></td> </tr> </tbody> </table> <p>从接近20GB到不足5GB——这是一个超过75%的惊人降幅。这标志着，一个强大的70亿参数大模型，在理论上终于获得了进入主流8GB、畅行12GB内存手机的“资格”。然而，理论上的可行性，还需要通过操作系统层面严苛的现实考验。</p> <hr> <h2 id="操作系统的内存管理机制"><strong>操作系统的内存管理机制</strong></h2> <p>优化后的模型最终仍需在操作系统的管理下运行。Android和iOS的机制差异，直接影响了应用的稳定性。</p> <ul> <li> <strong>Android:</strong> 采用<strong>低内存杀手（Low-Memory Killer, LMK）</strong>机制。当系统总内存不足时，会根据进程优先级强制“杀死”后台应用。一个占用数GB内存的AI应用，即便能运行，也非常容易在切换到后台时被LMK清理。</li> <li> <strong>iOS:</strong> 对每个应用有严格的内存使用上限。一旦应用消耗的“脏内存”（Dirty Memory，由程序动态分配的内存）超限，就会被系统强制终止（闪退）。 <ul> <li>在iOS中运行LLM类型的任务，需要申请<code class="language-plaintext highlighter-rouge">com.apple.developer.kernel.increased-memory-limit</code>权限，可以提升APP的内存上限。</li> <li>另外一个有效方案是使用<code class="language-plaintext highlighter-rouge">mmap</code>技术加载模型权重。<code class="language-plaintext highlighter-rouge">mmap</code>将只读的权重映射为“干净内存”（Clean Memory），这部分内存iOS并不会计入APP的内存占用，从而避免了因触及“脏内存”上限导致闪退。</li> </ul> </li> </ul> <hr> <h2 id="结论多大内存的手机才配得上ai大脑的称号"><strong>结论：多大内存的手机，才配得上“AI大脑”的称号？</strong></h2> <p>综合以上分析，我们可以得出一份基于2025年主流设备的AI能力评估矩阵：</p> <h3 id="真实世界ai能力适配矩阵-2025年机型"><strong>真实世界AI能力适配矩阵 (2025年机型)</strong></h3> <table> <thead> <tr> <th style="text-align: left">设备典型代表</th> <th style="text-align: left">物理内存容量</th> <th style="text-align: left">运行7B模型 (4-bit, GQA) 体验评估</th> <th style="text-align: left">运行13B模型 (4-bit, GQA) 体验评估</th> <th style="text-align: left">场景分析与最终评价</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>标准版iPhone 17 / 主流安卓中端机</strong></td> <td style="text-align: left"><strong>8GB</strong></td> <td style="text-align: left"><strong>勉强可用</strong></td> <td style="text-align: left"><strong>几乎不可行</strong></td> <td style="text-align: left"> <strong>体验受限</strong>：模型加载后系统可用内存极低，多任务处理能力差，后台易被终止。仅适合轻量级、非连续的AI任务。物理容量是硬伤。</td> </tr> <tr> <td style="text-align: left"><strong>iPhone 17 Pro / 安卓主流高端机</strong></td> <td style="text-align: left"><strong>12GB</strong></td> <td style="text-align: left"><strong>体验流畅</strong></td> <td style="text-align: left"><strong>基本可行</strong></td> <td style="text-align: left"> <strong>AI体验基准</strong>：为当前主流的7B模型提供了充足的运行空间和良好的多任务缓冲。是硬件容量和软件优化结合的最佳平衡点，构成了高质量端侧AI体验的基础。</td> </tr> <tr> <td style="text-align: left"><strong>安卓旗舰机</strong></td> <td style="text-align: left"><strong>16GB</strong></td> <td style="text-align: left"><strong>游刃有余</strong></td> <td style="text-align: left"><strong>体验流畅</strong></td> <td style="text-align: left"> <strong>面向未来</strong>：不仅能轻松驾驭13B模型，也为更复杂的AI应用（如多模态交互）预留了充足的硬件冗余。是追求极致性能和长期AI体验保障的最佳选择。</td> </tr> <tr> <td style="text-align: left"><strong>安卓顶级旗舰机</strong></td> <td style="text-align: left"><strong>24GB</strong></td> <td style="text-align: left"><strong>性能过剩</strong></td> <td style="text-align: left"><strong>游刃有余</strong></td> <td style="text-align: left"> <strong>开发与探索</strong>：远超当前普通用户的需求，主要价值在于为开发者提供了一个不受内存束缚的实验平台，探索端侧AI的未来可能性，可运行<code class="language-plaintext highlighter-rouge">GPT-OSS-20B</code>。</td> </tr> </tbody> </table> <hr> <h3 id="最终结论"><strong>最终结论</strong></h3> <p>分析指向一个明确的结果：<strong>物理内存容量是端侧AI能力的根基，而软件优化则是在这个根基上实现效率最大化的手段。</strong></p> <ol> <li> <strong>8GB内存</strong> 在端侧AI时代已显不足，无法提供稳定、流畅的大模型体验。</li> <li> <strong>12GB内存</strong> 正如iPhone 17 Pro所展示的，已成为2025年“AI手机”的有效起步线，是保证高质量体验的基础。</li> <li> <strong>16GB内存</strong> 则提供了更强的性能保障和面向未来的扩展性，足以流畅运行下一代百亿参数模型。</li> </ol> <p>最终，决定手机AI体验上限的，不再仅仅是算力（TOPS）数字，而是那个实实在在的物理内存容量。在这场将AI装进口袋的竞赛中，内存，正是那张最关键的“入场券”。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwenfamily/">一图读懂Qwen</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwen3-next/">Qwen3-Next：下一代MoE模型架构解析</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/minicpm/">MNN模型支持：面壁小钢炮MiniCPM-V-4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/device-llm-memory-bandwidth/">端侧LLM硬件系列（一）：内存带宽</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/coreml-conv1d/">CoreML踩坑记：慎用Conv1D</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhaode Wang. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>