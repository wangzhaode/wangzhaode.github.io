<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 端侧LLM硬件系列（一）：内存带宽 | Zhaode's blog </title> <meta name="author" content="Zhaode Wang"> <meta name="description" content="A blog about technology and life. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, blog"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhaode.wang/blog/2025/device-llm-memory-bandwidth/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhaode's blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">端侧LLM硬件系列（一）：内存带宽</h1> <p class="post-meta"> Created on September 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a>   <a href="/blog/category/device"> <i class="fa-solid fa-tag fa-sm"></i> device</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>在AI大模型席卷一切的今天，我们都期待手机成为真正的“口袋里的AI大脑”。但你有没有感觉，手机上的AI助手总是慢半拍？无论是想让它快速总结一篇长文，还是在图片编辑时使用“AI消除”功能，那种等待的延迟感，总在提醒我们“理想与现实的差距”。</p> <p>这个瓶颈，或许不在于芯片厂商们大力宣传的算力（TOPS）数字有多高，而在于一个常常被忽视的参数——内存带宽。</p> <p>这就像拥有了一台超级跑车的引擎（AI计算核心），却只给它配了一根细细的油管，空有一身力气却踩不上油门。</p> <h2 id="为什么说llm的性能主要卡在内存带宽"><strong>为什么说LLM的性能，主要卡在内存带宽？</strong></h2> <p>要理解这个问题，我们首先需要知道，大模型在手机上生成内容，分为两个关键阶段：</p> <ol> <li> <strong>Prefill（预填充/提示处理）</strong>：这是模型“读懂问题”的阶段。它会并行处理你输入的所有文字（Prompt），比如“帮我写一首关于星空的诗”。这个阶段计算量大，可以充分利用NPU/GPU的并行计算能力，速度通常很快。</li> <li> <strong>Decoding（解码/逐字生成）</strong>：这是模型“思考并回答”的阶段。它会一个字一个字地生成答案，比如“浩瀚的…夜空…”。每生成一个新字，都需要把前面所有的字连同新生成的字一起，再和全部的模型参数进行一次运算。</li> </ol> <p><strong>而瓶颈，恰恰就出在这个决定了用户最终体验的“解码”阶段。</strong></p> <blockquote> <p><strong>一个反直觉的现象：为什么强大的NPU/GPU，在解码时反而可能不如CPU？</strong></p> <p>很多技术测试都发现一个奇怪的现象：在手机上运行LLM时，<strong>Prefill阶段用NPU/GPU处理，速度极快。但到了最关键的Decoding阶段，它们的生成速度经常还不如直接用CPU。</strong></p> <p><strong>原因正是内存带宽的制约，我们可以用一个简单的公式来揭示真相：</strong></p> \[\text{理论解码速度（Tokens/s）} \approx \frac{\text{内存带宽（GB/s）}}{\text{模型单次推理数据量（GB）}}\] <p>这个公式告诉我们，解码速度的上限，完全取决于“数据搬运”的效率。<strong>每生成一个字（Token），都需要将重达数GB的模型参数从系统内存（RAM）完整地“搬运”一遍。</strong></p> <ul> <li> <strong>NPU/GPU的“窘境”</strong>：它们算力再强，也得等数据从内存中漫长地传输过来。大部分时间都在“等米下锅”，强大的算力被闲置，自然快不起来。</li> <li> <strong>CPU的“智慧”</strong>：CPU核心虽少，但它自带的<strong>巨大高速缓存（Cache）</strong> 相当于一个“厨房里的冰箱”。它可以把最常用的参数暂存起来随用随取，避免了频繁访问“几公里外的大仓库（RAM）”，因此在解码这种“计算量小、数据访问频繁”的任务上反而效率更高。</li> </ul> <p><strong>让我们量化一下这个瓶颈：</strong> 假设一款手机的内存带宽是<strong>64 GB/s</strong>，运行一个需要加载<strong>4GB</strong>参数的7B模型。理论上，它每秒最多只能生成 <code class="language-plaintext highlighter-rouge">64 ÷ 4 = 16</code> 个Token。一个汉字约等于2个Token，这意味着手机每秒最多能生成8个汉字——这个速度已经低于人类的平均阅读速度，用户会明显感觉到“卡顿”。如果想让体验流畅（如32 Tokens/s），内存带宽就需要翻倍到<strong>128 GB/s</strong>。</p> <p><strong>至此，结论已经非常清晰：</strong> 对于决定LLM交互流畅度的解码环节，数据传输效率远比纯粹的计算能力更重要。这就是AI社区将解码称为“内存密集型”（Memory-bound）任务的根本原因。</p> </blockquote> <hr> <h2 id="拆解带宽瓶颈天花板与破局之路"><strong>拆解带宽瓶颈：“天花板”与“破局之路”</strong></h2> <p>要理解手机AI性能的真正瓶颈，我们必须深入到内存带宽的“引擎室”，看看它由什么构成，又被什么所限制。</p> <p>内存带宽的计算公式看似简单，由两个核心参数相乘决定：</p> \[\text{内存带宽（GB/s）} = \frac{\text{数据传输速率（MT/s）} \times \text{总线宽度（bits）}}{8}\] <ul> <li> <strong>数据传输速率 (Data Rate):</strong> 这是我们最常听到的参数，比如LPDDR5X-8533。它就像是数据高速公路上的<strong>“最高限速”</strong>，是各大厂商目前提升带宽最直接、竞争最激烈的战场。</li> <li> <strong>内存总线宽度 (Bus Width):</strong> 这可以理解为高速公路的<strong>“车道数量”</strong>。目前，旗舰手机芯片几乎无一例外地被“焊死”在了<strong>64-bit</strong>这个宽度上。</li> </ul> <p>问题来了：既然车道越多，数据并行能力越强，为什么手机芯片不像PC显卡（如RTX 4090拥有384-bit总线）那样，把“路”修得更宽呢？</p> <p>答案是，在手机内部寸土寸金的空间里，拓宽总线这条“物理之路”几乎已经走到了尽头。它面临着三座难以逾越的大山，也就是我们所说的<strong>“物理天花板”</strong>：</p> <ol> <li> <strong>空间限制：</strong> 每一bit总线都需要一条独立的、像头发丝一样精密的PCB走线。将64-bit翻倍到128-bit，意味着布线复杂度和面积需求的指数级增长，这在手机主板上是不可想象的。</li> <li> <strong>信号干扰：</strong> “车道”越多、越密，彼此间的“信号串扰”就越严重，数据很容易出错。为了保证信号同步，所有走线的长度必须像军人列队般精确对齐，这对设计和制造工艺是巨大的挑战。</li> <li> <strong>功耗失控：</strong> 每一条“车道”都需要消耗电能来驱动数据传输。粗暴地加宽总线会直接导致功耗和发热激增，这对于依靠电池续命的手机来说是致命的。</li> </ol> <p>既然横向的“扩路”已触及天花板，聪明的芯片工程师们便另辟蹊径，开启了三条通往未来的<strong>“破局之路”</strong>。</p> <h4 id="破局之路一纵向提速--不断刷新的数据率-lpddr6及后续"><strong>破局之路一：纵向提速 —— 不断刷新的数据率 (LPDDR6及后续)</strong></h4> <p>这是最经典、最确定的演进路线：既然路宽不了，那就让车速更快。从LPDDR4X的4266MT/s，到LPDDR5X的8533MT/s，再到即将登场的<strong>LPDDR6</strong>（起步速率可达<strong>12800MT/s</strong>），这条路一直在坚定地向前延伸。它虽然可靠，但边际效应也开始显现，更高的速率对功耗和信号控制的要求也越来越苛刻。</p> <h4 id="破局之路二空间折叠--用3d封装拓宽总线-先进封装技术"><strong>破局之路二：空间折叠 —— 用3D封装拓宽总线 (先进封装技术)</strong></h4> <p>既然平面上修路已无可能，那就“向上”发展，搭建“立交桥”。<strong>先进的3D封装技术</strong>，允许将内存芯片（DRAM）直接堆叠在SoC芯片的上方，通过成千上万条超短的垂直通道连接。这就像把两个城市直接叠在一起，交通距离无限缩短。</p> <ul> <li> <strong>核心优势：</strong> 极短的距离意味着极低的功耗和信号干扰，这使得<strong>在垂直方向上实现128-bit甚至更宽的总线</strong>成为可能。PC领域的HBM（高带宽内存）正是该技术的体现，而移动端正在全力探索如何将其小型化、低功耗化，这被视为未来颠覆带宽格局的关键。</li> </ul> <h4 id="破局之路三修建前置仓--用系统级缓存抄近道-slc"><strong>破局之路三：修建“前置仓” —— 用系统级缓存抄近道 (SLC)</strong></h4> <p>如果每次都去几公里外的大仓库（系统内存）取货太慢，那就在车间旁边建一个“前置仓”（缓存）。<strong>扩大和优化系统级缓存（System Level Cache, SLC）</strong>正是这一思路的体现。这块大容量高速缓存由CPU、GPU、NPU等所有计算核心共享。</p> <ul> <li> <strong>工作原理：</strong> 在运行LLM时，芯片可以将最核心的模型参数（如注意力权重）提前加载到SLC这个“前置仓”里。当NPU需要时，直接从“隔壁”高速调取，<strong>彻底避免了访问外部系统内存的漫长等待</strong>。苹果和联发科都是该策略的忠实拥趸，通过不断增大片上缓存，在实际AI应用中获得了远超理论带宽的能效表现。</li> </ul> <hr> <h2 id="2025年旗舰芯片内存带宽大比拼谁的水管最粗"><strong>2025年旗舰芯片内存带宽大比拼：谁的“水管”最粗？</strong></h2> <p>进入2025年，各家旗舰芯片在内存带宽上展开激烈角逐。基于现有信息和行业预测，我们整理了以下旗舰芯片的内存系统分析：</p> <table> <thead> <tr> <th style="text-align: left">芯片型号</th> <th style="text-align: left">内存标准</th> <th style="text-align: left">理论峰值速率</th> <th style="text-align: left">理论峰值带宽</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>联发科 天玑9400/9500</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">10667 Mbps</td> <td style="text-align: left"><strong>85.3 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>高通 骁龙 8 Elite</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">9600 Mbps</td> <td style="text-align: left"><strong>76.8 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>三星 Exynos 2500</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">9600 Mbps</td> <td style="text-align: left"><strong>76.8 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>高通 骁龙 8 Elite Gen 5</strong></td> <td style="text-align: left">LPDDR6</td> <td style="text-align: left"><strong>最高14400 Mbps</strong></td> <td style="text-align: left"><strong>最高115.2 GB/s</strong></td> </tr> <tr> <td style="text-align: left"><strong>苹果 A19 Pro</strong></td> <td style="text-align: left">LPDDR5X</td> <td style="text-align: left">待定</td> <td style="text-align: left">待定</td> </tr> </tbody> </table> <ul> <li> <p><strong>联发科：激进的LPDDR5X优化者</strong> 联发科一直致力于在LPDDR5X标准下压榨极限。<strong>天玑9400/9500</strong>系列率先将内存速率推高到惊人的<strong>10667Mbps</strong>，一举刷新行业记录，并以<strong>85.3GB/s</strong>的理论带宽领跑。同时，它还通过增大片上缓存（L3/SLC）来减少对主内存的访问，进一步提升了数据处理效率。</p> </li> <li> <p><strong>高通：LPDDR6的先行者</strong> 高通在<strong>骁龙8 Elite</strong>上采用了<strong>9600Mbps</strong>的LPDDR5X，与三星Exynos 2500并驾齐驱。但更引人注目的是，它很可能成为首个在<strong>骁龙8 Elite Gen 5</strong>上商用<strong>LPDDR6</strong>的手机芯片，这将使带宽一举突破100GB/s大关，达到<strong>115.2GB/s</strong>，为未来的端侧大模型奠定坚实基础。</p> </li> <li> <p><strong>苹果：从容的内存容量策略</strong> 尽管苹果的A系列芯片在内存带宽数据上相对封闭，但最新的A19 Pro芯片却做出了一个罕见而关键的升级：将<strong>iPhone 17 Pro</strong>的运行内存从8GB提升至<strong>12GB</strong>。这背后正是为了适配“Apple Intelligence”等AI功能，通过更大的容量来承载更复杂的模型和数据，间接缓解了带宽压力。这也可以看作是对‘破局之路三’中增大缓存、减少主内存访问策略的一种宏观体现。</p> </li> </ul> <hr> <h2 id="总结"><strong>总结</strong></h2> <p>内存带宽无疑是当前制约手机LLM性能的头号瓶颈，其重要性甚至超过了单纯的算力堆砌。在这场竞赛中，联发科凭借对LPDDR5X的极致优化暂时领跑，而高通则可能通过率先拥抱LPDDR6标准，在未来实现颠覆性突破。</p> <p>然而，决定最终AI体验的，将是一个完整的“供水系统”。无论是继续提升内存速率，还是探索先进封装和更大系统缓存等未来技术，其最终目的都是为了构建一个高效的数据流。除了加粗“水管”（内存带宽），还需要强大的“水泵”（NPU/GPU算力）、足够大的“蓄水池”（内存容量），以及智能的“用水策略”（算法与软件优化）。</p> <p>只有当硬件、软件和AI模型三方协同进化，我们才能真正迎来那个理想的未来：在手机上与强大、流畅、懂你心意的AI进行实时互动。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mnn-eagle/">MNN支持Eagle3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/llm-train-improved/">LLM训练实战：从零到一的完整指南</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/llm-train/">LLM训练实战手册</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwen3vl/">MNN模型支持：Qwen3-VL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwenfamily/">一图读懂Qwen</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhaode Wang. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>