<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLM训练实战：从零到一的完整指南 | Zhaode's blog </title> <meta name="author" content="Zhaode Wang"> <meta name="description" content="A blog about technology and life. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, blog"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhaode.wang/blog/2025/llm-train-improved/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhaode's blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLM训练实战：从零到一的完整指南</h1> <p class="post-meta"> Created on November 11, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> llm</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="1-前言">1. 前言</h2> <p>在旁观者眼中，一篇光鲜亮丽的研究论文似乎总把模型训练描绘得轻而易举：正确的架构选择、精心策划的数据集，再加上充足的算力，一切水到渠成。最终的结果光彩夺目，消融实验（ablations）清晰明了，仿佛每一个决策都是理所当然。</p> <p>然而，这些报告往往只展示了成功的“果”，并用“玫瑰色的滤镜”回顾了过程。它们没有记录下那些<strong>凌晨两点还在调试的 dataloader</strong>、<strong>训练中突然爆发的 Loss 尖峰</strong>，或是那个<strong>悄无声息破坏你训练的张量并行（Tensor Parallelism）Bug</strong>。现实远比论文描述的更加混乱、更依赖迭代，充满了那些最终未能写进纸面的“纠结瞬间”。</p> <p>这一次，我们将带你深入幕后，直击 <strong>SmolLM</strong> 的训练全程——这是一个在 <strong>11万亿（11T）Token</strong> 上训练出来的 <strong>30亿（3B）参数多语言推理模型</strong>。</p> <p>这并非一篇按部就班的博客，而是一份详尽的实战梳理，它网罗了我们在构建世界级语言模型过程中的所有决策、发现乃至走过的弯路，旨在为你揭示那些最核心的洞察。</p> <p>同时，本文也是我们<strong>模型训练系列长文</strong>的收官之作。此前，我们已经深入探讨了<strong>大规模数据集的构建</strong>（FineWeb）、<strong>如何调度数千块GPU协同工作</strong>（Ultra Scale Playbook），以及<strong>如何在每个环节选择最佳评估方法</strong>（Evaluation Guidebook）。现在，我们将把所有这些要素融会贯通，共同打造一个强大的AI模型。</p> <p>我们将全程陪伴，不仅分享最终成功的“秘方”，更会剖析那些<strong>塑造了每一个决策的失败、基础设施故障以及艰难的调试过程</strong>。</p> <p>这个故事读起来就像一出<strong>扣人心弦的“戏剧”</strong>：</p> <ul> <li>你会看到，为何那些在小规模实验中前景光明的想法，到大规模训练时却可能<strong>“水土不服”</strong>。</li> <li>为什么我们在训练了 <strong>1万亿（1T）Token 后不得不从头重启</strong>。</li> <li>如何有效地平衡<strong>多语言、数学和代码</strong>这几个相互竞争的目标，同时保持强大的英语性能。</li> <li>以及最终，我们是如何<strong>后训练（post-train）</strong>出一个混合推理模型的。</li> </ul> <p>我们努力将这趟冒险之旅组织成一个连贯的故事，而非一份冰冷的步骤清单。请将它视为一本<strong>实战手册</strong>，献给所有渴望从“我们有优秀的数据集和GPU”迈向“我们构建了一个真正强大的模型”的实践者。</p> <p>我们希望这种毫无保留的分享，能够<strong>弥合研究与生产之间的鸿沟</strong>，让你下一次的训练之旅<strong>少一些混乱，多一份从容</strong>。</p> <h3 id="11-如何阅读这篇文章">1.1 如何阅读这篇文章？</h3> <p>坦白说，这篇文章篇幅<strong>极长</strong>，想一口气从头读到尾并不现实。</p> <p>好消息是，我们已将文章结构化为几个<strong>相对独立的板块</strong>，你可以根据自己的兴趣和需求，选择性地阅读或跳过。</p> <p>以下是文章的结构指南：</p> <ul> <li> <strong>🧭 训练指南针 (Training Compass)</strong>： <ul> <li> <strong>核心内容：</strong> 宏观探讨你<strong>是否应该亲自预训练（Pretrain）一个模型</strong>。</li> <li> <strong>适用人群：</strong> 在你“烧光”所有风投资金之前，我们为你列出几个必须扪心自问的问题，并系统地指导你完成决策。</li> <li> <strong>温馨提示：</strong> 这部分偏向战略思考。如果你是<strong>“技术党”</strong>，想直奔硬核内容，可以快速浏览。</li> </ul> </li> <li> <strong>🚀 预训练 (Pretraining)</strong>： <ul> <li> <strong>核心内容：</strong> 涵盖了构建可靠预训练流程所需的一切知识：如何运行消融实验、选择评估指标、混合数据源、制定架构决策、调整超参数，以及最终<strong>“熬过”</strong>这场训练马拉松。</li> <li> <strong>适用人群：</strong> 无论你是计划<strong>从头预训练</strong>，还是对<strong>持续预训练（Continued Pretraining）</strong>感兴趣，这个板块都值得一读。</li> </ul> </li> <li> <strong>🍒 后训练 (Post-training)</strong>： <ul> <li> <strong>核心内容：</strong> 你将学到最大化利用预训练模型所需的所有“技巧”。从 <strong>SFT (监督式微调)</strong>、<strong>DPO (直接偏好优化)</strong> 到 <strong>模型合并（Model Merging）</strong>，我们将揭示这些充满“炼金术”色彩操作背后的门道。</li> <li> <strong>价值所在：</strong> 我们将分享那些通过惨痛教训换来的宝贵经验，希望能让你在实践中少走弯路。</li> </ul> </li> <li> <strong>🏭 基础设施 (Infrastructure)</strong>： <ul> <li> <strong>核心比喻：</strong> 如果说预训练是蛋糕，后训练是糖霜，那么<strong>基础设施就是那台工业级烤箱</strong>。没有它，一切免谈；如果它出故障，你愉快的烘焙时光就会变成一场灾难。</li> <li> <strong>核心内容：</strong> 本节将详细讲解 <strong>GPU 布局、各组件间的通信模式</strong>，以及如何<strong>识别和克服瓶颈</strong>，这些知识往往分散在各种库、文档和论坛中。</li> </ul> </li> </ul> <p>那么，从何处开始呢？很简单，<strong>选择你最感兴趣的板块，让我们即刻出发！</strong></p> <h2 id="2-训练指南针灵魂三问why--what--how">2. 训练指南针：灵魂三问（Why → What → How）</h2> <p>机器学习领域似乎对<strong>“优化”</strong>有种近乎痴迷的执着。我们的目光总是聚焦于Loss曲线、模型架构和训练吞吐量。然而，在深入这些技术细节之前，一个更根本的问题常常被忽略：<strong>我们真的有必要从头训练这个模型吗？</strong></p> <p>如今的开源AI生态系统几乎每天都在涌现世界级的模型：Qwen、Gemma、DeepSeek、Llama……这份名单还在不断加长。它们不再是简单的研究原型，而是<strong>可以直接用于生产的强大模型</strong>，覆盖了从多语言理解到代码生成，再到复杂推理的广泛应用场景。更重要的是，它们大多附带<strong>宽松的许可协议</strong>，并拥有活跃的社区随时提供支持。</p> <p>这也引出了一个<strong>残酷的真相</strong>：也许，你根本不需要训练自己的模型。</p> <p>这听起来像是一个“大模型训练指南”最奇怪的开场白。但事实是，许多失败的训练项目，并非因为超参数设置不当或代码有Bug，而是因为<strong>决策者决定训练一个他们根本不需要的模型</strong>。因此，在投入真金白银和时间之前，你必须先回答两个核心问题：<strong>你为什么要训练这个模型？</strong> <strong>你应该训练一个什么样的模型？</strong> 如果对这两个问题没有清晰的答案，你很可能会浪费数月的算力与工程师时间，最终造出一个“重复的轮子”，甚至更糟——<strong>一个根本没人需要的东西</strong>。</p> <p>让我们从 <strong>“Why”（为什么）</strong> 开始。因为如果目标不明确，后续的所有决策都将是<strong>无的放矢</strong>。</p> <h3 id="21-why那个没人愿意回答的问题">2.1 Why：那个没人愿意回答的问题</h3> <p>让我们坦诚地谈谈现实中经常发生的一幕。</p> <p>某人（如果足够幸运）获得了GPU集群的访问权限，思路通常是这样的：“我们有100块H100，闲置三个月，不如来训练一个模型吧！”于是，模型大小被随意选定，数据集从各处拼凑而来，训练就这样开始了。六个月后，算力预算和团队士气消耗殆尽，模型却无人问津，<strong>只因从未有人认真问过“为什么”</strong>。</p> <p>以下是一些<strong>绝对不应该成为你训练模型理由</strong>的常见想法：</p> <ul> <li>“我们恰好有空闲的算力。”（这是<strong>资源</strong>，不是<strong>目标</strong>。）</li> <li>“别人都在做。”（这是<strong>同侪压力</strong>，不是<strong>战略</strong>。）</li> <li>“AI是未来。”（这是<strong>陈词滥调</strong>，不是<strong>计划</strong>。）</li> <li>“我们想要最强大的模型。”（这个目标<strong>过于模糊</strong>，无法指导任何实际决策。）</li> </ul> <p>“我们训练了自己的模型”这句话听起来很有诱惑力。但在投入大量资源之前，请务必扪心自问：<strong>你到底为什么需要训练这个模型？</strong></p> <p>在启动一个大型预训练项目前，你应该先理清思路。从技术角度看，首要问题是：<strong>是否已经有一个现成的模型，通过简单的提示（Prompt）或微调（Fine-tune）就能满足你的需求？</strong></p> <p>本质上，只有在以下三个领域，定制化的预训练才可能真正有意义：<strong>你想进行开创性的研究，你的生产用例有非常特殊的需求，或者你想填补开源生态系统中的某个空白</strong>。让我们逐一分析：</p> <h4 id="211-科研research你到底想验证什么">2.1.1 科研（Research）：你到底想验证什么？</h4> <p>在LLM领域，有无数值得研究的课题。这些研究项目的共同点是，它们通常都始于一个<strong>清晰定义的问题</strong>。例如：</p> <ul> <li>我们能否将基于某种<strong>新型优化器</strong>的训练扩展到<strong>百亿参数以上</strong>的模型？</li> <li> <strong>仅使用强化学习，不依赖SFT</strong>，能否有效激发模型的推理能力？</li> <li>我们能否<strong>仅靠纯合成的“教科书式”数据</strong>，训练出优秀的小型模型？</li> <li>我们能否通过只训练<strong>公开许可的数据</strong>来达到具有竞争力的性能？</li> </ul> <p>把研究假设设定得<strong>尽可能具体</strong>，并提前思考所需的<strong>实验规模</strong>，能够大大增加成功的几率。</p> <h4 id="212-产品化production为什么现有模型不够用">2.1.2 产品化（Production）：为什么现有模型不够用？</h4> <p>企业之所以不能直接使用现成的通用模型（off-the-shelf models），主要有三个原因。其中两个是技术性的，另一个则关乎治理。</p> <p><strong>第一个原因是领域特殊性（Domain Specificity）。</strong> 当你的数据或任务涉及<strong>高度专业化的词汇或结构</strong>，而现有通用模型无法很好地处理时。例如：</p> <ul> <li>一个处理DNA序列的模型，需要独特的词汇表和处理长距离依赖关系的能力。</li> <li>一个法律或金融模型，要求对领域特定的术语和逻辑有深刻的理解。</li> </ul> <p><strong>第二个原因是部署约束（Deployment Constraints）。</strong> 当你需要模型适配特定的硬件、延迟或隐私要求时。例如，一个需要在<strong>无人机上运行</strong>，或在<strong>配备定制硬件（如FPGA）的本地系统</strong>上运行的大模型。</p> <p>这里有一个简单的测试方法：花几天时间，基于Qwen、Gemma或其他SOTA模型进行构建。你能通过<strong>提示（Prompting）、工具调用（Tool-use）或后训练（Post-training）</strong> 达到你的性能目标吗？如果不能，那么或许就该自己训练一个了。</p> <ul> <li> <strong>友情提醒：</strong> 即使为了满足需求，所需的<strong>后训练预算非常庞大，它仍可能比从头开始预训练更经济。</strong> 毕竟，为模型微调<strong>1万亿（1T）Token</strong>，依然比从头训练<strong>10万亿（10T）Token</strong>更划算。</li> </ul> <p><strong>第三个原因是安全与治理（Safety and Governance）。</strong> 如果你身处一个受严格监管的行业或处理高风险应用，你需要对训练数据、模型行为和更新周期拥有<strong>完全的控制权</strong>。你需要确切地知道模型中包含了什么，并能够向监管机构证明这一点。在这种情况下，自建模型可能是唯一的选择。</p> <p>以上是企业训练内部模型的主要原因。那么，那些发布开源模型的组织又是如何考虑的呢？</p> <h4 id="213-战略性开源strategic-open-source你是否看到了可以填补的空白">2.1.3 战略性开源（Strategic Open-Source）：你是否看到了可以填补的空白？</h4> <p>经验丰富的AI实验室发布新的开源模型，最常见的原因之一是：<strong>他们识别出了开源生态系统中的一个特定空白或一个新的AI应用场景。</strong></p> <p>这种模式通常是这样的：你注意到一个<strong>未被充分探索的领域</strong>。也许现在缺少一个强大且具备超长上下文能力的<strong>端侧（on-device）模型</strong>；或者现有的多语言模型在<strong>低资源语言上表现不佳</strong>；又或者，行业正在转向<strong>交互式世界模型</strong>，但还没有好的开源替代品出现。</p> <p>你有理由相信自己可以做得更好。也许你整理了<strong>更优质的训练数据</strong>，开发了<strong>更出色的训练“配方”</strong>，或者拥有其他机构无法企及的<strong>算力优势来支持“过度训练”（Overtrain）</strong>。你的目标是具体的：不是“史上最强模型”，而是“<strong>最适合端侧使用的3B模型</strong>”，或“<strong>首个具备1M上下文的小模型</strong>”。</p> <p>这是一个真实且有价值的目标。成功会创造巨大的价值：开发者会采用你的模型，它会成为他人的基础设施，或者为你建立技术信誉。但<strong>成功需要经验</strong>。在一个竞争激烈的领域，你需要清楚地知道什么是可行的，以及如何可靠地执行。</p> <p>为了让这个思路更具体，让我们看看Hugging Face是如何思考这个问题的。</p> <h4 id="214-hugging-face的思考我们为什么要训练开源模型">2.1.4 Hugging Face的思考：我们为什么要训练开源模型？</h4> <p>Hugging Face致力于<strong>构建对开源生态系统有用的东西，并填补那些鲜有人涉足的空白。</strong></p> <p>这包括数据集、工具，当然也包括模型。我们启动的每一个LLM训练项目，都始于<strong>发现一个空白</strong>，并坚信我们能做出有意义的贡献。</p> <p>我们最初的LLM项目是在<strong>GPT-3</strong>发布后启动的。当时，似乎没有人愿意构建一个开放的替代品，我们担心相关知识最终会被少数几个工业实验室垄断。因此，我们发起了<strong>BigScience研讨会</strong>，旨在训练一个开源版本的GPT-3。最终诞生的<strong>Bloom</strong>模型，汇集了数十位贡献者一年的努力，从构建训练堆栈、分词器到预训练语料库，最终预训练出一个<strong>175B参数的模型</strong>。</p> <p>Bloom的继任者是2022年的<strong>StarCoder</strong>。当时OpenAI为GitHub Copilot开发的<strong>Codex</strong>是闭源的。很明显，一个开源替代品将为整个生态系统带来巨大价值。因此，我们与ServiceNow合作，在<strong>BigCode</strong>框架下，构建了<strong>The Stack</strong>数据集，并训练了<strong>StarCoder 15B</strong>来复现Codex的能力。<strong>StarCoder2</strong>的诞生，则源于我们认识到可以进行更长时间的训练，并意识到<strong>更小但训练更久（overtrained）的模型可能比一个巨型模型更有价值</strong>。我们训练了一个模型家族（3B/7B/15B），使用了数万亿的Token，远超当时任何开放代码模型的训练量。</p> <p><strong>SmolLM系列</strong>也遵循了类似的模式。我们注意到当时<strong>强大的小型模型非常稀缺</strong>，而我们刚刚构建了<strong>FineWeb-Edu</strong>这个强大的预训练数据集。<strong>SmolLM (135M/360M/1.7B)</strong>是我们的第一个版本。<strong>SmolLM2</strong>则专注于更好的数据和更长的训练，在多个方面达到了SOTA性能。而<strong>SmolLM3</strong>则扩展到了<strong>3B参数</strong>，同时加入了<strong>混合推理、多语言能力和长上下文</strong>等社区在2025年高度重视的功能。</p> <p>这种模式甚至延伸到了预训练之外：我们训练了<strong>Zephyr</strong>来证明<strong>DPO</strong>可以大规模应用；启动了<strong>Open-R1</strong>来复现DeepSeek R1的蒸馏管线；并发布了用于<strong>竞技编程</strong>的<strong>OlympicCoder</strong>。我们还探索了其他模态，例如用于视觉的<strong>SmolVLM</strong>和用于机器人技术的<strong>SmolVLA</strong>。</p> <p>希望这一部分已经说服你：<strong>深入思考你为什么要训练一个模型是极具价值的。</strong></p> <p>在本文接下来的部分，我们将假设你已经完成了这场“灵魂拷问”，并且有了一个<strong>充分且合理的理由</strong>来启动你的训练项目。</p> <h3 id="22-what将目标转化为实际决策">2.2 What：将目标转化为实际决策</h3> <p>既然你已经明确了<strong>为什么</strong>要训练，接下来就该确定<strong>应该训练什么</strong>了。</p> <p>这里的“做什么”（What）指的是：<strong>模型类型</strong>（密集型、MoE、混合型，还是全新类型）、<strong>模型规模</strong>、<strong>架构细节</strong>和<strong>数据混合配比</strong>。</p> <p>一旦确定了“Why”，你就可以顺理成章地推导出“What”。举例来说：</p> <table> <thead> <tr> <th style="text-align: left">训练目的 (Why)</th> <th style="text-align: left">$\rightarrow$</th> <th style="text-align: left">模型规格 (What)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>追求可在端侧运行的快速模型</strong></td> <td style="text-align: left">$\rightarrow$</td> <td style="text-align: left"><strong>小巧且高效的模型</strong></td> </tr> <tr> <td style="text-align: left"><strong>构建强大的多语言模型</strong></td> <td style="text-align: left">$\rightarrow$</td> <td style="text-align: left"><strong>更大的分词器词汇表</strong></td> </tr> <tr> <td style="text-align: left"><strong>需要超长上下文能力</strong></td> <td style="text-align: left">$\rightarrow$</td> <td style="text-align: left"><strong>混合型架构</strong></td> </tr> </tbody> </table> <p>除了受用例驱动的决策外，还有一些选择旨在优化训练本身，例如让训练<strong>更稳定、更具样本效率或速度更快</strong>。这些决策并非总是非黑即白，但你可以大致将决策过程分为两个阶段：</p> <p><strong>规划阶段（Planning）：</strong> 在实验之前，你需要将你的用例映射到具体的组件上：你的<strong>部署环境</strong>决定了<strong>模型规模</strong>的限制；你的<strong>时间表</strong>决定了你可以承担<strong>哪些架构风险</strong>；你的<strong>目标能力</strong>决定了<strong>数据集</strong>的要求。这个阶段的核心就是将“Why”中的每一个约束，都与“What”中的具体规格<strong>紧密相连</strong>。</p> <p><strong>验证阶段（Validation）：</strong> 一旦你有了一个起点和一份潜在修改清单，就要进行<strong>系统性的测试</strong>。由于测试成本高昂，你应该将精力集中在那些能<strong>有意义地提升用例性能</strong>或<strong>显著优化训练过程</strong>的改动上。这就是<strong>消融实验（Ablations）</strong>发挥作用的地方，我们将在后续章节详细介绍。</p> <p>在接下来的章节中，你将了解到定义模型的所有选项，以及如何通过系统性实验来缩小选择范围。但在那之前，我们想分享一些关于<strong>如何组织团队和项目</strong>的经验——这些经验来自于我们自己训练模型的实践，以及观察那些成功构建优秀LLM团队的做法。</p> <h3 id="23-how迭代速度与数据质量">2.3 How：迭代速度与数据质量</h3> <p>通往成功的道路不止一条，但我们发现，成功的LLM训练团队之所以能脱颖而出，其关键在于<strong>迭代速度</strong>。</p> <p>训练LLM本质上是一种<strong>“边做边学”</strong>的学科，你训练的次数越多，团队就会变得越优秀。因此，那些一年只训练一个模型的团队，和那些一个季度就能训练一个模型的团队相比，<strong>后者的进步速度会快得多</strong>。Qwen和DeepSeek等团队如今已是家喻户晓，他们正是凭借长期以来持续快速地发布新模型而奠定地位的。</p> <p>除了迭代速度，<strong>数据策划（Data Curation）</strong>是迄今为止对LLM训练<strong>最具影响力</strong>的方面。人们天然倾向于钻研架构来改进模型，但那些在大模型训练中表现优异的团队，无一不是<strong>对高质量数据痴迷到无以复加</strong>的团队。</p> <p>另一个与迭代速度紧密相关的因素是<strong>团队规模</strong>：对于主要的预训练任务，你只需要<strong>少数几个人</strong>，并为他们配备足够的算力。例如，如今要预训练一个像Llama 3这样的模型，可能只需要<strong>2到3个人</strong>。只有当你开始涉足更多元化的任务（如多模态、多语言、后训练等）时，才需要逐步增加人手。</p> <p>因此，秘诀就是：<strong>从一个小型、装备精良的团队开始，每隔两到三个月就构建一个新模型</strong>，你将在短时间内攀升至行业顶端。</p> <p>好了，接下来的文章将专注于这个团队的<strong>日常技术细节</strong>！</p> <h2 id="3-每个大模型的诞生都始于一场小小的消融实验">3. 每个大模型的诞生，都始于一场小小的“消融实验”</h2> <p>在开始训练一个大型语言模型（LLM）之前，我们需要做出无数决策，这些决策将直接影响模型的性能和训练效率。我们该选择什么样的架构？使用哪种优化器和学习率？如何混合不同的数据源？</p> <p>人们常常好奇这些决策是如何做出的，有时会期望它们源于<strong>深刻的理论推演</strong>。虽然战略性思维至关重要——它能帮你识别哪些改动值得测试——但<strong>仅仅依靠推理是远远不够的</strong>。在LLM领域，直觉并不可靠，那些“理论上应该有效”的假设在实践中往往会落空。</p> <p>举个例子，使用看起来<strong>“质量最高的数据”</strong>并不一定能产出更强的模型。以<strong>arXiv</strong>为例，它是人类科学知识的巨大宝库。直觉上，用如此丰富的STEM数据进行训练应该能产出更优秀的模型，对吗？但实际上，它<strong>并非总是如此</strong>，特别是对于小模型，<strong>甚至可能损害性能</strong>。原因是虽然arXiv论文知识丰富，但它们<strong>高度专业化</strong>，并以一种狭隘的学术风格撰写，这与模型最擅长学习的<strong>多样化、通用性文本</strong>截然不同。</p> <p>那么，如果冥思苦想没有帮助，我们如何知道什么才是有效的呢？<strong>答案是：像优秀的经验主义者一样，运行大量的实验！</strong> 机器学习更像是一门<strong>实验科学</strong>。由于这些实验将指导我们许多关键决策，因此正确地设置它们至关重要。我们希望从实验中获得两个主要属性：</p> <ol> <li> <strong>速度（Speed）：</strong> 实验应该尽可能快地运行，以便我们能够频繁迭代。</li> <li> <strong>可靠性（Reliability）：</strong> 实验应提供强大的<strong>判别力（discriminative power）</strong>。如果关注的指标无法在早期有意义地区分不同设置的优劣，那么实验提供的洞察就会很少。</li> </ol> <p>但在设置消融实验之前，我们需要对<strong>架构类型</strong>和<strong>模型规模</strong>做出一些基础性的选择。这些决策会影响我们使用哪种训练框架、如何分配算力预算，以及从哪个基线开始。</p> <p>对于SmolLM，我们选择了<strong>30亿参数的密集型（dense）Llama风格架构</strong>，因为我们的目标是小型设备端模型。但正如你将在后续章节中看到的，<strong>MoE或混合模型</strong>可能更适合你的用例。现在，让我们从最实际的第一步开始：<strong>选择你的基线（Baseline）</strong>。</p> <h3 id="31-选择你的基线baseline">3.1 选择你的基线（Baseline）</h3> <p>每一个成功的模型都建立在一个<strong>经过验证的基础</strong>之上，然后根据自身需求进行修改。</p> <ul> <li>当Qwen训练他们的第一个模型家族时，他们以<strong>Llama的架构</strong>为起点。</li> <li>当Meta训练Llama 3时，他们从<strong>Llama 2</strong>开始。</li> <li>Kimi K2则始于<strong>DeepSeek-V3的MoE架构</strong>。</li> </ul> <p>这种“继承”不仅适用于架构，也适用于训练超参数和优化器。</p> <p><strong>为什么呢？</strong> 优秀的架构和训练设置是多年迭代和众多组织智慧的结晶。标准的Transformer结构和Adam等优化器，都是经过<strong>数千次实验</strong>才被完善的。人们已经发现了它们的失败模式，调试了不稳定性，并优化了实现。</p> <p><strong>从一个经过验证的基础开始，意味着你继承了所有这些积累的知识。</strong> 而从零开始，则意味着你需要自己重新发现每一个问题。</p> <p>一个好的架构起点应该具备以下条件：</p> <ol> <li> <strong>符合你的约束条件：</strong> 契合你的部署目标和实际用例。</li> <li> <strong>经过大规模验证：</strong> 在相似或更大的规模上，跑过<strong>数万亿（multi-trillion）Token</strong>的训练。</li> <li> <strong>文档完善：</strong> 有明确的、被证明有效的超参数配置。</li> <li> <strong>框架支持良好：</strong> 理想情况下，它应该被你计划使用的<strong>训练框架</strong>和<strong>推理框架</strong>所支持。</li> </ol> <p>下面列出了一份非详尽的清单，展示了2025年针对不同架构类型和模型规模的一些强大基线选项：</p> <table> <thead> <tr> <th style="text-align: left">架构类型</th> <th style="text-align: left">模型家族</th> <th style="text-align: left">常见规模 (Sizes)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>密集型（Dense）</strong></td> <td style="text-align: left">Llama 3.1, Llama 3.2, Qwen3, Gemma3, SmolLM2, SmolLM3</td> <td style="text-align: left">0.6B - 70B</td> </tr> <tr> <td style="text-align: left"><strong>MoE (专家混合)</strong></td> <td style="text-align: left">Qwen3 MoE, GPT-OSS, Kimi Moonlight, Kimi-k2, DeepSeek V3</td> <td style="text-align: left">16B - 1T</td> </tr> <tr> <td style="text-align: left"><strong>混合型（Hybrid）</strong></td> <td style="text-align: left">Zamba2, Falcon-H1</td> <td style="text-align: left">0.5B - 34B</td> </tr> <tr> <td style="text-align: left"><strong>MoE + 混合型</strong></td> <td style="text-align: left">Qwen3-Next, MiniMax-01</td> <td style="text-align: left">80B - 456B</td> </tr> </tbody> </table> <p>请找到你的架构类型，并选择一个参数量接近你目标模型的基线。<strong>不必过度纠结</strong>，因为你最初选择的架构并非一成不变。</p> <h4 id="311-修改你的基线去风险化的原则">3.1.1 修改你的基线：<strong>“去风险化”</strong>的原则</h4> <p>现在你拥有了一个有效的基线模型。你大可以停在这里，用你的数据进行训练，很可能会得到一个不错的模型。然而，基线模型并非为你的<strong>特定需求</strong>而优化。因此，你很可能需要进行一些修改。但请注意，<strong>每一次架构上的更改都伴随着风险</strong>：它可能提升性能、彻底搞砸，或者毫无作用。</p> <p>让你保持在正轨上的纪律是<strong>“去风险化”（Derisking）</strong>：<strong>除非你已经测试并确认它有帮助，否则绝不更改任何东西。</strong></p> <p>棘手之处在于，可修改的组件太多了：<strong>注意力机制、位置编码、激活函数、优化器、归一化方案</strong>等等。每一个都代表着一个潜在的实验，而这些组件往往以<strong>非线性</strong>的方式相互作用。你没有时间或算力来测试所有组合。</p> <p>正确的做法是：<strong>从测试有前景的改动开始，并以当前基线为参照。</strong> 当某个改动有效时，就将其集成进来，创建一个<strong>新的基线</strong>，然后针对这个新基线测试下一个改动。</p> <p><strong>千万不要掉入陷阱：</strong> 避免对每一个超参数进行详尽的<strong>网格搜索（Grid Searches）</strong>，也避免测试每一个新出现的架构变体。</p> <p>现在你知道了如何通过战略规划来确定哪些改动是有前景的，接下来就该进入<strong>经验验证</strong>了。</p> <h3 id="32-挑选训练框架">3.2 挑选训练框架</h3> <p>我们需要做的第一个决策是：选择哪个框架来训练模型。这个选择需要平衡以下三个关键考量点：</p> <ol> <li> <strong>架构兼容性：</strong> 框架必须支持我们的目标架构，或能让我们轻松地进行扩展。</li> <li> <strong>稳定性和生产就绪：</strong> 框架需要稳定、成熟，不会在训练中途神秘地崩溃。</li> <li> <strong>高吞吐量：</strong> 它应该能提供强大的吞吐量，以便我们快速迭代。</li> </ol> <p>在实践中，这些要求可能相互掣肘。让我们来看看几个流行的选项：</p> <table> <thead> <tr> <th style="text-align: left">框架</th> <th style="text-align: left">特性覆盖</th> <th style="text-align: left">实战检验</th> <th style="text-align: left">优化程度</th> <th style="text-align: left">核心/总代码行数</th> <th style="text-align: left">扩展性与调试难度</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Megatron-LM</strong></td> <td style="text-align: left">✅ 全面</td> <td style="text-align: left">✅ Kimi-K2, Nemotron</td> <td style="text-align: left">✅ 3D并行先驱</td> <td style="text-align: left">93k / 269k</td> <td style="text-align: left">⚠️ 难度大，不适合新手</td> </tr> <tr> <td style="text-align: left"><strong>DeepSpeed</strong></td> <td style="text-align: left">✅ 全面</td> <td style="text-align: left">✅ BLOOM, GLM</td> <td style="text-align: left">✅ ZeRO &amp; 3D并行先驱</td> <td style="text-align: left">94k / 194k</td> <td style="text-align: left">⚠️ 难度大，不适合新手</td> </tr> <tr> <td style="text-align: left"><strong>TorchTitan</strong></td> <td style="text-align: left">⚡ 持续增长</td> <td style="text-align: left">⚠️ 较新</td> <td style="text-align: left">⚡ 针对密集模型优化</td> <td style="text-align: left">7k / 9k</td> <td style="text-align: left">⚡ 适中</td> </tr> <tr> <td style="text-align: left"><strong>Nanotron</strong></td> <td style="text-align: left">🎯 为HF预训练定制</td> <td style="text-align: left">✅ StarCoder, SmolLM</td> <td style="text-align: left">✅ 高度优化</td> <td style="text-align: left">15k / 66k</td> <td style="text-align: left">⚡ 适中</td> </tr> </tbody> </table> <ul> <li> <strong>Megatron-LM &amp; DeepSpeed:</strong> 功能强大，久经沙场，但代码库庞大复杂，对新手不友好。</li> <li> <strong>TorchTitan:</strong> PyTorch官方出品，轻量且易于上手，非常适合快速实验，但相对较新，稳定性可能略逊一筹。</li> <li> <strong>Nanotron:</strong> Hugging Face自研框架，为大规模预训练深度优化，灵活性高，但某些功能（如MoE）仍在构建中。</li> </ul> <p>对于我们而言，从头构建Nanotron是合理的，但这需要巨大的投入。一个强大的替代方案是<strong>分叉（fork）</strong>一个现有框架并根据需求进行增强。</p> <p>最终，你的选择取决于<strong>团队的专业知识、目标功能，以及你愿意投入多少时间进行二次开发。</strong> 如果多个框架都能满足需求，请在你的硬件上<strong>比较它们的吞吐量</strong>。对于快速实验，<strong>更简洁的代码库通常会胜出</strong>。</p> <h3 id="33-消融实验设置">3.3 消融实验设置</h3> <p>既然训练框架已定，我们现在就需要设计我们的消融实验。实验需要足够快以便快速迭代，但又需要足够大，以确保结果能<strong>外推（extrapolate）</strong>到最终模型。</p> <h4 id="331-搭建消融实验框架">3.3.1 搭建消融实验框架</h4> <p>主要有两种方法：</p> <ol> <li> <strong>方法一（减少Token）：</strong> 保持目标模型大小不变，但在<strong>更少的Token</strong>上进行训练。例如，在SmolLM的消融实验中，我们用完整的<strong>3B参数模型</strong>，但在<strong>100B Token</strong>上训练，而不是最终的11T Token。</li> <li> <strong>方法二（减小模型）：</strong> 如果目标模型太大，我们可以训练一个<strong>更小的代理模型</strong>。例如，Kimi在开发其<strong>1T参数的K2模型</strong>时，就使用了一个<strong>3B参数的MoE模型</strong>来运行部分消融实验。</li> </ol> <p>一个关键问题是：这些小规模的发现真的能迁移吗？根据我们的经验，如果某个改动在<strong>小规模上损害了性能，你可以放心地将其排除</strong>。但如果某个改动在小规模上有效，你仍然需要确保在<strong>合理的Token数量</strong>上进行了训练，才能高概率地得出结论。<strong>训练时间越长，结果越可靠。</strong></p> <p>在本文中，我们将使用一个<strong>1B参数的Llama 3.2风格Transformer</strong>，在<strong>45B Token</strong>上进行所有消融实验。这在一个配备<strong>8块H100</strong>的节点上大约需要<strong>1.5天</strong>。</p> <p>我们的基线配置以YAML格式记录了所有重要的训练细节，包括数据集、模型架构、优化器和并行化策略。在实验中，我们会根据测试内容修改不同的部分，同时保持其他一切不变。</p> <h4 id="332-理解效果评估至关重要">3.3.2 理解效果：评估至关重要</h4> <p>我们如何判断哪些改动有效？第一直觉是看<strong>Loss曲线</strong>。它确实很重要，你希望看到它平滑下降。</p> <p>然而，<strong>仅看Loss并不可靠</strong>。例如，用维基百科训练比用网页训练得到的<strong>Loss更低</strong>，但这<strong>不意味着模型更有能力</strong>。如果更改了分词器，Loss也无法直接比较。因此，我们需要更细致的<strong>下游评估（downstream evaluations）</strong>来测试知识、推理等能力。</p> <p>对于消融实验，最好关注那些<strong>能提供良好早期信号</strong>并<strong>避免嘈杂基准</strong>的任务。我们主要关注<strong>多项选择格式 (MCF)</strong> 或<strong>完形填空表述 (CF)</strong>。研究表明，<strong>CF更适合获取早期信号</strong>。</p> <p>我们的消融评估套件包括MMLU、ARC、HellaSwag等一系列基准，覆盖了<strong>世界知识、推理和常识</strong>。</p> <h5 id="3321-消融实验使用什么数据混合配比">3.3.2.1 消融实验使用什么数据混合配比？</h5> <p>对于<strong>架构消融实验</strong>，我们使用<strong>固定的高质量数据集混合</strong>（英语、数学、代码）。对于<strong>数据消融实验</strong>，我们<strong>固定架构</strong>，并<strong>系统地改变数据混合配比</strong>。</p> <h4 id="333-估算消融实验成本">3.3.3 估算消融实验成本</h4> <p>消融实验需要GPU时间，理解其成本至关重要。下表显示了SmolLM预训练的算力分解：</p> <table> <thead> <tr> <th style="text-align: left">阶段</th> <th style="text-align: left">GPU 数量</th> <th style="text-align: left">天数</th> <th style="text-align: left">GPU-小时</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">主预训练运行</td> <td style="text-align: left">384</td> <td style="text-align: left">30</td> <td style="text-align: left">276,480</td> </tr> <tr> <td style="text-align: left">消融实验（预训练前）</td> <td style="text-align: left">192</td> <td style="text-align: left">15</td> <td style="text-align: left">69,120</td> </tr> <tr> <td style="text-align: left">消融实验（训练中）</td> <td style="text-align: left">192</td> <td style="text-align: left">10</td> <td style="text-align: left">46,080</td> </tr> <tr> <td style="text-align: left">训练重启与调试</td> <td style="text-align: left">384/192</td> <td style="text-align: left">3/4</td> <td style="text-align: left">46,080</td> </tr> <tr> <td style="text-align: left"><strong>总成本</strong></td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> <td style="text-align: left"><strong>437,760</strong></td> </tr> </tbody> </table> <p>这些数字揭示了一个重要事实：<strong>消融实验和调试总共消耗了超过主训练运行一半的成本。</strong> 这突出表明了为什么<strong>消融实验的成本必须计入你的算力预算</strong>。</p> <h3 id="34-实验守则">3.4 实验守则</h3> <p><strong>验证你的评估套件。</strong> 在训练任何模型之前，确保你的评估套件能够<strong>复现</strong>已知模型的结果。</p> <p><strong>测试每一个更改，无论多小。</strong> 一个看似无害的库升级或代码修改都可能引入<strong>微妙的Bug</strong>。</p> <p><strong>一次只改变一件事。</strong> 首先评估<strong>每个变化的单独贡献</strong>，然后再尝试将它们结合起来。</p> <p><strong>训练足够的Token，并使用充分的评估。</strong> 在这里“偷工减料”将导致嘈杂的结果和错误的决策。</p> <p>遵守这些规则可能感觉<strong>过于谨慎</strong>，但它能让你避免花费数天时间调试<strong>神秘的性能下降</strong>。<strong>黄金原则：一旦你拥有一个良好的设置，任何更改都应该经过测试！</strong></p> <h2 id="4-设计模型架构">4. 设计模型架构</h2> <p>既然我们已经有了实验框架，是时候做出定义我们模型的<strong>重大决策</strong>了。从模型大小到注意力机制再到分词器，每一个选择都会直接影响模型的训练和最终使用。</p> <p>请记住<strong>“训练指南针”</strong>：在做出任何技术选择之前，我们都需要对<strong>“为什么”（Why）和“做什么”（What）</strong>有清晰的认识。我们的目标是构建一个<strong>英语SOTA模型</strong>吗？<strong>长上下文</strong>是我们的首要任务吗？还是我们试图验证一种<strong>新的架构</strong>？</p> <p>对于SmolLM3，我们的目标是构建一个<strong>强大的端侧应用模型</strong>，同时具备<strong>有竞争力的多语言性能、可靠的数学和编码能力，以及强大的长上下文处理能力。</strong> 这引导我们选择了<strong>一个30亿参数的密集型模型</strong>：它足够大以提供强大的能力，但又足够小以舒适地安装在手机上。</p> <p>我们从SmolLM2获得了一个小规模（1.7B）的英语训练“配方”，但扩大规模意味着<strong>重新验证一切</strong>，并解决<strong>多语言和扩展上下文长度</strong>等新挑战。例如，在SmolLM2中，我们在预训练结束时才努力扩展上下文长度，因此对于SmolLM3，我们从一开始就做出了架构选择——比如使用<strong>NoPE</strong>和<strong>文档内掩码</strong>——以最大化成功的几率。</p> <p>一旦目标明确，我们就可以开始做出技术决策。在本章中，我们将系统地探讨这些核心决策：<strong>架构、数据和超参数</strong>。</p> <h3 id="41-架构选择">4.1 架构选择</h3> <p>如果你观察最近的模型，比如Qwen3、Gemma3或DeepSeek v3，你会发现尽管它们存在差异，但都共享着同一个基础——<strong>2017年引入的Transformer架构</strong>。多年来发生变化的不是基本结构，而是对其核心组件的<strong>精炼和改进</strong>。无论你是构建<strong>密集型模型（Dense Model）</strong>、<strong>专家混合模型（Mixture of Experts, MoE）</strong>还是<strong>混合架构（Hybrid Architecture）</strong>，你都在使用这些相同的构建块。</p> <p>这些改进源于各大团队对更优性能的追求，以及对特定挑战的攻克：<strong>推理时的内存限制、大规模训练时的不稳定性</strong>，或<strong>处理更长上下文的需求</strong>。一些修改，比如从<strong>多头注意力（MHA）</strong>转向计算效率更高的<strong>分组查询注意力（GQA）</strong>，现已被广泛采纳。而其他修改，比如不同的<strong>位置编码方案</strong>，仍在争论之中。</p> <p>那么，现代LLM今天实际在使用什么呢？下表展示了部分前沿模型的架构细节：</p> <table> <thead> <tr> <th style="text-align: left">模型</th> <th style="text-align: left">架构</th> <th style="text-align: left">参数量</th> <th style="text-align: left">训练 Token</th> <th style="text-align: left">注意力机制</th> <th style="text-align: left">上下文长度</th> <th style="text-align: left">位置编码</th> <th style="text-align: left">精度</th> <th style="text-align: left">初始化 (Std)</th> <th style="text-align: left">优化器</th> <th style="text-align: left">最大学习率</th> <th style="text-align: left">学习率调度</th> <th style="text-align: left">Warmup 步数</th> <th style="text-align: left">Batch Size</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">DeepSeek LLM 7B</td> <td style="text-align: left">Dense</td> <td style="text-align: left">7B</td> <td style="text-align: left">2T</td> <td style="text-align: left">GQA</td> <td style="text-align: left">4K</td> <td style="text-align: left">RoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">0.006</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$4.2 \times 10^{-4}$</td> <td style="text-align: left">Multi-Step</td> <td style="text-align: left">2K</td> <td style="text-align: left">9.4M</td> </tr> <tr> <td style="text-align: left">DeepSeek V2</td> <td style="text-align: left">MoE</td> <td style="text-align: left">236B (21B)</td> <td style="text-align: left">8.1T</td> <td style="text-align: left">MLA</td> <td style="text-align: left">128K</td> <td style="text-align: left">Partial RoPE</td> <td style="text-align: left">-</td> <td style="text-align: left">0.006</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$2.4 \times 10^{-4}$</td> <td style="text-align: left">Multi-Step</td> <td style="text-align: left">2K</td> <td style="text-align: left">9.4M -&gt; 37.7M</td> </tr> <tr> <td style="text-align: left">Kimi K2</td> <td style="text-align: left">MoE</td> <td style="text-align: left">1T (32B)</td> <td style="text-align: left">15.5T</td> <td style="text-align: left">MLA</td> <td style="text-align: left">128K</td> <td style="text-align: left">Partial RoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">可能是 0.006</td> <td style="text-align: left">MuonClip</td> <td style="text-align: left">$2 \times 10^{-4}$</td> <td style="text-align: left">WSD</td> <td style="text-align: left">500</td> <td style="text-align: left">67M</td> </tr> <tr> <td style="text-align: left">OLMo 2 7B</td> <td style="text-align: left">Dense</td> <td style="text-align: left">7B</td> <td style="text-align: left">5T</td> <td style="text-align: left">MHA</td> <td style="text-align: left">4K</td> <td style="text-align: left">RoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">0.02</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$3 \times 10^{-4}$</td> <td style="text-align: left">Cosine</td> <td style="text-align: left">2K</td> <td style="text-align: left">4.2M</td> </tr> <tr> <td style="text-align: left">SmolLM3</td> <td style="text-align: left">Dense</td> <td style="text-align: left">3B</td> <td style="text-align: left">11T</td> <td style="text-align: left">GQA</td> <td style="text-align: left">128K</td> <td style="text-align: left">NoPE</td> <td style="text-align: left">BF16</td> <td style="text-align: left">0.02</td> <td style="text-align: left">AdamW</td> <td style="text-align: left">$2 \times 10^{-4}$</td> <td style="text-align: left">WSD</td> <td style="text-align: left">2K</td> <td style="text-align: left">2.3M</td> </tr> </tbody> </table> <p>如果你还不理解其中的一些术语，比如<strong>MLA、NoPE</strong>或<strong>WSD</strong>，请不用担心。我们将在本节中逐一解释。现在，你只需要注意其中的多样性：<strong>不同的注意力机制</strong>（MHA, GQA, MLA）、<strong>位置编码</strong>（RoPE, NoPE, partial RoPE）以及<strong>学习率调度</strong>（Cosine, Multi-Step, WSD）。</p> <p>我们将首先关注<strong>最简单的密集型模型</strong>，然后深入探讨<strong>MoE和混合模型</strong>，最后探索<strong>分词器（Tokenizer）</strong>。</p> <h4 id="411-注意力机制attention">4.1.1 注意力机制（Attention）</h4> <p>注意力机制在<strong>推理时</strong>是主要的瓶颈，尤其是在长上下文场景中。它的计算成本高，并且<strong>KV缓存（KV cache）</strong>会迅速消耗GPU内存。</p> <h5 id="4111-我的注意力需要多少个头">4.1.1.1 我的注意力需要多少个头？</h5> <p><strong>多头注意力（Multi-Head Attention, MHA）</strong>是标准机制。在推理时，我们可以重用过去Token的KV值，存储这些值的内存被称为<strong>KV-Cache</strong>。随着上下文窗口的增长，这个缓存会迅速成为瓶颈。例如，对于Llama3 70B模型，在8192的序列长度下，KV缓存大约需要20GB内存。</p> <p>一个自然的问题是：我们真的需要为<strong>每个头</strong>都计算新的KV值吗？</p> <ul> <li> <strong>多查询注意力（Multi-Query Attention, MQA）</strong>在所有头之间共享KV值，极大地减小了KV缓存的大小。</li> <li> <strong>分组查询注意力（Grouped Query Attention, GQA）</strong>则是一个折中方案，在分组的头之间共享KV值，在效率和性能之间取得了平衡。</li> <li>最近，<strong>多潜变量注意力（Multi-Latent Attention, MLA）</strong>被提出，它不减少KV值的数量，而是减少它们的维度，通过存储一个可在运行时解压的<strong>潜变量</strong>来压缩缓存。</li> </ul> <p>下表比较了这些注意力机制的KV缓存大小：</p> <table> <thead> <tr> <th style="text-align: left">注意力机制</th> <th style="text-align: left">每个Token的KV-Cache参数量</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>MHA</strong></td> <td style="text-align: left">$2 \times n_{heads} \times n_{layers} \times dim_{head}$</td> </tr> <tr> <td style="text-align: left"><strong>MQA</strong></td> <td style="text-align: left">$2 \times 1 \times n_{layers} \times dim_{head}$</td> </tr> <tr> <td style="text-align: left"><strong>GQA</strong></td> <td style="text-align: left">$2 \times g \times n_{layers} \times dim_{head}$ (g为组数)</td> </tr> <tr> <td style="text-align: left"><strong>MLA</strong></td> <td style="text-align: left">$4.5 \times n_{layers} \times dim_{head}$</td> </tr> </tbody> </table> <h5 id="4112-消融实验---gqa胜过mha">4.1.1.2 消融实验 - GQA胜过MHA</h5> <p>我们比较了MHA、MQA和不同分组比率的GQA。为了保持参数量大致相同，我们调整了部分配置的层数。</p> <p>实验结果显示，<strong>MQA和高分组比率的GQA性能显著低于MHA</strong>。而<strong>分组比率较低的GQA（如2、4、8）性能与MHA大致持平</strong>。这个结论在Loss曲线和下游评估中都保持一致。</p> <p>基于这些实验，<strong>GQA是MHA的一个可靠替代品</strong>。它在保持性能的同时，在推理时更加高效。<strong>对于SmolLM3，我们最终使用了分组比率为4的GQA。</strong></p> <h5 id="4113-文档掩码document-masking">4.1.1.3 文档掩码（Document Masking）</h5> <p>在预训练期间，我们通常将多个可变长度的文档<strong>打包（Packing）</strong>成固定长度的序列。在标准的<strong>因果掩码（Causal Masking）</strong>下，一个Token可以关注打包序列中的所有先前Token，即使它们来自不相关的文档。这意味着<strong>绝大多数Token将浪费算力去关注不相关的内容</strong>，并可能引入噪音，降低性能。</p> <p><strong>文档内掩码（intra-document masking）</strong>解决了这个问题：我们修改注意力掩码，使得Token<strong>只能关注同一文档内的先前Token</strong>。Llama 3等模型也使用了这种技术，发现在长上下文扩展时益处显著。</p> <p>我们的消融实验显示，与标准因果掩码相比，文档内掩码在短上下文任务上的Loss和评估得分<strong>几乎完全相同</strong>。然而，考虑到它在扩展到长序列时能<strong>加快训练速度</strong>，我们在SmolLM3的整个训练过程中都采用了它。</p> <h4 id="412-嵌入层共享embedding-sharing">4.1.2 嵌入层共享（Embedding Sharing）</h4> <p>LLM有两个嵌入组件：<strong>输入嵌入</strong>和<strong>输出嵌入</strong>。在小型模型中，这两部分可以占据总参数量的很大一部分。<strong>嵌入层共享</strong>（在输出层重用输入嵌入）成为一种自然的优化。</p> <h5 id="4121-消融实验---绑定嵌入可媲美更大参数量的非绑定变体">4.1.2.1 消融实验 - 绑定嵌入可媲美更大参数量的非绑定变体</h5> <p>我们比较了三种模型：</p> <ol> <li> <strong>基线模型 (1.2B):</strong> 绑定嵌入（16层）。</li> <li> <strong>非绑定-减少层数 (1.2B):</strong> 非绑定嵌入，但层数更少（12层）以保持参数预算。</li> <li> <strong>非绑定-相同层数 (1.46B):</strong> 非绑定嵌入，层数与基线相同（16层）。</li> </ol> <p>结果表明，我们的<strong>1.2B绑定嵌入基线模型</strong>，在参数少18%的情况下，实现了与<strong>1.46B非绑定模型</strong>相当的性能。而参数量相同但层数更少的<strong>1.2B非绑定模型性能最差</strong>。这表明，在参数预算相等的情况下，<strong>增加模型深度比解绑嵌入层更有益</strong>。</p> <p>基于这些结果，我们为<strong>SmolLM3 3B模型保留了绑定嵌入</strong>。</p> <h4 id="413-位置编码与长上下文">4.1.3 位置编码与长上下文</h4> <p>Transformer天生对词序不敏感，需要<strong>位置嵌入（Positional Embeddings）</strong>来赋予序列中每个Token一个独特的“地址”。随着上下文窗口从几百扩展到百万级，位置编码的选择变得至关重要。</p> <h5 id="4131-rope将位置编码为旋转">4.1.3.1 RoPE：将位置编码为旋转</h5> <p>主导近期大模型的技术是<strong>旋转位置嵌入（Rotary Position Embedding, RoPE）</strong>。其核心思想是将位置信息编码为<strong>高维空间中的旋转角度</strong>。通过依赖于其绝对位置的角度来旋转查询（Query）和键（Key）向量，使得注意力模式<strong>仅取决于Token间的相对距离</strong>，从而学习到基于距离的模式，并可以外推到更长的序列。</p> <h5 id="4132-如何处理长上下文">4.1.3.2 如何处理长上下文？</h5> <p>典型的做法是用较短的序列完成大部分预训练，然后在最后阶段使用更长的序列。然而，随着序列长度增长，RoPE的旋转角度可能导致远处Token的注意力得分衰减过快。解决方案是<strong>增加基础频率</strong>，使用诸如<strong>ABF</strong>和<strong>YaRN</strong>之类的方法来减缓衰减。</p> <h5 id="4133-混合位置编码方法">4.1.3.3 混合位置编码方法</h5> <p>最近的研究挑战了显式位置编码的必要性。</p> <ul> <li> <strong>NoPE (No Position Embedding):</strong> 在<strong>没有任何显式位置编码</strong>的情况下训练Transformer，允许模型通过因果掩码隐式学习位置信息。它表现出更好的长度泛化能力，但可能在短上下文任务上表现较弱。</li> <li> <strong>RNoPE (混合方法):</strong> 结合RoPE和NoPE，在模型中交替使用RoPE层和NoPE层，试图兼得两者之长。Llama 4、Command A和SmolLM3都采用了这种技术。</li> </ul> <h5 id="4134-消融实验---nope在短上下文上与rope匹配">4.1.3.4 消融实验 - NoPE在短上下文上与RoPE匹配</h5> <p>我们比较了纯RoPE基线、一个每隔4层移除位置编码的NoPE变体，以及结合NoPE和文档掩码的配置。结果显示，所有三种配置的性能<strong>相似</strong>。这表明NoPE在<strong>保持强大的短上下文能力</strong>的同时，为更好的长上下文处理提供了基础。因此，我们为<strong>SmolLM3采用了NoPE + 文档掩码的组合</strong>。</p> <h5 id="4135-限制注意力范围">4.1.3.5 限制注意力范围</h5> <p>另一种互补的策略是<strong>限制哪些Token相互关注</strong>，以降低计算成本和内存需求。</p> <ul> <li> <strong>分块注意力（Chunked Attention）:</strong> 将序列分成块，Token只能在自己的块内关注。</li> <li> <strong>滑动窗口注意力（Sliding Window Attention, SWA）:</strong> 每个Token只关注最近的N个Token。</li> <li> <strong>双块注意力（Dual Chunk Attention, DCA）:</strong> 一种免训练的方法，结合了块内、块间和连续块注意力，以保持跨块的信息流。</li> </ul> <h5 id="4136-注意力汇聚attention-sinks">4.1.3.6 注意力汇聚（Attention Sinks）</h5> <p>研究发现，模型会为序列中的<strong>起始Token分配异常高的注意力分数</strong>，这些Token充当了注意力的“汇聚点”。利用这一现象，仅保留起始几个Token和最近Token的滑动窗口的KV缓存，就可以在很大程度上<strong>恢复长上下文性能</strong>。这可以通过添加特殊Token或学习到的偏差来实现。</p> <h4 id="414-提高稳定性">4.1.4 提高稳定性</h4> <p>大规模训练中，<strong>不稳定性</strong>（如Loss尖峰）是一个巨大挑战。一些架构技术可以提供帮助：</p> <ul> <li> <strong>Z-loss:</strong> 一种正则化技术，通过惩罚过大的Logits来提高数值稳定性。</li> <li> <strong>从嵌入层中移除权重衰减:</strong> OLMo的研究发现，这样做可以提高训练稳定性，因为它可以防止早期层中的梯度变得过大。</li> <li> <strong>QK-norm:</strong> 在计算注意力之前对查询和键向量进行层归一化，有助于防止注意力Logits过大。但有研究发现它可能损害长上下文任务。</li> </ul> <p>在我们的实验中，Z-loss和移除嵌入层权重衰减对性能没有负面影响，因此我们在SmolLM3中采用了后者。由于可能损害长上下文性能，我们<strong>没有使用QK-norm</strong>。</p> <h4 id="416-走向稀疏专家混合模型moe">4.1.6 走向稀疏：专家混合模型（MoE）</h4> <p>MoE的核心思想是：<strong>增加总参数量，同时不增加每个Token的“活跃”参数量</strong>。它将MLP层替换为多个“专家”，并由一个可学习的“路由器”为每个Token选择少数专家进行计算。这使得模型可以在拥有巨大容量的同时，保持较低的训练和推理成本。</p> <p>设计MoE涉及几个核心问题：</p> <ul> <li> <strong>稀疏度/激活比率:</strong> 最近的趋势是MoE模型正变得<strong>越来越稀疏</strong>（即总专家数远大于激活专家数），但这存在一个收益递减点。</li> <li> <strong>粒度:</strong> 指的是每个专家应该有多大。最近的模型倾向于<strong>更高的粒度</strong>（即更多、更小的专家）。</li> <li> <strong>共享专家:</strong> 让每个Token都通过一组始终激活的“共享专家”，可以帮助它们吸收通用模式，让其他专家更专注于特定领域。通常<strong>一个共享专家</strong>就足够了。</li> <li> <strong>负载均衡:</strong> 这是MoE的关键。如果路由器总把Token发给少数几个专家，会造成资源浪费和性能下降。通过<strong>辅助损失函数</strong>来鼓励路由器均匀分配Token是标准做法。</li> </ul> <h4 id="417-混合模型hybrid-models">4.1.7 混合模型（Hybrid Models）</h4> <p>最近的另一个趋势是使用<strong>状态空间模型（SSM）</strong>或<strong>线性注意力</strong>来增强标准Transformer，以解决其在处理极长上下文时效率低下的问题。这些混合模型试图<strong>兼具循环模型（RNN）的线性扩展能力和Transformer的强大上下文利用能力</strong>。</p> <p>它们通过重新排序计算，将注意力的复杂度从$O(N^2)$降低到$O(N)$，从而在长序列上实现显著的效率优势。Mamba、RetNet等都属于这一范畴。虽然它们显示出巨大潜力，但当扩展到大规模时，仍需考虑许多细微差别，其成熟度也不及密集型和MoE模型。</p> <h4 id="418-要不要moe如何选择基础架构">4.1.8 要不要MoE：如何选择基础架构？</h4> <p>你的选择取决于<strong>部署环境、团队专业知识和项目时间线</strong>。</p> <ol> <li> <strong>部署场景决定内存约束：</strong> <ul> <li> <strong>端侧/低显存:</strong> 内存是瓶颈，基本排除MoE。首选<strong>密集型</strong>。</li> <li> <strong>云端/算力充裕:</strong> 内存不是瓶颈，优先考虑<strong>MoE</strong>（最佳性能/计算比）或<strong>混合型</strong>（极致长上下文）。</li> </ul> </li> <li> <strong>团队经验和时间线决定技术风险：</strong> <ul> <li> <strong>时间紧迫/经验相对较少:</strong> 选择<strong>密集型</strong>，这是最稳健的路径。</li> <li> <strong>有时间探索/有资深工程师:</strong> 可以在<strong>MoE</strong>或<strong>混合型</strong>上进行探索。</li> </ul> </li> </ol> <p><strong>以SmolLM3为例：</strong> 我们的目标是<strong>端侧部署</strong>，时间线约为<strong>3个月</strong>，且过去主要训练<strong>密集型模型</strong>。因此，我们选择了<strong>Llama风格的密集型模型</strong>。</p> <h4 id="419-分词器tokenizer被低估的语言翻译官">4.1.9 分词器（Tokenizer）：被低估的“语言翻译官”</h4> <p>分词器是将原始文本转换为模型可处理的数字序列的桥梁，其质量至关重要。设计分词器需要回答几个根本问题：<strong>支持哪些语言？哪些领域很重要？目标数据混合配比是什么？</strong></p> <ul> <li> <strong>词汇表大小:</strong> 更大的词汇表能更有效地压缩文本，但会增加嵌入矩阵的大小。对于多语言模型，通常需要<strong>100k以上</strong>的词汇表。Llama 3等现代模型已采用<strong>128k+</strong>。</li> <li> <strong>分词算法:</strong> <strong>BPE (Byte-Pair Encoding)</strong>仍然是最流行的选择。</li> </ul> <h5 id="衡量分词器质量">衡量分词器质量</h5> <p>我们使用两个关键指标：</p> <ol> <li> <strong>Token消耗比（Fertility）:</strong> 编码一个词平均所需的Token数量。<strong>越低越好</strong>。</li> <li> <strong>连续词比例:</strong> 有多少百分比的词语被分割成多个片段。<strong>越低越好</strong>。</li> </ol> <p>我们对流行的分词器进行了评估，发现<strong>Gemma 3</strong>在多种语言上表现优异，这得益于其<strong>庞大的262k词汇表</strong>。<strong>Qwen 3</strong>在<strong>中文</strong>上表现出色。<strong>Llama 3</strong>则在多语言和代码上提供了<strong>良好的平均质量</strong>。</p> <p>对于<strong>SmolLM3</strong>，我们选择了<strong>Llama 3的分词器</strong>，因为它在我们的目标语言上提供了最佳的权衡，同时词汇表大小适中。</p> <h4 id="4110-smollm3的最终架构选择">4.1.10 SmolLM3的最终架构选择</h4> <p>我们以SmolLM2的1.7B架构为基础，系统性地进行了一系列消融实验，最终确定了SmolLM3的架构：</p> <ol> <li> <strong>分词器:</strong> Llama 3.2的分词器 (128k)。</li> <li> <strong>注意力:</strong> GQA，分组比率为4。</li> <li> <strong>位置编码:</strong> NoPE（每隔4层移除RoPE）+ 文档内掩码。</li> <li> <strong>模型布局:</strong> 采用了更深的Qwen 2.5-3B布局。</li> <li> <strong>稳定性:</strong> 保留了绑定嵌入，并移除了嵌入层的权重衰减。</li> </ol> <p>这种系统性的方法让我们能够自信地将所有这些修改结合起来。</p> <h2 id="5-数据策划的艺术">5. 数据策划的艺术</h2> <p>如果说模型架构定义了模型<strong>如何学习</strong>，那么数据就定义了它<strong>学什么</strong>。再好的架构也无法从糟糕的数据中拯救一个模型。搞定训练数据不仅关乎拥有好的数据集，更关乎组建<strong>正确的混合配比（Mixture）</strong>。</p> <h3 id="51-数据混合的非直观性">5.1 数据混合的非直观性</h3> <p>找到好的数据混合配比并非易事。不同领域的数据可能会<strong>相互竞争</strong>。增加代码数据的权重可能会损害模型的自然语言能力。此外，我们希望在利用<strong>高质量数据</strong>的同时，避免过度重复，因为这可能是有害的。</p> <p>为了平衡这一切，我们需要仔细设计<strong>混合配比</strong>，并在训练过程中动态调整，即<strong>多阶段训练（Multi-Stage Training）</strong>或<strong>课程学习（Curriculum）</strong>。其核心思想是：<strong>在训练早期增加更多样化的数据来源，在接近尾声时混入更小、更高质量的来源</strong>，因为模型的最终行为受到训练末期看到的数据的强烈影响。</p> <h3 id="52-smollm3的数据策划过程">5.2 SmolLM3的数据策划过程</h3> <p>对于SmolLM3，我们希望模型能处理<strong>英语和多种其他语言</strong>，并在<strong>数学和代码</strong>方面表现出色。</p> <ul> <li> <strong>英语网络数据:</strong> 我们混合了<strong>FineWeb-Edu</strong>和<strong>DCLM</strong>这两个高质量数据集，比例为50/50，构成了数据的基础层。</li> <li> <strong>多语言网络数据:</strong> 我们从<strong>FineWeb2-HQ</strong>中选择了5种目标语言，并加入了其他10种语言。消融实验发现，网络混合中<strong>12%的多语言内容</strong>达到了最佳平衡，在不降低英语性能的情况下提高了多语言能力。</li> <li> <strong>代码数据:</strong> 我们从<strong>The Stack v2</strong>和<strong>StarCoder2</strong>语料库中提取数据。实验表明，<strong>10%的代码比例</strong>是一个很好的权衡点。</li> <li> <strong>数学数据:</strong> 早期我们使用了<strong>FineMath3+</strong>等通用数据集，后期则上采样了<strong>FineMath4+</strong>并引入了<strong>MegaMath</strong>等更高质量的数据集。</li> </ul> <p>我们通过<strong>从头开始的消融实验</strong>来确定第一阶段的混合配比，并通过<strong>退火实验</strong>（从主运行的中间检查点开始，用修改后的数据继续训练）来测试新阶段的数据集。</p> <h2 id="6-训练马拉松">6. 训练马拉松</h2> <p>万事俱备，真正的乐趣即将开始。对于SmolLM3，我们在<strong>384块H100 GPU</strong>上训练了<strong>近一个月</strong>，处理了<strong>11万亿Token</strong>。本节将带你了解在一次漫长的训练运行中<strong>实际会发生什么</strong>。</p> <h3 id="61-起飞前检查清单">6.1 起飞前检查清单</h3> <p>在按下“训练”按钮前，我们会过一遍检查清单：</p> <ul> <li> <strong>基础设施:</strong> 确保Slurm预留、GPU压力测试、存储管理都已就绪。</li> <li> <strong>评估:</strong> 评估流程完全自动化，并能正确记录结果。</li> <li> <strong>检查点与恢复:</strong> 验证检查点能正确保存，并且作业可以自动恢复。</li> <li> <strong>指标记录:</strong> 确认所有关心的指标（吞吐量、Loss、硬件健康状况等）都在记录中。</li> <li> <strong>配置检查:</strong> 仔细检查所有配置文件和启动脚本。</li> </ul> <h3 id="62-规模化带来的惊喜">6.2 规模化带来的“惊喜”</h3> <p>在为SmolLM3运行了广泛的消融实验后，我们满怀信心地开始了全面规模的运行。然而，现实很快就给我们<strong>抛来了曲线球</strong>。</p> <h4 id="谜团-1--消失的吞吐量">谜团 #1 – 消失的吞吐量</h4> <p>启动后几小时内，<strong>吞吐量骤降</strong>。经过排查，我们发现问题出在<strong>数据存储</strong>上。我们最初使用的FSx存储在我们庞大的数据集压力下开始<strong>驱逐数据分片</strong>，导致训练停顿。<strong>修复方法是：将数据存储在每个节点的本地存储<code class="language-plaintext highlighter-rouge">/scratch</code>中</strong>，并预留一个<strong>带有预加载数据的备用节点</strong>，以实现节点故障时的零延迟恢复。</p> <h4 id="谜团-2--持续的吞吐量下降">谜团 #2 – 持续的吞吐量下降</h4> <p>即使更换了存储，<strong>个别的吞吐量下降仍在发生</strong>。我们最终发现，问题出在<strong>软件瓶颈</strong>上，具体来说是nanotron的<strong>内置数据加载器（nanosets）</strong>。它在处理非常大的步数时，会构建一个巨大的索引，导致共享内存增加，从而引发吞吐量下降。<strong>修复方法是：将SmolLM2中使用的、经过验证的TokenizedBytes数据加载器复制到nanotron中。</strong></p> <h4 id="谜团-3--嘈杂的loss">谜团 #3 – 嘈杂的Loss</h4> <p>换上新的数据加载器后，吞吐量稳定了，但<strong>Loss曲线变得更嘈杂</strong>。我们发现这是因为新的数据加载器存在一个<strong>洗牌Bug</strong>：它只洗牌了文档，但一个批次内的序列是按顺序读取的。这导致一个长的、低质量的文件可能会填满整个批次，引发Loss尖峰。<strong>修复方法是：离线预洗牌分词后的序列。</strong></p> <h4 id="谜团-4--不尽如人意的性能">谜团 #4 – 不尽如人意的性能</h4> <p>在解决了上述所有问题并重新启动后，训练平稳进行。然而，在大约<strong>1万亿Token</strong>时，我们发现尽管拥有更多参数和更好的数据，<strong>3B模型的表现竟然比1.7B的SmolLM2更差</strong>。由于所有其他组件都经过了验证，我们最终将矛头指向了唯一未经充分测试的差异：<strong>张量并行（Tensor Parallelism, TP）</strong>。</p> <p><strong>最终的修复：</strong> 经过排查，我们发现了一个微妙但致命的Bug：我们在<strong>所有TP等级（Ranks）上使用了相同的随机种子</strong>，而每个等级应该使用不同的种子。这导致了权重初始化相关，从而影响了收敛。修复这个Bug后，性能终于与预期一致。</p> <p>这次调试过程强化了一个核心原则：<strong>一个坚实的消融实验设置的真正价值，不仅在于构建一个好模型，更在于当问题发生时，它能让你快速定位问题根源。</strong></p> <h3 id="63-坚持到底">6.3 坚持到底</h3> <ul> <li> <strong>训练监控:</strong> 不要只看Loss曲线。<strong>下游评估结果</strong>和与历史运行的比较至关重要。同时，持续监控<strong>吞吐量</strong>和<strong>硬件健康状况</strong>。</li> <li> <strong>处理Loss尖峰:</strong> 尖峰分为<strong>可恢复</strong>和<strong>不可恢复</strong>两种。常见的罪魁祸首包括高学习率、坏数据、糟糕的初始化或精度问题。修复方法包括<strong>跳过有问题的批次</strong>、临时收紧梯度裁剪等。</li> </ul> <h3 id="64-中途训练">6.4 中途训练</h3> <p>现代LLM预训练通常涉及<strong>多个阶段</strong>。SmolLM3也遵循了类似的理念，有<strong>计划好的干预</strong>以引入更高质量的数据和扩展上下文。</p> <ul> <li> <strong>第一阶段 (8T Token, 4k上下文):</strong> 基础训练，使用核心预训练混合数据。</li> <li> <strong>第二阶段 (2T Token, 4k上下文):</strong> 注入更高质量的过滤数据集，如Stack-Edu和FineMath4+。</li> <li> <strong>第三阶段 (1.1T Token, 4k上下文):</strong> 在学习率衰减阶段，进一步上采样高质量数据，并引入指令和推理数据。</li> </ul> <h4 id="长上下文扩展从4k到128k">长上下文扩展：从4k到128k</h4> <p>我们没有直接跳到128k，而是<strong>分阶段逐渐扩展上下文</strong>：4k → 32k → 64k。在每个阶段，我们都重新开始一个短期的学习率调度，并运行消融实验来寻找最佳的<strong>长上下文数据混合</strong>和<strong>RoPE theta值</strong>。最终，我们使用<strong>YARN</strong>技术，让在64k上下文上训练的模型能够在<strong>推理时外推到128k</strong>。</p> <h2 id="7-超越基础模型--2025年的后训练">7. 超越基础模型 — 2025年的后训练</h2> <p>预训练赋予了SmolLM3原始的能力，但要成为一个人们能实际使用的模型，我们还需要进行<strong>后训练（Post-Training）</strong>。这包括<strong>监督式微调（SFT）、强化学习（RL）、模型合并</strong>等等。</p> <p>如果说预训练是<strong>将知识强行灌输</strong>到权重中，那么后训练就是将这种原始能力<strong>雕琢</strong>成某种<strong>可靠且可控</strong>的东西。</p> <h3 id="71-后训练指南针">7.1 后训练指南针</h3> <p>就像预训练一样，后训练也需要一个清晰的指南针：</p> <ul> <li> <strong>为什么？</strong> 明确你的目标：是进行研究，是满足特定的生产需求，还是填补开源空白？</li> <li> <strong>做什么？</strong> 确定你的优先事项：是追求指令遵循，还是多功能助手，还是推理引擎？</li> <li> <strong>怎么做？</strong> 这就是“配方”发挥作用的地方，包括SFT、偏好优化（PO）、RL、数据策划和评估。</li> </ul> <p>对于SmolLM3，我们的目标是训练一个<strong>混合推理模型</strong>，其推理能力在<strong>非英语语言中也保持良好</strong>，并支持<strong>工具调用</strong>和<strong>长上下文</strong>。</p> <h3 id="72-首要任务评估先于一切">7.2 首要任务：评估先于一切</h3> <p>后训练的第一步是决定<strong>正确的评估集</strong>。一个好的助手应该能够处理模糊指令、进行分步规划、编写代码和调用工具。这依赖于<strong>推理、长上下文处理、数学、代码和工具使用</strong>的混合技能。</p> <p>我们使用一个<strong>分层的评估套件</strong>，包括：</p> <ul> <li> <strong>能力评估:</strong> 针对基本技能，如使用GPQA Diamond进行知识评估，使用AIME进行数学评估，使用LiveCodeBench进行代码评估。</li> <li> <strong>综合任务评估:</strong> 测试接近真实场景的能力，如使用RULER和HELMET进行长上下文评估，使用IFEval进行指令遵循评估，使用AlpacaEval和ArenaHard进行对齐评估，使用TAU-Bench进行工具调用评估。</li> <li> <strong>防止过拟合评估:</strong> 使用如GSMPlus这样对现有基准进行扰动的评估。</li> <li> <strong>内部评估和“感觉”评估:</strong> 实现针对特定能力的内部评估，并让专家与模型互动，以发现评估分数无法捕捉到的微妙问题。</li> </ul> <p>至此，我们已经走过了从预训练到后训练规划的完整旅程。每一个环节都充满了挑战与权衡，但通过系统性的方法和严格的实验，我们最终将原始的算力转化为了一个强大而可靠的AI模型。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mnn-eagle/">MNN支持Eagle3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/llm-train/">LLM训练实战手册</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwen3vl/">MNN模型支持：Qwen3-VL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/qwenfamily/">一图读懂Qwen</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/device-llm-memory-capacity/">端侧LLM硬件系列（二）：内存容量</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhaode Wang. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>