---
layout: post
title: LLM训练实战手册
date:   2025-11-04
last_modified_at: 2025-11-04
categories: [llm]
---

## 1. 前言

表面上看，已发表的研究论文似乎把一切都说得轻描淡写：无非是战略性的架构选择、精心策划的数据集，以及充足的算力。结果光鲜亮丽，消融实验（ablations）结构清晰，事后看来每一步决策都显得理所当然。

但是，这些报告往往只展示了成功的“果”，并进行了一番“玫瑰色的回顾”（Rosy Retrospection）。它们并没有记录下**凌晨两点的 dataloader 调试**、**突然爆发的 Loss 尖刺**，或者那个**默默破坏你训练的细微张量并行（Tensor Parallelism）Bug**（后面我们还会专门提到！）。现实更加混乱、迭代性更强，充满了那些最终没有写进论文的“纠结瞬间”。

这一次，我们将带领大家深入幕后，直击 **SmolLM3** 的训练过程——这是一个在 **11 万亿（11T）Token** 上训练出来的 **30 亿（3B）参数多语言推理模型**。

这绝非一篇普通的博客文章，而是对一系列决策、发现和死胡同的“蜘蛛网”式梳理，它将为你揭示构建世界级语言模型所需要的核心洞察。

同时，本文也是我们**模型训练长文系列**的**“收官之作”**（或“压轴大戏”）：我们之前已经深入探讨了**大规模数据集构建**（FineWeb）、**编排数千块 GPU 协同工作**（Ultra Scale Playbook），以及**在过程的每一步选择最佳评估方法**（Evaluation Guidebook）。现在，我们将所有这些元素整合起来，共同打造一个强大的 AI 模型。

我们将全程陪伴大家，不仅仅是分享最终成功的“秘方”，更包括那些**塑造了每一个决策的失败、基础设施故障以及调试过程**。

这个故事读起来就像一出**扣人心弦的“戏剧”**：

*   你会看到那些有前景的小规模消融实验为什么在大规模上却可能 **“水土不服”**；
*   为什么我们在训练了 **1 万亿（1T）Token 之后不得不重新启动**；
*   如何有效地在保持强大的英语性能的同时，**平衡多语言、数学和代码这几个相互竞争的目标**；
*   以及最终，我们是如何**后训练（post-train）**出一个混合推理模型的。

我们努力将这趟冒险之旅组织成一个连贯的故事，而非仅仅是**冰冷的步骤列表**。请将它视为一本**实战指南**，献给所有想从“我们有优秀的数据集和 GPU”迈向“我们构建了一个真正强大的模型”的实践者。

我们希望这种毫无保留的开放分享，能够**弥合研究与生产之间的鸿沟**，让你的下一次训练跑起来**少一点混乱，多一点笃定**。


### 1.1 如何阅读这篇文章？（你可能不需要“一次性看完”）

老实说，这篇文章篇幅**非常、非常长**，想要一口气从头读到尾，在现实中可能不太可行。

好消息是，我们已经将文章结构化为几个**独立且清晰**的板块，你可以根据自己的兴趣和需求选择性地跳过或单独阅读。

以下是文章的结构指南：

- 🧭 训练指南针（Training compass）：

  *   **核心内容：** 这是一个高层次的讨论，关于你 **是否应该自行预训练（Pretrain）一个模型**。
  *   **适用人群：** 在你“烧光”所有风投资金之前，我们为你列出了几个必须问自己的基本问题，并系统地指导你完成决策过程。
  *   **温馨提示：** 这是一个比较宏观的部分。如果你是 **“技术党”**，想要直接看硬核技术内容，请快速略过此部分。

- 🚀 预训练（Pretraining）：

  *   **核心内容：** 在这之后的部分，涵盖了构建你自己 **预训练运行的可靠“配方”**所需了解的一切：如何运行消融实验（Ablations）、选择评估指标、混合数据源、进行架构决策、调整超参数，以及最终 **“熬过”** 这场训练马拉松。
  *   **适用人群：** 无论是计划 **从头开始预训练**，还是对 **持续预训练（Continued Pretraining，或称 Mid-training** 感兴趣的朋友，这个板块都适用。

- 🍒 后训练（Post-training）：

  *   **核心内容：** 在这部分，你将学到如何最大限度地利用你的预训练模型所需的所有“技巧”。从 **SFT (监督式微调)**、**DPO (直接偏好优化)**、**GRPO** 等一整套后训练“字母表”开始，一直到**模型合并（Model Merging）**这种充满“黑暗艺术与炼金术”的操作。
  *   **价值所在：** 关于如何让这些算法真正起效的知识，往往是通过痛苦的教训换来的。我们将在这里分享我们的经验，希望能让你少走一些弯路。

- 🏭 基础设施（Infrastructure）：

  *   **核心比喻：** 如果说预训练是蛋糕，后训练是上面的糖霜和樱桃，那么**基础设施就是那台工业级的烤箱**。没有它，什么都做不了；如果它坏了，你愉快的周日烘焙时间就会变成一场火灾隐患。
  *   **核心内容：** 理解、分析和调试 GPU 集群的知识分散在各种库、文档和论坛中。本节将详细讲解 **GPU 布局、CPU/GPU/节点/存储之间的通信模式**，以及如何**识别和克服瓶颈**。

**那么，我们从何处开始呢？** 很简单，**选择你最感兴趣的那个板块，让我们出发吧！**

## 2. 训练指南针：灵魂三问（Why → What → How）

机器学习领域对于 **“优化”** 似乎有着一种近乎痴迷的关系。我们的注意力总是集中在 Loss 曲线、模型架构和训练吞吐量上；毕竟，机器学习从根本上说就是关于 **优化模型的损失函数**。然而，在深入这些技术细节之前，有一个更根本的问题却常常被忽略：**我们真的有必要训练这个模型吗？**

开源 AI 生态系统几乎每天都在发布世界级的模型：Qwen、Gemma、DeepSeek、Kimi、Llama（当然，还有它的继任者们）、Olmo……这份名单每个月都在持续增长。它们不仅仅是研究原型或“玩具”示例，而是 **可以直接用于生产的模型**，涵盖了令人震惊的广泛用例——从多语言理解到代码生成，再到复杂的推理能力。更重要的是，它们大多带有 **宽松的许可协议（Permissive Licenses）**，并拥有活跃的社区随时准备为你提供帮助。

这也提出了一个**令人不安的真相**：也许，你根本不需要训练自己的模型。

这听起来像是一个“大模型训练指南”**最奇怪的开场白**。但事实是，许多失败的训练项目，并非因为超参数设置不当或代码有 Bug，而是因为 **有人决定训练一个他们根本不需要的模型**。所以，在你投入资金和时间开始训练，并钻研“如何执行”之前，你必须先回答两个核心问题：**你为什么要训练这个模型？** **你应该训练一个什么样的模型？** 如果对这两个问题没有清晰的答案，你很可能会浪费数月的算力投入和工程师时间，最终却造出了一个“全世界早已拥有”的东西，甚至更糟—— **一个根本没人需要的东西**。

让我们从 **“Why”（为什么）** 开始。因为如果不懂得你的目标，你后续的所有决策都将是 **毫无章法**。

### 2.1 Why：那个没人愿意回答的问题

让我们直言不讳地谈谈实践中经常发生的事情。

某个人（如果他们足够幸运）获得了 GPU 集群的访问权限——可能是通过研究经费，也可能是利用公司闲置的算力，而他们的思路大致是这样的：“我们有 100 块 H100s，可以用三个月。来训练一个模型吧！”模型大小被随意选择，数据集则从各种可用的资源中拼凑起来。训练开始了。六个月后，在烧光了算力预算和团队士气之后，这个模型却无人问津，**因为从没有人问过“为什么”**。

这里有一些**你绝对不应该训练模型**的常见理由：

*   **“我们恰好有空闲的算力。”**（这只是**资源**，不是**目标**。）
*   **“其他人都在做。”**（这是**同侪压力**，不是**战略**。）
*   **“AI 是未来。”**（这是**陈词滥调**，不是**计划**。）
*   **“我们想要最强大的模型。”**（这个目标**不够具体**，无法指导任何实际决策。）


“我们训练了自己的模型”这种诱惑力是强大的。但在投入大量时间和资源之前，有必要问一句：**你到底为什么需要训练这个模型？**

下面的流程图提供了一个思路指导，这是你在开始一个大型预训练项目前应该经历的思考过程。从技术角度看，你首先应该搞清楚的是：**是否已经有一个现成的模型，通过简单的提示（Prompt）或微调（Fine-tune）就能完成你的工作？**

本质上，只有在以下三个常见领域中，定制化的预训练才可能真正有意义：**你想进行开创性的研究，你的生产用例有非常特殊的需求，或者你想填补开源模型生态系统中的空白**。让我们快速看看每一种情况：

#### 2.1.1 科研（Research）：你到底想理解什么？

在大模型（LLM）领域，你可以做的研究课题非常多。

这些 LLM 研究项目的共同点是：你通常都会从一个**清晰定义的问题**开始。例如：

*   我们能否将基于这种**新型优化器**的训练扩展到 **100 亿（10B）参数以上**的模型？（*参考自：Muon 在 LLM 训练中的可扩展性*）
*   **仅使用强化学习，不依赖 SFT**，能否产出推理能力？（*参考自：DeepSeek-R1：通过强化学习激励 LLM 的推理能力*）
*   我们能否**仅靠纯合成的“教科书式”数据**，训练出优秀的小模型？（*参考自：Textbooks Are All You Need*）
*   我们能否通过仅训练**公开许可（Openly Licensed）的数据**来达到具有竞争力的性能？（*参考自：The Common Pile v0.1：一个 8TB 的公共领域和公开许可文本数据集*）

把假设设定得**尽可能具体**，并提前思考所需的**实验规模**，能够大大增加你成功的几率。

#### 2.1.2 产品化（Production）：为什么你不能直接用现成的模型？

企业不能直接使用现成的通用模型（off-the-shelf models）来解决其特定用例，主要有三个原因。其中两个是技术性的，另一个则关乎治理。

**第一个要自己训练模型的原因是：领域特殊性（Domain Specificity）。** 当你的数据或任务涉及 **高度专业化的词汇或结构**，而现有通用模型无法很好地处理时。例如：

*   一个关于 DNA 的模型，它需要独特的词汇表和处理长距离依赖关系的能力。
*   一个法律或金融模型，要求对领域特定的术语和逻辑有深入的理解。

**第二个相关原因是：部署约束（Deployment Constraints）。** 当你需要一个模型来适配你的硬件、延迟或隐私要求时。例如，一个需要在 **无人机上运行**，或在 **配备定制硬件（如 FPGA）的本地（on-prem）系统** 上运行的大模型。

这里有一个简单的测试方法：花几天时间，基于 Qwen3、Gemma3 或其他当前的 SOTA 模型进行构建。你能通过 **Prompting（提示）、工具调用（Tool-use）或后训练（Post-training）** 达到你的性能目标吗？如果不能，那么，可能就是时候自己训练一个了。

*   **一个小小的提醒：** 即使为了满足你的要求，所需的 **后训练预算非常庞大，它仍可能比从头开始训练更经济。** 毕竟，为你的模型微调 **1 万亿（1T）Token**，仍然比从头开始训练 **10 万亿（10T）Token** 更划算。
*   （*也是从这个时候开始，大模型训练者开始**神奇地**称之为 **“Mid-training”（中途训练）**，而不是 Post-training 了。*）

**第三个建立内部语言模型的原因是：安全与治理（Safety and Governance）。** 由于你身处一个受严格监管的行业或处理高风险的应用，你需要对训练数据、模型行为和更新周期拥有**完全的控制权**。你需要确切地知道模型中包含了什么，并能够向监管机构证明这一点。在某些情况下，你可能别无选择，只能自建模型。

以上是公司训练内部模型的主要原因。那么，那些发布开源模型的公司或组织又是怎么考虑的呢？

#### 2.1.3 战略性开源（Strategic Open-Source）：你看到了可以填补的空白吗？

经验丰富的 AI 实验室发布新的开源模型，最常见的原因之一是：**他们识别出了开源生态系统中的一个特定空白或一个新的 AI 用例。**

这种模式通常是这样的：你注意到一个 **未被充分探索的领域**。也许现在没有强大且具备超长上下文能力的 **设备端（on-device）模型**；或者现有的多语言模型在 **低资源语言上表现很弱**；又或者，领域正在转向像 Genie3 那样的 **交互式世界模型**，但还没有好的开源模型出现。

你有理由相信自己可以做得更好。也许你整理出了 **更优质的训练数据**，开发出了 **更棒的训练“配方”**，或者拥有其他机构无法达到的 **算力优势来支持“过度训练”（Overtrain）**。你的目标是具体的：不是“有史以来最好的模型”，而是“**最适合设备端使用的 3B 模型**”，或“**第一个具备 1M 上下文的小模型**”。

这是一个真实且有价值的目标。成功会创造价值：开发者会采用你的模型，它会成为其他人的基础设施，或者为你建立技术信誉。但**成功需要经验**。在一个竞争激烈的领域中，你需要知道什么才是真正可行的，以及如何可靠地执行。

为了让这个思路更具体，让我们来看看 Hugging Face 是如何思考这个问题的。

#### 2.1.4 Hugging Face 的思考：我们为什么要训练开源模型？

那么，Hugging Face 为什么要训练和发布开源模型呢？答案很简单：**我们致力于构建对开源生态系统有用的东西，并填补那些极少人涉足的空白。**

这包括数据集、工具，当然也包括训练模型。我们启动的每一个 LLM 训练项目，都是始于 **发现一个空白**，并坚信我们能做出有意义的贡献。

我们最初的 LLM 项目是在 **GPT-3 (Brown et al., 2020)** 发布之后启动的。当时，感觉没有人愿意构建一个开放的替代品，我们担心相关的知识最终会被锁定在少数几个工业实验室中。因此，我们发起了 **BigScience 研讨会**，旨在训练一个开源版本的 GPT-3。最终诞生的模型就是 **Bloom**，它汇集了数十位贡献者一年的努力，从构建训练堆栈、分词器（tokenizer）到预训练语料库，最终预训练了一个 **1750 亿（175B）参数的模型**。

Bloom 的继任者是 2022 年的 **StarCoder (Li et al., 2023)**。当时 OpenAI 为 GitHub Copilot 开发了 **Codex (Chen et al., 2021)**，但它是闭源的。很明显，构建一个开源替代品将为整个生态系统带来巨大的价值。因此，我们与 ServiceNow 合作，在 **BigCode** 的框架下，构建了 **The Stack** 数据集，并训练了 **StarCoder 15B** 来复现 Codex 的能力。**StarCoder2 (Lozhkov et al., 2024)** 的诞生，则是源于我们认识到可以进行更长时间的训练，并意识到 **更小但训练更久的模型可能比一个巨型模型更有价值**。我们训练了一个模型家族（3B/7B/15B），使用了数万亿（Trillions）的 Token，远远超过当时任何开放代码模型的训练量。

**SmolLM 系列**遵循了类似的模式。我们注意到当时 **强大的小型模型非常稀缺**，而我们刚刚构建了 **FineWeb-Edu (Penedo et al., 2024)** 这个强大的预训练数据集。**SmolLM (135M/360M/1.7B)** 是我们的第一个版本。**SmolLM2 (Allal et al., 2025)** 则专注于更好的数据和更长的训练，在多个方面达到了 SOTA 性能。而 **SmolLM3** 则扩展到了 **30 亿参数**，同时加入了 **混合推理（hybrid reasoning）、多语言能力和长上下文** 等社区在 2025 年高度重视的功能。

这种模式甚至延伸到了预训练之外：我们训练了 **Zephyr (Tunstall et al., 2023)** 来证明 **DPO** 可以进行大规模应用；启动了 **Open-R1** 来复现 DeepSeek R1 的蒸馏管线；并发布了用于 **竞技编程（competitive programming）** 的 **OlympicCoder**，在国际信息学奥林匹克竞赛中达到了 SOTA 性能。我们还探索了其他模态，例如用于视觉的 **SmolVLM (Marafioti et al., 2025)** 和用于机器人技术的 **SmolVLA (Shukor et al., 2025)**。

希望这一部分已经成功说服你：**深入思考你为什么要训练一个模型是极具价值的。**

在本文接下来的部分，我们将假设你已经完成了这种“灵魂拷问”，并且有了一个**合法的、充分的理由**去启动你的训练项目。

### 2.2 What：将目标转化为实际决策

既然你已经明确了 **为什么** 要训练，那么接下来就该确定 **应该训练什么** 了。

这里的“做什么”（What）指的是：**模型类型**（密集型 Dense、MoE 专家混合、混合型 Hybrid，还是全新的类型）、**模型规模**、**架构细节**和**数据混合配比**。

一旦你确定了“为什么”（Why），你就可以由此推导出“做什么”（What）。举例来说：

| 训练目的 (Why) | $\rightarrow$ | 模型规格 (What) |
| :--- | :--- | :--- |
| **追求设备端运行的快速模型** | $\rightarrow$ | **小且高效的模型** |
| **构建强大的多语言模型** | $\rightarrow$ | **大型分词器词汇表（Large Tokenizer Vocab）** |
| **需要超长上下文能力** | $\rightarrow$ | **混合型架构（Hybrid Architecture）** |

除了受用例驱动的决策外，还有一些选择是旨在优化训练本身的，例如让训练**更稳定、更具样本效率，或速度更快**。这些决策并非总是非黑即白，但你可以大致将决策过程分为两个阶段：

**规划阶段（Planning）：** 在进行实验之前，你需要将你的用例映射到需要确定的组件上：你的**部署环境**决定了**模型规模**的限制。你的**时间表**决定了你可以承担**哪些架构风险**。你的**目标能力**决定了**数据集**的要求。这个阶段的核心就是：将“Why”中的每一个约束条件，都与“What”中的具体规格**紧密连接**起来。

**验证阶段（Validation）：**一旦你有了一个起始点和一份潜在修改清单，就要进行**系统性的测试**。由于测试成本高昂，你应将精力集中在那些能 **有意义地提升你的用例性能**，或能 **显著优化你的训练过程**的改动上。这就是 **消融实验（Ablations）** 发挥作用的地方，我们将在后续的“消融实验”章节详细介绍。

在接下来的章节中，你将了解到定义模型的所有选项，以及如何通过系统性实验来缩小选择范围。但在那之前，我们想分享一些关于**如何组织团队和项目**的经验——这些经验来自于我们自己训练模型的实践，以及观察那些成功构建优秀 LLM 团队的优秀做法。

### 2.3 How：迭代速度与数据质量

通往罗马的道路当然不止一条，但我们发现，成功的大模型（LLM）训练团队之所以能脱颖而出，其关键因素在于 **迭代速度**。

训练 LLM 本质上是一种 **“边训边学”** 的学科，你训练的次数越多，你的团队就会变得越优秀。因此，那些一年只训练一个模型的团队，和那些一个季度就能训练一个模型的团队相比，**后者的进步速度会快得多**。你可以参考 Qwen 和 DeepSeek 等团队，他们现在已是家喻户晓的名字，凭借的就是长期以来持续快速地发布新模型。

除了迭代速度之外，**数据策划（Data Curation）**是迄今为止对 LLM 训练**最具影响力**的方面。人们天然倾向于钻研架构选择来改进模型，但那些在大模型训练中表现优异的团队，却是**对高质量数据痴迷程度高于一切**的团队。

另一个与迭代速度紧密相关的因素是**团队规模**：对于主要的预训练任务，你只需要**少数几个人**，并为他们配备足够的算力来执行即可。例如，如今要预训练一个像 Llama 3 这样的模型，你可能只需要 **2 到 3 个人**。只有当你开始涉足更多元化的训练和下游任务（如多模态、多语言、后训练等）时，才需要慢慢增加更多的人手，以在每个领域做到精通。

因此，秘诀就是：**从一个小型、装备精良的团队开始，每隔两到三个月就构建一个新模型**，你将在短时间内攀升至行业的顶端。

好了，接下来的文章将专注于这个团队的**日常技术细节**！

## 3. 每个大模型的诞生，都始于一场小小的“消融实验”

在我们开始训练一个大型语言模型（LLM）之前，我们需要做出无数将影响模型性能和训练效率的决策。我们该选择什么样的架构最适合我们的用例？使用哪种优化器和学习率调度？如何混合不同的数据源？

“这些决策是如何做出的？”这是一个被频繁问及的问题。人们有时期望它们是通过**深入的思考**得出的。虽然战略性思维至关重要——正如我们在前一节中讨论的，它能帮你识别出哪些架构更改值得测试——但**仅仅依靠推理是不够的**。在 LLM 领域，事物并不总是直觉化的，那些“应该有效”的假设在实践中往往会落空。

举个例子，使用看起来 **“质量最高的数据”** 并不一定能产出更强的模型。以 **arXiv** 为例，它是人类科学知识的巨大宝库。直觉上，用如此丰富的 STEM 数据进行训练应该能产出更优秀的模型，对吗？但实际上，它**并非总是如此**，特别是对于小模型，**甚至可能损害性能** (Shao et al., 2024)。**为什么会这样？** 原因是虽然 arXiv 论文知识丰富，但它们**高度专业化**，并以一种狭隘的学术风格撰写，这与模型最擅长学习的**多样化、通用性文本**截然不同。

那么，如果苦思冥想没有帮助，我们如何知道什么才是有效的呢？**答案是：像优秀的经验主义者一样，运行大量的实验！** 机器学习不是纯粹的数学，它更像是一门**实验科学**。由于这些实验将指导我们许多关键决策，因此将它们设置好至关重要。我们本质上希望从这些实验中获得两个主要属性：
1.  **速度（Speed）：** 它们应该尽可能快地运行，以便我们能够频繁迭代。我们能运行的消融实验越多，能验证的假设就越多。
2.  **可靠性（Reliability）：** 它们应该提供强大的**判别力（discriminative power）**。如果我们关注的指标无法在早期有意义地分辨不同设置之间的优劣，那么我们的消融实验可能提供的洞察就很少（如果结果充满噪音，我们就有追逐噪音的风险！）。

但在设置消融实验之前，我们需要对**架构类型**和**模型规模**做出一些基础性的选择。这些由我们的“指南针”指导的决策，会影响我们使用哪种训练框架、如何分配算力预算，以及从哪个基线开始。

对于 SmolLM3，我们选择了 **30 亿参数的密集型（dense）Llama 风格架构**，因为我们的目标是小型设备端模型。但正如你将在 **“设计模型架构”** 一章中看到的，**MoE 或混合模型**可能更适合你的用例，不同的模型规模也伴随着不同的权衡取舍。稍后我们将深入探讨这些选择，并向你展示如何做出这些决定。现在，让我们从最实际的第一步开始：**选择你的基线（Baseline）**。

### 3.1 选择你的基线（Baseline）

每一个成功的模型都是建立在一个**经过验证的基础**之上，然后根据自身需求进行修改的。

*   当 Qwen 训练他们的第一个模型家族时 (Bai et al., 2023)，他们以 **Llama 的架构**为起点。
*   当 Meta 训练 Llama 3 时，他们从 **Llama 2** 开始。
*   Kimi K2 则始于 **DeepSeek-V3 的 MoE 架构**。

这种“继承”不仅适用于架构，也适用于训练超参数和优化器。

**为什么呢？** 优秀的架构和训练设置需要多年的迭代，并汇集众多组织的智慧。标准的 Transformer 结构和 Adam 等优化器，都是经过**数千次实验**才被完善的。人们已经发现了它们的失败模式，调试了不稳定性，并优化了实现。

**从一个经过验证的基础开始，意味着你继承了所有这些积累的知识。** 而从零开始，则意味着你需要自己重新发现每一个问题。

以下是一个好的架构起点应该具备的条件：

1.  **符合你的约束条件：** 契合你的部署目标和实际用例。
2.  **经过大规模验证：** 在相似或更大的规模上，跑过**数万亿（multi-trillion）Token** 的训练。
3.  **文档完善：** 有明确的、被证明在开源模型中有效的超参数配置。
4.  **框架支持良好：** 理想情况下，它应该被你考虑使用的**训练框架**和计划用于推理的**推理框架**所支持。

下面列出了一份非详尽的清单，展示了 2025 年针对不同架构类型和模型规模的一些强大基线选项：

| 架构类型 | 模型家族 | 常见规模 (Sizes) |
| :--- | :--- | :--- |
| **密集型（Dense）** | Llama 3.1 | 8B, 70B |
| **密集型（Dense）** | Llama 3.2 | 1B, 3B |
| **密集型（Dense）** | Qwen3 | 0.6B, 1.7B, 4B, 14B, 32B |
| **密集型（Dense）** | Gemma3 | 12B, 27B |
| **密集型（Dense）** | SmolLM2, SmolLM3 | 135M, 360M, 1.7B, 3B |
| **MoE (专家混合)** | Qwen3 MoE | 30B-A3B, 235B-A122B |
| **MoE (专家混合)** | GPT-OSS | 21B-A3B, 117B-A5B |
| **MoE (专家混合)** | Kimi Moonlight | 16B-A3B |
| **MoE (专家混合)** | Kimi-k2 | 1T-A32B |
| **MoE (专家混合)** | DeepSeek V3 | 671B-A37B |
| **混合型（Hybrid）** | Zamba2 | 1.2B, 2.7B, 7B |
| **混合型（Hybrid）** | Falcon-H1 | 0.5B, 1.5B, 3B, 7B, 34B |
| **MoE + 混合型** | Qwen3-Next | 80B-A3B |
| **MoE + 混合型** | MiniMax-01 | 456B-A46B |

因此，请找到你的架构类型，并选择一个参数数量接近你目标模型的基线。**不要过度纠结于此**，因为你最初选择的架构并非一成不变。在下一节中，我们将看到如何从一个基线出发，一步步构建出最适合你的**最终架构**。

#### 3.1.1 修改你的基线：**“去风险化”**的原则

现在你拥有了一个有效且符合你用例的基线模型。你大可以停在这里，用你的数据混合集（假设数据质量不错）进行训练，很可能会得到一个体面的模型。许多成功的项目正是这样做的。然而，基线模型并非为你的**特定约束**而优化，它们是为其构建者的用例和部署目标而设计的。因此，你很可能需要进行一些修改，使其更好地契合你的目标。但请注意，**每一次架构上的更改都伴随着风险**：它可能提升性能、彻底搞砸，或者什么也没做，只是浪费了你的消融实验算力。

让你保持在正轨上的纪律是 **“去风险化”（Derisking）**：**除非你已经测试并确认它有帮助，否则绝不更改任何东西。**

棘手之处在于，你的基线和训练设置有太多可修改的组件：**注意力机制、位置编码、激活函数、优化器、训练超参数、归一化方案、模型布局**等等。每一个都代表着一个潜在的实验，而这些组件往往以**非线性**的方式相互作用。你既没有时间，也没有算力来测试所有组合或探索所有的交互。

正确的做法是：**从测试有前景的改动开始，并以当前基线为参照。** 当某个改动有效时，就将其集成进来，创建一个**新的基线**，然后针对这个新基线测试下一个改动。如果你的算力预算允许，你可以独立测试多项改动，并运行 **“留一法分析”（leave-one-out analysis）**。

**千万不要掉入陷阱：** 避免对每一个超参数进行详尽的**网格搜索（Grid Searches）**，也避免测试每一个新出现的架构变体。

现在你知道了如何通过战略规划来确定哪些改动是有前景的，接下来就该进入**经验验证**了。在接下来的部分中，我们将向你展示如何在实践中真正测试这些更改。我们将涵盖如何设置可靠的实验、如何解读结果以及避免常见的陷阱。随后，在后续章节中，我们将通过具体案例，讲解如何测试流行的架构、数据、基础设施和训练决策。

那么，让我们先搭建一个可用于实验的简单消融设置。第一步，我们需要决定选择哪个训练框架。

### 3.2 挑选训练框架

我们需要做的第一个决策是：选择哪个框架来训练模型，进而也决定了用来运行所有消融实验的框架。这个选择需要平衡以下三个关键考量点：

1.  **架构兼容性：** 框架必须支持我们的目标架构，或能让我们轻松地进行扩展。
2.  **稳定性和生产就绪：** 框架需要稳定、成熟，不会在训练中途神秘地崩溃。
3.  **高吞吐量：** 它应该能提供强大的吞吐量，以便我们快速迭代，最大限度地利用算力预算。

在实践中，这些要求可能相互掣肘，形成权衡取舍。让我们来看看可用的选项。

| 框架 | 特性覆盖 | 实战检验 | 优化程度 | 核心/总代码行数 | 扩展性与调试难度 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Megatron-LM** | ✅ 功能全面 | ✅ Kimi-K2, Nemotron | ✅ 3D 并行先驱 | 93k / 269k | ⚠️ 难度大，不适合新手 |
| **DeepSpeed** | ✅ 功能全面 | ✅ BLOOM, GLM | ✅ ZeRO & 3D 并行先驱 | 94k / 194k | ⚠️ 难度大，不适合新手 |
| **TorchTitan** | ⚡ 功能集不断增长 | ⚠️ 较新，但经 PyTorch 团队检验 | ⚡ 针对密集模型优化，MoE 正在改进中 | 7k / 9k | ⚡ 适中：需了解并行知识 |
| **Nanotron** | 🎯 极简，为 HF 预训练定制 | ✅ 是 (StarCoder, SmolLM) | ✅ 高度优化 (UltraScale Playbook) | 15k / 66k | ⚡ 适中：需了解并行知识 |

上表总结了流行框架之间的关键权衡。（前三个框架的代码行数数据来自 TorchTitan 技术报告 (Liang et al., 2025)）。让我们详细讨论每一个框架：

**Megatron-LM**
Nvidia 的 Megatron-LM 已经存在多年，久经沙场。它是 Kimi 的 K2 (Team et al., 2025) 等模型的幕后功臣，提供了可靠的吞吐量和我们想要的大多数生产功能。但这种成熟也带来了**复杂性**：当你刚接触它时，代码库可能会让人难以理解和修改。

**DeepSpeed**
DeepSpeed 属于类似的类别。它是 **ZeRO 优化**的先驱，为 BLOOM 和 GLM 等模型提供了动力。和 Megatron-LM 一样，它经过了广泛的实战检验和优化，但面临着同样的**复杂性挑战**。庞大的代码库（总计 194k 行）在入门时可能令人生畏，尤其是当你需要实现自定义功能或调试意外行为时。

**TorchTitan**
另一方面，PyTorch 最近推出的 **TorchTitan** 库则**轻量且更易于导航**，这得益于其紧凑和模块化的代码库。它具备预训练所需的核心功能，非常适合**快速实验**。然而，由于它相对较新，实战检验不如前两者充分，并且由于仍在积极开发中，**稳定性可能略逊一筹**。

**Nanotron**
我们 Hugging Face 采取了不同的路径，从头构建了自己的框架 **Nanotron**。这为我们带来了**完全的灵活性**和对大规模预训练的**深入理解**——这些洞察后来演变成了《Ultra Scale Playbook》。虽然我们开源了该库并获得了社区的宝贵反馈，但在大多数情况下，我们不得不先自己对功能进行实战检验。该框架现在支持我们训练所需的所有生产功能，但仍在构建如 **MoE 支持**等区域。

对于我们而言，从头构建当时是合理的，但这需要对团队专业知识和时间进行重大投入，用于调试问题和添加缺失功能。一个强大的替代方案是**分叉（fork）**一个现有框架，并根据你的需求进行增强。例如，Thinking Machines Lab 就是将他们的内部预训练库作为 TorchTitan 的一个分叉来构建的（来源）。

最终，你的选择取决于**团队的专业知识、目标功能，以及你愿意投入多少时间进行开发，而不是直接使用最成熟的生产选项。**

如果多个框架都能满足你的需求，那么请在你的特定硬件上**比较它们的吞吐量**。对于快速实验和竞速运行，**更简洁的代码库通常会获胜**。


### 3.3 消融实验设置

既然训练框架已定，我们现在就需要设计我们的消融实验设置。我们需要实验足够快以便快速迭代，但又需要足够大，以确保结果能提供有价值的信号，并能 **外推（extrapolate）** 到最终的模型。让我们来看看如何设置它。

#### 3.3.1 搭建消融实验框架

消融实验的目标是在小规模上运行实验，并获得可以自信地外推到最终生产运行的结论。

主要有两种方法：

1.  **方法一（减 Token）：** 保持目标模型大小不变，但在**更少的 Token** 上进行训练。例如，在 SmolLM3 的消融实验中，我们用完整的 **30 亿参数模型**，但在 **1000 亿（100B）Token** 上进行训练，而不是最终的 11 万亿 Token。
2.  **方法二（减模型）：** 如果目标模型太大，我们可以训练一个**更小的代理模型**来进行消融。例如，Kimi 在开发他们拥有 32 亿激活参数的 **1 万亿（1T）参数 Kimi K2 模型**时，对所有消融实验都使用完整大小显然过于昂贵，因此他们用一个 **30 亿参数 MoE 模型**（0.5B 激活参数）运行了部分消融实验 (Team et al., 2025)。

一个关键问题是：这些小规模的发现真的能迁移吗？根据我们的经验，如果某个改动在**小规模上损害了性能，你可以放心地将其排除在更大规模的训练之外**。但如果某个改动在小规模上有效，你仍然需要确保在**合理的 Token 数量**上进行了训练，才能高概率地得出这些发现可以外推到更大规模的结论。**训练时间越长，消融模型与最终模型越接近，结果就越可靠。**

在本文中，我们将使用一个**基线版（Vanilla）Transformer** 来进行所有消融实验。我们的主要设置为：一个遵循 **Llama 3.2 1B 架构的 10 亿参数 Transformer**，在 **450 亿 Token** 上训练。这在一个配备 **8 块 H100** 的节点上大约需要 **1.5 天**（使用 nanotron 配置，速度约为每 GPU 每秒 42k Token）。在 SmolLM3 的训练过程中，我们是在一个 3B 模型上用 100B Token 运行这些消融的（配置）。我们会在每个章节的末尾分享这些结果（你会看到结论是吻合的）。

我们的基线 1B 配置以结构化的 YAML 格式捕获了所有重要的训练细节。以下是关键部分：

```yaml
## 数据集及其混合权重
data_stages:
- data:
    dataset:
      dataset_folder:
      - fineweb-edu           # FineWeb 教育数据集
      - stack-edu-python      # Python 代码数据集
      - finemath-3plus        # 针对 3 级以上数学数据集

      dataset_weights:
      - 0.7                   # 权重 70%
      - 0.2                   # 权重 20%
      - 0.1                   # 权重 10%

## 模型架构，Llama3.2 1B 配置
model:
  model_config:
    hidden_size: 2048         # 隐藏层大小
    num_hidden_layers: 16     # 隐藏层数量
    num_attention_heads: 32   # 注意力头数量
    num_key_value_heads: 8    # K/V 头数量 (GQA)
    intermediate_size: 8192   # 中间层大小
    max_position_embeddings: 4096  # 最大位置嵌入
    rope_theta: 50000.0       # RoPE 旋转角度
    tie_word_embeddings: true # 绑定词嵌入

## 训练超参数，带余弦调度的 AdamW 优化器
optimizer:
  clip_grad: 1.0              # 梯度裁剪
  learning_rate_scheduler:
    learning_rate: 0.0005     # 最大学习率
    lr_decay_starting_step: 2000
    lr_decay_steps: 18000
    lr_decay_style: cosine
    lr_warmup_steps: 2000
    lr_warmup_style: linear
    min_decay_lr: 5.0e-05     # 最小衰减学习率
  optimizer_factory:
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.0e-08
    name: adamW

## 并行化，单节点配置
parallelism:
  dp: 8  # 数据并行，跨 8 块 GPU
  tp: 1  # 1B 规模不需要张量或流水线并行
  pp: 1

## 分词器
tokenizer:
  tokenizer_max_length: 4096
  tokenizer_name_or_path: HuggingFaceTB/SmolLM3-3B

## Batch size, 序列长度和总共 30B Token 的训练量
tokens:
  batch_accumulation_per_replica: 16 # 每个副本的批次累积
  micro_batch_size: 3 # GBS (全局批次大小)=dp * batch_acc* MBS * sequence=1.5M tokens
  sequence_length: 4096
  train_steps: 20000 # GBS * 20000 = 30B

...
```

在我们的消融实验中，我们会根据测试的内容修改不同的部分，同时保持其他一切不变：测试架构选择时修改 `model` 部分，测试优化器和训练超参数时修改 `optimizer` 部分，测试数据策略时修改 `data_stages` 部分。

运行消融实验时，某些架构更改会显著改变参数数量。例如，从 **绑定（tied）** 词嵌入切换到 **不绑定（untied）** 会使我们的嵌入参数翻倍，而从 MHA 切换到 GQA 或 MQA 则会大幅减少我们的注意力参数。为了确保公平比较，我们需要跟踪参数数量，并偶尔调整其他超参数（如隐藏层大小或层数）以**保持模型大小大致相同**。

#### 3.3.2 理解效果：评估至关重要

一旦我们启动了消融实验，我们如何判断哪些改动有效，哪些无效呢？

任何训练模型的人第一直觉可能是查看 **Loss 曲线**，这确实很重要。你希望看到它平滑下降，没有剧烈的尖刺或不稳定。对于许多架构选择，Loss 与下游性能有很好的相关性，可能就足够了 (Y. Chen et al., 2025)。

然而，**仅看 Loss 并不总是可靠的**。以**数据消融实验**为例，你会发现用维基百科训练比用网页训练得到**更低的 Loss**（预测下一个 Token 更容易），但这**并不意味着你会得到一个更有能力的模型**。同样，如果我们在不同训练时更改了分词器，Loss 就没有直接可比性，因为文本被分割的方式不同了。某些更改可能还会专门影响推理和数学等特定能力，而这些影响在平均 Loss 中会被稀释。最后但同样重要的一点是，**模型即使在预训练 Loss 收敛后，仍可能在下游任务上持续改进** (Liu et al., 2022)。

我们需要更细致的评估，才能看清全貌并理解这些细微的影响。一个自然的方法是使用 **下游评估（downstream evaluations）** 来测试知识、理解、推理以及对我们重要的任何其他领域。

对于这些消融实验，最好关注那些 **能提供良好早期信号** 并 **避免嘈杂基准（noisy benchmarks）** 的任务。在 FineTasks 和 FineWeb2 中，可靠的评估任务由四个关键原则定义：

1.  **单调性（Monotonicity）：** 基准分数应随着模型训练时间的延长而持续改善。
2.  **低噪音（Low noise）：** 当我们使用相同设置但不同随机种子训练模型时，基准分数不应有剧烈波动。
3.  **高于随机水平的性能（Above-random performance）：** 许多能力只有在训练后期才会浮现，因此长期保持随机水平性能的任务对消融实验没有用处。例如，我们稍后将解释的多项选择格式的 MMLU 就是这种情况。
4.  **排名一致性（Ranking consistency）：** 如果某种方法在早期阶段优于另一种方法，那么随着训练的继续，这种排序应该保持稳定。

任务的质量还取决于**任务表述（task formulation）**（我们如何向模型提问）和**指标选择（metric choice）**（我们如何计算答案得分）。

三种常见的任务表述是：**多项选择格式 (MCF)、完形填空表述 (CF)** 和**自由形式生成 (FG)**。

*   **MCF** 要求模型从 Prompt 中明确给出并标有 A/B/C/D 的选项中进行选择（例如 MMLU 的做法）。
*   **CF** 中，我们比较不同选项的似然性（Likelihood），以查看哪个更可能，而无需在 Prompt 中提供它们。
*   **FG** 中，我们查看模型对给定 Prompt 进行贪婪生成（greedy generation）的准确性。FG 需要模型具备大量的**潜在知识**，对于进行完整的预训练之前的短时消融实验来说，通常难度太大而没有太大用处。

因此，在运行小型消融实验时，我们主要关注 **MCF 或 CF**。

研究表明，模型在训练早期阶段难以应对 MCF，只有经过大量训练后才能掌握这项技能，这使得 **CF 更适合用于获取早期信号** (Du et al., 2025; Gu et al., 2025; J. Li et al., 2025)。因此，我们使用 **CF** 进行小型消融实验，并在主运行中集成 MCF，因为它在模型达到一定阈值、信噪比足够高时，能提供更好的中期训练信号。**快速说明：** 为了在 CF 这样的序列似然评估中对模型的答案进行评分，我们将准确率计算为**正确答案具有最高对数概率（Log Probability）**（并按字符数标准化）的问题百分比。这种标准化可以防止偏向较短的答案。

我们的消融评估套件包括 FineWeb 消融实验中的基准，但排除了 SIQA（我们发现它噪音太大）。我们增加了 **GSM8K** 和 **HumanEval** 等数学和代码基准，以及用于长上下文消融的 **RULER** 长上下文基准。如下表所示，这些任务聚合在一起，以多种格式测试**世界知识、推理和常识**。为了加快评估速度，以牺牲一些额外噪音为代价，我们只评估每个基准的 **1,000 个问题**（GSM8K、HumanEval 和 RULER 除外，我们在 3B SmolLM3 消融中使用了完整集合，但在下面的 1B 实验中省略）。我们还对所有多项选择基准使用了**完形填空表述（CF）**的评估方式，如上所述。请注意，对于多语言消融和实际训练，我们增加了更多基准来测试多语言能力，这将在后面详述。下表总结了每个基准的关键特征：

| 基准 | 领域 | 任务类型 | 问题数量 | 测试能力 |
| :--- | :--- | :--- | :--- | :--- |
| **MMLU** | 知识 | 多项选择 | 14k | 跨 57 个学科的广泛学术知识 |
| **ARC** | 科学与推理 | 多项选择 | 7k | 小学级别的科学推理 |
| **HellaSwag** | 常识推理 | 多项选择 | 10k | 关于日常情景的常识推理（叙事补全） |
| **WinoGrande** | 常识推理 | 二元选择 | 1.7k | 需要世界知识的代词消解 |
| **CommonSenseQA** | 常识推理 | 多项选择 | 1.1k | 关于日常概念的常识推理 |
| **OpenBookQA** | 科学 | 多项选择 | 500 | 包含推理的小学科学事实 |
| **PIQA** | 物理常识 | 二元选择 | 1.8k | 关于日常物体的物理常识 |
| **GSM8K** | 数学 | 自由形式生成 | 1.3k | 小学数学应用题 |
| **HumanEval** | 代码 | 自由形式生成 | 164 | 根据文档字符串合成 Python 函数 |

注意 MMLU 和 ARC 如何用多项选择来测试事实知识，GSM8K 如何需要计算数学问题的数值答案，以及 HumanEval 如何需要生成完整的 Python 代码。这种多样性确保我们在整个消融实验中测试了模型能力的各个方面。

##### 3.3.2.1 消融实验使用什么数据混合配比？

对于**架构消融实验**，我们使用**固定混合的高质量数据集**进行训练，这些数据集能在广泛的任务上提供早期信号。我们使用了**英语 (FineWeb-Edu)、数学 (FineMath) 和代码 (Stack-Edu-Python)**。架构上的发现应该可以很好地外推到其他数据集和领域，包括多语言数据，因此我们可以保持我们的数据混合相对简单。

对于**数据消融实验**，我们采取相反的方法：我们**固定架构**，并**系统地改变数据混合配比**，以了解不同的数据源如何影响模型性能。

有时评估结果的差异可能很小。如果你的算力充足，可能值得用 **不同的随机种子（Seeds）** 重新运行相同的消融实验，以观察结果的变化程度。

一个可靠的消融设置的真正价值，并不仅仅在于构建一个好的模型。当我们在主训练运行中不可避免地出现问题时（无论我们准备得多么充分，问题总会发生），我们希望对我们所做的每一个决定都充满信心，并能**快速识别哪些组件没有经过充分测试，可能是问题的根源**。这种准备工作可以节省调试时间，并 **“武装”** 我们未来的心理健康。

#### 3.3.3 估算消融实验成本

消融实验非常棒，但它们需要 GPU 时间，因此有必要理解这些实验的成本。下表显示了 SmolLM3 预训练的完整算力分解：主运行（包括偶尔的停机时间）、训练前后的消融实验，以及用于解决意外的扩展问题（迫使我们重启训练）和一些调试所花费的算力（我们将在后面详细介绍）。

| 阶段 | GPU 数量 | 天数 | GPU-小时 |
| :--- | :--- | :--- | :--- |
| 主预训练运行 | 384 | 30 | 276,480 |
| 消融实验（预训练前） | 192 | 15 | 69,120 |
| 消融实验（训练中） | 192 | 10 | 46,080 |
| 训练重启与调试 | 384/192 | 3/4 | 46,080 |
| **总成本** | - | - | **437,760** |

这些数字揭示了一个重要的事实：**消融实验和调试总共消耗了 161,280 个 GPU 小时，超过了我们主训练运行（276,480 个 GPU 小时）的一半成本。** 在 SmolLM3 的开发过程中，我们总共进行了超过 100 次消融实验：我们在预训练消融上花费了 20 天，在中途训练消融上花费了 10 天，并花费了 7 天从意外的训练问题中恢复，导致重启和一些调试。

这突出表明了为什么**消融实验的成本必须计入你的算力预算**：你需要为**训练成本 + 消融成本 + 应对意外的缓冲**进行规划。如果你以 SOTA 性能为目标，正在实施新的架构更改，或者还没有一个经过验证的配方，那么消融实验将成为一个**主要的成本中心**，而不仅仅是小实验。

在我们进入下一节之前，让我们先确立一些每个进行实验的人都应该遵守的**基本规则**。

### 3.4 实验守则

**验证你的评估套件。** 在训练任何模型之前，请确保你的评估套件能够**复现**你将要比较的模型的已发布结果。如果任何基准测试本质上是**生成式**的（例如 GSM8K），请额外**保持警惕**，手动检查几个样本，以确保 Prompt 格式正确，并且任何后处理步骤都能提取到正确的信息。由于评估结果将指导你做出每一个决策，因此**正确完成这一步对于项目的成功至关重要！**

**测试每一个更改，无论多小。** 不要低估那个看似无害的库升级，或者那个声称“只修改了两行代码”的 Commit 所带来的影响。这些微小的变化可能会引入**微妙的 Bug** 或**性能偏移**，从而污染你的结果。你需要一个在对你重要的用例上具有强大测试套件的库，以避免回归。

**一次只改变一件事。** 在实验之间，保持所有其他设置**完全相同**。有些变化可能会以意想不到的方式相互作用，所以我们首先要评估**每个变化的单独贡献**，然后才能尝试将它们结合起来，看看它们的总体影响。

**训练足够的 Token，并使用充分的评估。** 正如我们前面提到的，我们需要确保评估套件有良好的覆盖，并训练足够长的时间以获得**可靠的信号**。在这里“偷工减料”将导致嘈杂的结果和错误的决策。

遵守这些规则可能会让你感觉**过于谨慎**，但另一种选择是花费数天时间调试**神秘的性能下降**，而结果发现这只是几天前一个**不相关的依赖更新**造成的。**黄金原则：一旦你拥有一个良好的设置，任何更改都应该经过测试！**

## 4. 设计模型架构

既然我们已经有了实验框架，是时候做出定义我们模型的**重大决策**了。我们做出的每一个选择，从模型大小到注意力机制再到分词器（Tokenizer）的选择，都会创造出各种约束和机遇，直接影响模型的训练和最终的使用。

请记住 **“训练指南针”**：在做出任何技术选择之前，我们都需要对 **“为什么”（Why）和“做什么”（What）** 有清晰的认识。我们为什么要训练这个模型？它应该是什么样子？

这听起来很显而易见，但正如我们在“训练指南针”中所解释的，在这里保持**深思熟虑**将塑造我们的决策，并避免我们在无尽的实验空间中迷失方向。我们的目标是构建一个**英语 SOTA** 模型吗？**长上下文**是我们的首要任务吗？还是我们试图验证一种**新的架构**？虽然所有这些情况下的训练循环可能看起来相似，但我们运行的实验以及我们接受的**权衡取舍**将截然不同。**尽早回答这个问题**有助于我们决定如何平衡我们在数据和架构工作之间的时间分配，以及在开始运行之前，要在每个方面创新多少。

因此，让我们以身作则，回顾一下指导 SmolLM3 设计的目标。我们想要一个**强大的设备端应用模型**，同时具备**有竞争力的多语言性能、可靠的数学和编码能力，以及强大的长上下文处理能力。**正如我们前面提到的，这引导我们选择了**一个 30 亿参数的密集型模型**：它足够大以提供强大的能力，但又足够小以舒适地安装在手机上。考虑到边缘设备的内存限制和我们的项目时间线（大约 3 个月），我们选择了**密集型 Transformer**，而不是 MoE 或混合模型。

我们从 SmolLM2 获得了一个小规模（17 亿参数）的英语训练工作“配方”，但扩大规模意味着**重新验证一切**，并解决**多语言和扩展上下文长度**等新挑战。这就是**明确的目标如何塑造我们的方法**的一个清晰例子。例如，在 SmolLM2 中，我们在预训练结束时努力扩展上下文长度，因此对于 SmolLM3，我们从一开始就做出了架构选择——比如使用 **NoPE** 和**文档内掩码 (intra-document masking)**（后续会详细介绍）——以最大化我们成功的几率，而事实证明这种方法奏效了。

一旦我们的目标明确，我们就可以开始做出将目标变为现实的技术决策。在本章中，我们将系统地探讨这些核心决策：**架构、数据和超参数**。请将此视为我们的**战略规划阶段**，把这些基础工作做好，将使我们在真正的训练马拉松中避免代价高昂的错误。

### 4.1 架构选择

如果你观察最近的模型，比如 Qwen3、Gemma3 或 DeepSeek v3，你会发现尽管它们存在差异，但它们都共享着同一个基础——**2017 年引入的 Transformer 架构**。多年来发生变化的不是基本结构，而是对其核心组件的**精炼和改进**。无论你是构建**密集型模型（Dense Model）**、**专家混合模型（Mixture of Experts, MoE）**还是**混合架构（Hybrid Architecture）**，你都在使用这些相同的构建块。

这些改进源于各大团队对更优性能的追求，以及对特定挑战的攻克：**推理时的内存限制、大规模训练时的不稳定性**，或**处理更长上下文的需求**。一些修改，比如从**多头注意力（MHA）**转向计算效率更高的注意力变体，如**分组查询注意力（GQA）**，现已被广泛采纳。而其他修改，比如不同的**位置编码方案**，仍在争论之中。最终，今天的实验将凝结成明天的**基线架构**。

那么，现代 LLM 今天实际在使用什么呢？让我们看看领先模型所趋同的关键点。不幸的是，并非所有模型都公开了其训练细节，但我们从 DeepSeek、OLMo、Kimi 和 SmolLM 等模型家族获得的透明度，足以让我们一窥当前的技术格局：

| 模型 | 架构 | 参数量 | 训练 Token | 注意力机制 | 上下文长度（最终） | 位置编码 | 精度 | 初始化 (Std) | 优化器 | 最大学习率 | 学习率调度 | Warmup 步数 | Batch Size |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| DeepSeek LLM 7B | Dense | 7B | 2T | GQA | 4K | RoPE | BF16 | 0.006 | AdamW | $4.2 \times 10^{-4}$ | Multi-Step | 2K | 9.4M |
| DeepSeek LLM 67B | Dense | 67B | 2T | GQA | 4K | RoPE | BF16 | 0.006 | AdamW | $3.2 \times 10^{-4}$ | Multi-Step | 2K | 18.9M |
| DeepSeek V2 | MoE | 236B (21B 激活) | 8.1T | MLA | 128K | Partial RoPE | - | 0.006 | AdamW | $2.4 \times 10^{-4}$ | Multi-Step | 2K | 9.4M -> 37.7M (Warmup 225B) |
| DeepSeek V3 | MoE | 671B (37B 激活) | 14.8T | MLA | 129K | Partial RoPE | FP8 | 0.006 | AdamW | $2.2 \times 10^{-4}$ | Multi-Step + Cosine | 2K | 12.6M -> 62.9M (Warmup 469B) |
| MiniMax-01 | MoE + Hybrid | 456B (45.9B 激活) | 11.4T | Linear + GQA | 4M | Partial RoPE | - | Xavier Init + Deepnorm | AdamW | $2 \times 10^{-4}$ | Multi-Step | 500 | 16M -> 32M -> 64M -> 128M |
| Kimi K2 | MoE | 1T (32B 激活) | 15.5T | MLA | 128K | Partial RoPE | BF16 | 可能是 0.006 | MuonClip | $2 \times 10^{-4}$ | WSD | 500 | 67M |
| OLMo 2 7B | Dense | 7B | 5T | MHA | 4K | RoPE | BF16 | 0.02 | AdamW | $3 \times 10^{-4}$ | Cosine | 2K | 4.2M |
| SmolLM3 | Dense | 3B | 11T | GQA | 128K | NoPE | BF16 | 0.02 | AdamW | $2 \times 10^{-4}$ | WSD | 2K | 2.3M |

如果你还不理解其中的一些术语，比如 **MLA、NoPE** 或 **WSD**，请不用担心。我们将在本节中解释每一个。现在，你只需要注意其中的多样性：**不同的注意力机制**（MHA, GQA, MLA）、**位置编码**（RoPE, NoPE, partial RoPE）以及**学习率调度**（Cosine, Multi-Step, WSD）。

看到如此长的架构选择清单，你可能会感到不知所措，不知道从何处开始。与大多数类似情况一样，我们将**循序渐进**，逐步建立所有必要的专业知识。我们将首先关注**最简单的基础架构（密集型模型）**，并详细研究每个架构方面。稍后，我们将深入探讨 **MoE 和混合模型**，并讨论何时使用它们是一个好的选择。最后，我们将探索**分词器（Tokenizer）**——一个经常被忽视和低估的组件。我们应该使用现有的分词器还是自己训练？我们又如何评估我们的分词器是否优秀？

但现在，让我们从每个 LLM 的核心开始：**注意力机制（Attention Mechanism）**。

#### 4.1.1 注意力机制（Attention）

Transformer 架构周围最活跃的研究领域之一就是注意力机制。虽然在预训练期间**前馈层（feedforward layers）**占据了大部分计算，但**注意力机制**在**推理时**成为了主要的瓶颈（尤其是在长上下文场景中）。在这种情况下，它会推高计算成本，并且 **KV 缓存（KV cache）**会迅速消耗 GPU 内存，从而降低吞吐量。让我们快速回顾一下主要的注意力机制，以及它们如何在**容量**和**速度**之间进行权衡。

##### 4.1.1.1 我的注意力需要多少个注意力头？

**多头注意力（Multi-Head Attention, MHA）** 是最初的 Transformer 引入的标准注意力机制。其核心思想是，你有 **N 个注意力头**，每个头独立地执行相同的检索任务：将隐藏状态转换为查询（Queries）、键（Keys）和值（Values），然后使用当前的查询来匹配键，检索出最相关的 Token，最后转发与匹配 Token 相关联的值。在推理时，我们不需要重新计算过去 Token 的 KV 值，可以直接重用它们。存储过去 KV 值的内存被称为 **KV-Cache**。随着上下文窗口的增长，这个缓存会迅速成为推理的瓶颈，并消耗 GPU 内存的很大一部分。以下是一个简单的计算，用于估算 Llama 3 架构在 MHA 和 8192 序列长度下的 KV-Cache 内存：

$$s_{KV} = 2 \times n_{bytes} \times seq \times n_{layers} \times n_{heads} \times dim_{heads}$$
$$= 2 \times 2 \times 8192 \times 32 \times 32 \times 128 \approx 4 \text{ GB (Llama3 8B)} $$
$$= 2 \times 2 \times 8192 \times 80 \times 64 \times 128 \approx 20 \text{ GB (Llama3 70B)}$$

*（注意：最前面的因子 2 来自于同时存储键和值缓存。）*

正如你所见，缓存大小与序列长度**线性**增加，但上下文窗口却以**指数级**增长，现在已经达到了数百万 Token。因此，提高缓存的效率将使推理时的上下文扩展变得更容易。

一个自然而然的问题是：我们真的需要为**每个头**都计算新的 KV 值吗？可能不需要。**多查询注意力（Multi-Query Attention, MQA）** (Shazeer, 2019) 和**分组查询注意力（Grouped Query Attention, GQA）** (Ainslie et al., 2023) 都解决了这个问题。

最简单的情况是**在所有头之间共享 KV 值**，从而将 KV 缓存的大小除以 $\text{n}_{heads}$。例如，对于 Llama 3 70B，这可以带来 64 倍的减少！这就是 **MQA** 的思想，并被 StarCoder 等一些模型用作 MHA 的替代方案。

然而，我们可能会因此**牺牲一些我们不愿意放弃的注意力容量**。因此，我们可以考虑**中间立场**：**在分组的头之间共享 KV 值**，例如 4 个头共享相同的 KV 值。这就是 **GQA** 的方法，它在 MQA 和 MHA 之间取得了平衡。

最近，DeepSeek-v2（并在 v3 中使用）引入了 **多潜变量注意力（Multi-Latent Attention, MLA）** (DeepSeek-AI et al., 2024)，它使用了一种不同的策略来压缩缓存：**它不减少 KV 值的数量，而是减少它们的尺寸**，简单地存储一个**潜变量（latent variable）**，该变量可以在运行时解压缩为 KV 值。通过这种方法，他们成功地将缓存压缩到相当于 **GQA 具有 2.25 个组**的等效值，同时提供了比 MHA **更强大的性能**！为了让它与 RoPE 配合工作，需要一个小小的调整，即增加一个额外的微小潜向量。在 DeepSeek-v2 中，他们选择了 $4 \times \text{dim}_{head}$ 作为主潜变量，和 $1/2 \times \text{dim}_{head}$ 用于 RoPE 部分，因此总共是 $4.5 \times \text{dim}_{head}$，它同时用于 K 和 V，从而消除了最前面乘数的 2。

下表比较了我们刚才讨论的注意力机制。为了简化，我们比较了**每个 Token 使用的参数量**。如果你想计算总内存，只需乘以每个参数的字节数（通常为 2）和序列长度即可：

| 注意力机制 (Attention Mechanism) | 每个 Token 的 KV-Cache 参数量 |
| :--- | :--- |
| **MHA (多头注意力)** | $= 2 \times n_{heads} \times n_{layers} \times dim_{head}$ |
| **MQA (多查询注意力)** | $= 2 \times 1 \times n_{layers} \times dim_{head}$ |
| **GQA (分组查询注意力)** | $= 2 \times g \times n_{layers} \times dim_{head}$ （通常 $g=2, 4, 8$） |
| **MLA (多潜变量注意力)** | $= 4.5 \times n_{layers} \times dim_{head}$ |

现在，让我们看看这些注意力机制在实际实验中的表现如何！

##### 4.1.1.2 消融实验 - GQA 胜过 MHA

在这里，我们比较了不同的注意力机制。我们的基线模型使用了 **32 个查询头（Query Heads）** 和 **8 个 KV 头（KV Heads）**，这对应于 **GQA（分组查询注意力）**，其比率为 32/8=4。如果我们使用 MHA，或者如果我们使用更少的 KV 头和更高的 GQA 比率，性能将如何变化？

请注意，更改 KV 头的数量会影响参数量，特别是对于 MHA 的情况。为了保持一致性，我们**调整了 MHA 运行的层数**，因为它否则会有超过 1 亿参数的差异；对于其他设置，我们保持了默认的 16 层。

| 注意力类型 | 查询头（Query Heads） | KV 头（KV Heads） | 层数 | 参数量 | 备注 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **MQA** | 32 | 1 | 16 | 1.21B | |
| **GQA (比率 16)** | 32 | 2 | 16 | 1.21B | |
| **GQA (比率 8)** | 32 | 4 | 16 | 1.22B | 我们的基线 |
| **GQA (比率 4)** | 32 | 8 | 16 | 1.24B | |
| **GQA (比率 2)** | 32 | 16 | 15 | 1.22B | **减少了层数** |
| **MHA** | 32 | 32 | 14 | 1.20B | **减少了层数** |
| GQA (比率 2) | 32 | 16 | 16 | 1.27B | **参数量过大 - 未进行消融** |
| MHA | 32 | 32 | 16 | 1.34B | **参数量过大 - 未进行消融** |

因此，我们比较了 MHA、MQA 和 4 种 GQA 设置（比率分别为 2、4、8、16）。

观察消融结果，我们发现 **MQA 和 GQA（16 个分组）**（分别只留下 1 个和 2 个 KV 头）的性能显著**低于 MHA**。另一方面，**GQA 配置（2、4、8 个分组）的性能大致与 MHA 持平**。

这个结果在 Loss 曲线和下游评估中都保持一致。我们在 HellaSwag、MMLU 和 ARC 等基准测试中清楚地观察到了这一点，而 OpenBookQA 和 WinoGrande 等基准则显示出少许噪音。

基于这些消融实验，**GQA 是 MHA 的一个可靠替代品**。它在保持性能的同时，在推理时更加高效。一些最新的模型采用了 **MLA** 来实现更进一步的 KV 缓存压缩，尽管它尚未被广泛采用。由于在进行消融实验时 MLA 尚未在 nanotron 中实现，我们没有对其进行消融。

**对于 SmolLM3，我们最终使用了 GQA，分组数量为 4。**

除了注意力架构本身，我们在训练中使用的**注意力模式（Attention Pattern）**也很重要。接下来，让我们看看**注意力掩码（Attention Masking）**。

##### 4.1.1.3 文档掩码（Document Masking）

我们如何在训练序列中应用注意力，直接影响着计算效率和模型性能。这就引出了 **文档掩码（Document Masking）** 以及更广泛的问题：我们在数据加载器（dataloader）中如何构建训练样本？

在预训练期间，我们使用**固定长度的序列**进行训练，但我们的文档长度是**可变**的。一篇研究论文可能有 10k 个 Token，而一个简短的代码片段可能只有几百个 Token。我们如何将可变长度的文档放入固定长度的训练序列中呢？

将较短的文档填充（Padding）到目标长度会浪费算力在无意义的填充 Token 上。相反，我们使用**打包（Packing）**：将文档与**序列结束（End-of-Sequence, EOS）Token** 一起打乱并连接起来，然后将结果分割成与序列大小匹配的固定长度块。

实际操作看起来是这样的：

```
File 1: "Recipe for granola bars..." (400 tokens) <EOS>
File 2: "def hello_world()..." (300 tokens) <EOS>
File 3: "Climate change impacts..." (1000 tokens) <EOS>
File 4: "import numpy as np..." (3000 tokens) <EOS>
...

After concatenation and chunking into 4k sequences:
Sequence 1: [File 1] + [File 2] + [File 3] + [partial File 4]
Sequence 2: [rest of File 4] + [File 5] + [File 6] + ...
```

如果一个文件足够长能填满我们的 4k 上下文，那么一个训练序列可能只包含一个完整文件。但在大多数情况下，文件都很短，所以序列包含了多个**随机连接**在一起的文件。

在标准的 **因果掩码（Causal Masking）** 下，一个 Token 可以关注打包序列中的所有先前 Token。在上面的例子中，文件 4 中那个 Python 函数里的 Token 可以关注燕麦棒食谱、气候变化文章以及其他碰巧打包在一起的内容。

让我们快速看看一个典型的 4k 预训练上下文会包含什么。一项快速分析显示，在 CommonCrawl 和 GitHub 中，**绝大多数（约 80-90%）的文件都短于 2k Token**。

下方的图表检查了本文中使用的较新数据集的 Token 分布：

<!-- ![alt text](image-2.png) -->

这意味着，在一个 2k 或 4k 的训练序列和标准因果掩码下，**绝大多数 Token 将浪费算力去关注那些被打包在一起的、不相关的文档内容。**

除了计算效率低下之外，Zhao et al. (2024) 发现这种方法引入了来自不相关内容的**噪音**，可能**降低性能**。他们建议使用**文档内掩码（intra-document masking）**：我们修改注意力掩码，使得 Token **只能关注同一文档内的先前 Token**。下方可视化图（*注：原文指代的图表*）展示了这种差异。

Zhu et al. (2025) 在 SkyLadder 中也发现了文档内掩码的类似益处，但提供了另一种解释。他们发现**较短的上下文长度对训练更有利**，而文档内掩码有效地**降低了平均上下文长度**。

<!-- ![alt text](image-3.png) -->

Llama 3 (Grattafiori et al., 2024) 也使用文档内掩码进行训练，他们发现在短上下文预训练期间影响有限，但对于**长上下文扩展**来说益处显著，因为在那种情况下注意力开销变得更加重要。此外，ProLong 论文 (Gao et al., 2025) 表明，在持续预训练中利用文档掩码来扩展 Llama 3 8B 的上下文，对**长上下文和短上下文的基准都有益处**。

我们决定在我们的 1B 基线模型上进行一项消融实验，测试文档掩码是否会影响短上下文性能。你可以在这里找到配置。

结果显示，与标准因果掩码相比，Loss 曲线和下游评估得分**完全相同**（如下方图表所示）。我们唯一观察到的一个微小改进是在 **PIQA** 上。

要在 nanotron 中启用文档掩码，只需在模型配置中将以下标志设置为 `true`：

```yaml
model_config:
  _attn_implementation: flash_attention_2
  _fused_rms_norm: true
  _fused_rotary_emb: true
- _use_doc_masking: false
+ _use_doc_masking: true  # 启用文档内掩码
```

<!-- ![alt text](image-4.png)
![alt text](image-5.png) -->

与 Llama 3 类似，我们没有在短上下文任务上观察到明显的性能影响，除了 PIQA 的微小改进。然而，文档掩码在扩展到长序列时变得**至关重要**，因为它可以**加快训练速度**。这对于我们的长上下文扩展尤其重要，我们将序列从 4k 扩展到 64k Token（详情请见“训练马拉松”章节）。因此，我们在 SmolLM3 的整个训练过程中都采用了它。

在本节中，我们讨论了注意力如何处理序列。现在，让我们看看 Transformer 中的另一个主要参数块：**嵌入层（Embeddings）**。

#### 4.1.2 嵌入层共享（Embedding Sharing）

如果你观察我们基线消融模型的配置，你会发现与标准 Transformer 不同的地方之一是，它通过 **`tie_word_embeddings`** 标志启用了**嵌入层共享**。

LLM 有两个嵌入组件：

1.  **输入嵌入（Input Embeddings）：** 作为 Token 到向量的查找表（大小为 $\text{vocab\_size} \times \text{hidden\_dim}$）。
2.  **输出嵌入（Output Embeddings）：** 最后一个线性层，将隐藏状态映射到词汇表 Logits（大小为 $\text{hidden\_dim} \times \text{vocab\_size}$）。

在经典情况下，如果这两个矩阵是分开的，总嵌入参数量为 $2 \times \text{vocab\_size} \times \text{hidden\_dim}$。因此，在小型语言模型中，嵌入层可以占据总参数量的很大一部分，尤其是在词汇表大小很大的情况下。这使得**嵌入层共享**（在输出层重用输入嵌入）成为小型模型的一种自然优化。

大型模型通常不使用这种技术，因为嵌入层只占其参数预算的一小部分。例如，Llama 3.2 8B 中不共享的总嵌入参数仅占 13%，而在 Llama 3.1 70B 中仅占 3%。

##### 4.1.2.1 消融实验 - 绑定嵌入的模型可媲美参数量更大的非绑定变体

现在我们将评估**嵌入层共享**对我们消融模型的影响。我们借鉴了 **MobileLLM** 在 **125M 规模**上对该技术的全面消融实验所获得的洞察，该实验表明：共享嵌入在**参数量减少 11.8%** 的同时，**准确率退化极小**。

由于非绑定嵌入会将我们的参数量从 1.2B 增加到 1.46B，我们将训练另一个具有非绑定参数但**层数更少**的模型，使其参数量与基线的 1.2B 相匹配。我们将比较三个模型：

1.  **基线模型 (1.2B)：** 绑定嵌入（16 层）。
2.  **非绑定-减少层数模型 (1.2B)：** 非绑定嵌入，但层数更少（12 层）以保持相同的参数预算。
3.  **非绑定-相同层数模型 (1.46B)：** 非绑定嵌入，层数与基线相同（16 层），作为额外的参考点。

Loss 和评估结果表明，我们的 **1.2B 绑定嵌入基线模型**，在所有基准测试（WinoGrande 除外）上，都实现了与 **1.46B 非绑定等效模型**相当的性能，尽管其参数少了 **18%**。

而**非绑定嵌入且层数减少的 1.2B 模型**（12 层对比 16 层）**性能不如前两者**，表现出更高的 Loss 和更低的下游评估分数。这表明，在参数预算相等的情况下，**增加模型深度**比**解绑嵌入层**带来了更大的益处。

基于这些结果，我们为 **SmolLM3 3B 模型**保留了**绑定嵌入（Tied Embeddings）**。

至此，我们探索了嵌入层共享策略及其权衡。但仅靠嵌入层本身并不能捕获序列中 Token 的顺序；提供这些信息是**位置编码（Positional Encodings）**的作用。在下一节中，我们将探讨位置编码策略是如何演变的，从标准的 RoPE 到像 **NoPE (No Position Embedding)** 这样更新颖的方法，后者能更有效地进行长上下文建模。

#### 4.1.3 位置编码与长上下文（Positional Encodings & Long Context）

当 Transformer 处理文本时，它们面临一个根本性的挑战：它们天生对词序没有感知，因为它们通过并行注意力操作同时处理整个序列。这使得训练高效，但产生了一个问题。在没有明确的位置信息的情况下，从模型的角度来看，“亚当击败了穆恩”和“穆恩击败了亚当”看起来是相似的。

解决方案是**位置嵌入（Positional Embeddings）**：一种数学编码，赋予序列中的每个 Token 一个独特的“地址”。但是，随着我们不断将上下文推向**更长**——从早期 BERT 的 512 个 Token 到今天的百万级 Token 模型——位置编码的选择对于**性能和计算效率**变得越来越关键。

##### 4.1.3.1 位置编码的演变

早期的 Transformer 使用简单的**绝对位置嵌入（Absolute Position Embeddings, APE）** (Vaswani et al., 2023)，它本质上是一个学习到的查找表，将每个位置 $(1, 2, 3...)$ 映射到一个向量，然后将其添加到 Token 嵌入中。这对于短序列运行良好，但有一个主要限制：模型的最大输入序列长度被限制在它所训练的最大长度内。它们不具备**开箱即用**的泛化到更长序列的能力。

该领域转向了**相对位置编码（Relative Position Encodings）**，它捕获的是 Token 之间的**距离**，而不是它们的绝对位置。这在直觉上是合理的：两个词相隔 3 个位置，比它们是处于位置 (5, 8) 还是 (105, 108) 更重要。

**ALiBi (Attention with Linear Biases)** (Press et al., 2022) 通过 Token 距离修改注意力得分。两个 Token 距离越远，它们的注意力得分就会通过应用于注意力权重的简单线性偏差受到越大的惩罚。

但主导近期大型语言模型的技术是**旋转位置嵌入（Rotary Position Embedding, RoPE）** (Su et al., 2023)。

##### 4.1.3.2 RoPE：将位置编码为旋转

RoPE 的核心洞察是：将位置信息编码为**高维空间中的旋转角度**。RoPE 不是将位置向量添加到 Token 嵌入中，而是通过**依赖于其绝对位置的角度**来旋转查询（Query）和键（Key）向量。

其直觉是，我们将嵌入向量中的每对维度视为圆上的坐标，并根据以下因素确定的角度旋转它们：

*   Token 在序列中的位置 $p$。
*   我们正在处理的维度对 $k$（不同的维度对以不同的频率旋转，这些频率是基础/参考频率的指数）。

*(此处省略了原文提供的 RoPE 简化 Python 代码，以保持文章的可读性，读者可以直接查阅原文代码。)*

这个代码可能看起来复杂，所以让我们用一个具体的例子来分解它。考虑句子 “The quick brown fox” 中的“fox”这个词。在我们 1B 基线模型中，每个注意力头都使用一个 64 维的查询/键向量。RoPE 将这个向量分成 32 对：$(x_1, x_2), (x_3, x_4), (x_5, x_6)$，依此类推。我们对对进行操作，因为我们在二维空间中围绕圆旋转。为了简单起见，我们关注第一对 $(x_1, x_2)$。词“fox”出现在句子中的位置 3，因此 RoPE 将旋转第一对维度：

$$\text{rotation\_angle} = \text{position} \times \theta_0 = 3 \times \left(1 / 10000^{(0/32)}\right) = 3 \times 1.0 = 3.0 \text{ 弧度} = 172^{\circ}$$

我们的基础频率是 10000，但对于第一对维度 ($k=0$)，指数为零，因此基础频率不影响计算（我们将其提高到 0 次方）。

现在，当两个 Token 通过注意力相互作用时，奇迹发生了。它们的旋转表示之间的**点积（Dot Product）** 直接通过它们旋转角度之间的**相位差（Phase Difference）** 来编码它们的相对距离（其中 $m$ 和 $n$ 是 Token 的位置）：

$$\text{dot\_product}(\text{RoPE}(x, m), \text{RoPE}(y, n)) = \sum_{k} \left[x_k \times y_k \times \cos((m-n) \times \theta_k)\right]$$

注意力模式**仅取决于 $(m-n)$**，因此相隔 5 个位置的 Token 将始终具有相同的角度关系，无论它们在序列中的绝对位置如何。因此，模型学习了**基于距离的模式**，这些模式适用于序列中的任何绝对位置，并可以**外推到更长的序列**。

##### 4.1.3.3 如何设置 RoPE 频率？

在实践中，大多数 LLM 预训练都是从相对较短的上下文长度（2K-4K Token）开始的，使用的 RoPE 基础频率是几万，例如 $10\text{K}$ 或 $50\text{K}$。从一开始就用非常长的序列进行训练，由于注意力机制的**二次方缩放**以及**长上下文数据**（上下文长度超过 4K 的样本）的有限可用性，计算成本会非常高昂。研究还表明，这可能会**损害短上下文性能** (Zhu et al., 2025)。模型通常从学习词语之间的短程相关性开始，因此长序列帮助不大。

典型的方法是**用较短的序列完成大部分预训练**，然后进行**持续预训练（Continual Pretraining）**，或在最后几千亿 Token 上使用**更长的序列**。然而，随着序列长度的增长，与 Token 位置成正比的旋转角度也会增长，这可能导致**远处 Token 的注意力得分衰减过快** (Rozière et al., 2024; Xiong et al., 2023)：

$$\theta = \text{position} \times 1 / (\text{base}^{(k/(\text{dim}/2))})$$

解决方案是，随着序列长度的增加而**增加基础频率**，以防止这种衰减，使用诸如 **ABF** 和 **YaRN** 之类的方法。

*   **RoPE ABF (RoPE with Adjusted Base Frequency)** (Xiong et al., 2023b)：通过**增加 RoPE 公式中的基础频率**来解决长上下文中的注意力衰减问题。这种调整减慢了 Token 位置之间的旋转角度，防止了远处 Token 的注意力得分衰减过快。ABF 可以单阶段应用（直接提升频率）或多阶段应用（随着上下文增长而逐渐增加）。该方法易于实现，并以增加的粒度分布嵌入向量，使模型更容易区分远距离位置。虽然简单有效，但 ABF 对所有维度的统一缩放可能不适用于**极长上下文**。

*   **YaRN (Yet another RoPE extensioN)** (Peng et al., 2023)：采取了一种更复杂的方法，通过使用**斜坡或缩放函数**在 RoPE 维度上**不均匀地插值频率**。与 ABF 的统一调整不同，YaRN 对不同的频率分量应用不同的缩放因子，从而优化了扩展的上下文窗口。它包括动态注意力缩放和注意力 Logits 中的温度调整等额外技术，有助于在极大的上下文尺寸下保持性能。YaRN 支持高效的 **“短训练，长测试”策略**，只需要更少的 Token 和更少的微调即可实现稳健的外推。尽管比 ABF 更复杂，但 YaRN 通过提供更平滑的缩放和减轻灾难性的注意力损失，通常为**极长上下文**带来更好的经验性能。它也可以单独在推理中使用，无需任何微调。

这些频率调整方法**减缓了注意力得分衰减效应**，并保持了远程 Token 的贡献。例如，Qwen3 的训练就涉及在将序列长度从 4k 扩展到 32k 上下文时，使用 ABF 将频率从 10k 增加到 1M（该团队随后应用 YaRN 来达到 131k，即 4 倍外推）。

请注意，对于最佳值目前没有强烈的共识，通常最好在**上下文扩展阶段**尝试不同的 RoPE 值，以找到最适合你特定设置和评估基准的值。

今天大多数主要的模型都使用 **RoPE**：Llama、Qwen、Gemma 等等。该技术已被证明在不同模型大小和架构（密集型、MoE、混合型）中都稳健可靠。

##### 4.1.3.4 混合位置编码方法（Hybrid Positional Encoding Approaches）

然而，随着模型推向越来越大的上下文 (Meta AI, 2025; Yang et al., 2025)，即使是 RoPE 也开始遇到性能挑战。当在比 **Needle in the Haystack (NIAH)** (Kamradt, 2023) 更具挑战性的长上下文基准（如 Ruler 和 HELMET (Hsieh et al., 2024; Yen et al., 2025)）上进行评估时，在长上下文扩展期间增加 RoPE 频率的标准方法存在局限性。一些更新的技术被引入来提供帮助。

我们以 Transformer 需要位置信息来理解 Token 顺序开始本节，但最近的研究挑战了这一假设。**如果明确的位置编码根本不是必需的呢？**

**NoPE (No Position Embedding)** (Kazemnejad et al., 2023) 在**没有任何明确位置编码**的情况下训练 Transformer，允许模型通过**因果掩码和注意力模式**隐式学习位置信息。作者表明，与 ALiBi 和 RoPE 相比，这种方法表现出更好的**长度泛化能力**。由于没有明确的位置编码来外推训练长度之外，NoPE 自然可以处理更长的上下文。然而在实践中，与 RoPE 相比，NoPE 模型在**短上下文推理和知识任务上表现较弱** (Yang et al.)。这表明，虽然明确的位置编码可能会限制外推，但它们为训练上下文长度内的任务提供了有用的**归纳偏置（inductive biases）**。

**RNoPE 混合方法：** 考虑到这些权衡，B. Yang et al. (2025) 提出结合不同的位置编码策略可能很有趣。他们引入了 **RNoPE**，它在整个模型中交替使用 **RoPE 和 NoPE 层**。RoPE 层提供明确的位置信息，并以近因偏置（recency bias）处理局部上下文，而 NoPE 层则改善了跨长距离的信息检索。这项技术最近被用于 **Llama 4、Command A 和 SmolLM3**。

##### 4.1.3.5 消融实验 - NoPE 在短上下文上与 RoPE 匹配

让我们测试一下混合的 NoPE 方法。我们将比较一个**纯 RoPE 1B 消融基线**、一个**每隔 4 层移除位置编码的 NoPE 变体**，以及**结合 NoPE 和文档掩码**的第三种配置来测试这些技术之间的相互作用。我们的基本问题是：**我们能否在保持强大的短上下文性能的同时，获得长上下文能力？**

Loss 和评估结果显示，所有三种配置的性能**相似**，这表明 NoPE **保持了强大的短上下文能力**，同时为更好的长上下文处理提供了基础。基于这些结果，我们为 **SmolLM3 采用了 NoPE + 文档掩码的组合**。

**部分/分数 RoPE（Partial/Fractional RoPE）：** 另一个互补的想法是**只在模型维度的一个子集上应用 RoPE**。与 RNoPE 在整个层面上交替使用 RoPE 和 NoPE 不同，Partial RoPE 在**同一层内混合**它们。最近的模型，如 GLM-4.5 (5 Team et al., 2025) 或 Minimax-01 (MiniMax et al., 2025)，采用了这种策略，但这在 gpt-j (Wang & Komatsuzaki, 2021) 等较旧的模型中也存在。你也会在每个使用 **MLA** 的模型中看到这一点，因为它是拥有合理推理成本的**必备条件**。

##### 4.1.3.6 限制注意力范围以实现长上下文

到目前为止，我们探索了如何处理长上下文的位置信息：启用 RoPE、禁用它 (NoPE)、在某些层上部分应用 (RNoPE) 或在某些隐藏维度上应用 (Partial RoPE)，或调整其频率 (ABF, YaRN)。这些方法修改了模型编码位置的方式，以处理比训练期间看到的序列更长的序列。

但还有一种**互补的策略**：**我们可以限制哪些 Token 相互关注，而不是调整位置编码。**

为了理解为什么这很重要，考虑一个用 8 个 Token 序列预训练的模型。在推理时，我们想处理 16 个 Token（超过训练长度）。位置 8-15 超出了模型位置编码的分布范围。虽然像 RoPE ABF 这样的技术通过调整位置频率来解决这个问题，但**注意力范围方法**采取了不同的方法：它们**策略性地限制**哪些 Token 可以相互关注，将注意力模式保持在熟悉的范围内，同时仍然处理整个序列。这降低了计算成本和内存需求。

下方的图表比较了处理我们的 16 个 Token 序列（预训练窗口为 8）的五种策略：
<!-- ![alt text](image-6.png) -->

*   **分块注意力（Chunked Attention）** 将序列分成固定大小的块，其中 Token **只能在自己的块内关注**。在我们的示例中，16 个 Token 被分成两个 8 个 Token 的块（0 到 7 和 8 到 15），每个 Token 只能看到其块内的其他 Token。注意 Token 8 到 15 根本不能关注到前面的块。这创建了在块边界重置的**隔离注意力窗口**。Llama 4 (Meta AI, 2025) 在 RoPE 层（四分之三的解码器层）中使用了 8192 个 Token 块的分块注意力，而 NoPE 层则保持对完整上下文的访问。这通过限制每层的 KV 缓存大小来减少内存需求，尽管它意味着 Token 不能关注到前面的块，这可能会影响某些长上下文任务。

*   **滑动窗口注意力（Sliding Window Attention, SWA）**，由 Mistral 7B (Child et al., 2019; Jiang et al., 2023) 推广，基于**最近的 Token 最相关**的直觉，采用了不同的方法。每个 Token 只关注**最近的 N 个 Token**，而不是硬性的块边界。在图表中，每个 Token 最多可以看到前面 8 个位置，创建了一个在序列中连续移动的**滑动窗口**。注意 Token 15 可以关注位置 8 到 15，而 Token 10 关注位置 3 到 10。窗口向前滑动，在整个序列中保持局部上下文，没有分块的人为障碍。Gemma 3 将 SWA 与完整注意力在交替层中结合使用，类似于混合位置编码方法混合不同策略的方式。

*   **双块注意力（Dual Chunk Attention, DCA）** (An et al., 2024) 是一种**免训练**的方法，它扩展了分块注意力，同时保持了跨块的信息流。在我们的示例中，我们使用块大小 $s=4$，将 16 个 Token 分为 4 个块（沿对角线可视化 $4 \times 4$ 方格）。DCA 结合了三种机制：(1) **块内注意力**，Token 在其块内正常关注（对角线模式）。(2) **块间注意力**，查询使用位置索引 $c-1=7$ 来关注前面的块，创建相对位置上限为 7。(3) **连续块注意力**，具有局部窗口 $w=3$，保留相邻块之间的局部性。这使得所有相对位置都保持在训练分布内（0 到 7），同时保持跨块边界的平滑过渡。DCA 使 Qwen 2.5 等模型能够在推理时支持高达 **100 万 Token** 的超长上下文窗口，而无需在百万 Token 序列上进行持续训练。

##### 4.1.3.7 注意力汇聚（Attention Sinks）

在具有长上下文的 Transformer 模型中，出现了一种有趣的现象：模型会为序列中的**起始 Token 分配异常高的注意力分数**，即使这些 Token 在语义上并不重要。这种行为被称为**注意力汇聚（Attention Sinks）** (Xiao et al.)。这些起始 Token 充当了注意力分布的**稳定机制**，起到了注意力可以积累的“汇聚点”作用。

实际的洞察是：当上下文长度超过缓存大小时，**仅保留起始几个 Token 的 KV 缓存**以及**最近 Token 的滑动窗口**，可以在很大程度上**恢复性能**。这种简单的修改使模型能够处理更长的序列，而无需微调或性能下降。

现代的实现以不同的方式利用注意力汇聚。最初的研究建议在预训练期间**添加一个专用的占位符 Token** 作为明确的注意力汇聚点。最近，像 **gpt-oss** 这样的模型将注意力汇聚实现为**学习到的“每头偏差 Logits”（learned per-head bias logits）**，将其附加到注意力分数上，而不是作为输入序列中的实际 Token。这种方法在不修改分词输入的情况下达到了相同的稳定效果。

有趣的是，gpt-oss 还在注意力层本身使用了**偏差单元（bias units）**，这是自 GPT-2 以来就很少见的设计选择。虽然这些偏差单元通常被认为对于标准注意力操作是多余的（Dehghani et al. 的实证结果显示对测试 Loss 的影响很小），但它们可以服务于**实现注意力汇聚的专门功能**。

核心洞察是：无论是作为**特殊 Token、学习到的偏差**还是**每头 Logits** 实现，注意力汇聚都为长上下文场景中的注意力分布提供了一个**稳定的“锚点”**，允许模型存储关于**整个序列的通用有用信息**，即使上下文任意增长。

至此，我们已经涵盖了注意力的核心组件：平衡内存和计算的不同头部配置（MHA、GQA、MLA），帮助模型理解 Token 顺序的位置编码策略（RoPE、NoPE 及其变体），以及使长上下文变得可处理的注意力范围技术（滑动窗口、分块和注意力汇聚）。我们还研究了嵌入层应如何配置和初始化。这些架构选择定义了你的模型如何处理和表示序列。

但拥有正确的架构只是成功的一半。即使是精心设计的模型，也可能遭受训练不稳定的困扰，尤其是在大规模训练时。让我们来看看有助于保持训练稳定的技术。

#### 4.1.4 提高稳定性（Improving Stability）

现在，让我们转向 LLM 预训练中最大的挑战之一：**不稳定性（Instabilities）**。这些问题通常表现为 Loss 尖刺或训练 Loss 的突然跳跃，在大规模训练时尤为常见。

虽然我们将在“训练马拉松”部分深入探讨不同类型的尖刺以及如何处理它们（深入研究浮点精度、优化器和学习率），但某些架构和训练技术也可以帮助我们减少不稳定性。因此，让我们花点时间在这里研究它们。

我们将介绍最近大规模训练运行中（例如，Olmo2 (OLMo et al., 2025) 和 Qwen3 (A. Yang, Li, et al., 2025)）用于提高稳定性的几种简单技术：**Z-Loss、从嵌入层中移除权重衰减**和 **QK-norm**。

##### 4.1.4.1 Z-loss

**Z-loss** (Chowdhery et al., 2022) 是一种正则化技术，它通过**在损失函数中添加一个惩罚项**来防止最终输出的 Logits 变得过大。这种正则化促使 Softmax 的分母（即 $Z$ 项）保持在一个合理的范围内，这有助于在训练过程中保持**数值稳定性**。

下方在我们的 1B 模型上进行的消融结果（*注：指代原文图表*）显示，添加 Z-loss 不会影响训练 Loss 或下游性能。对于 SmolLM3，我们最终没有使用它，因为在开始训练时，我们的 Z-loss 实现引入了一些**我们没有优化的训练开销**。

##### 4.1.4.2 从嵌入层中移除权重衰减

权重衰减（Weight Decay）通常作为一种正则化技术应用于所有模型参数，但 OLMo et al. (2025) 发现，**将嵌入层排除在权重衰减之外可以提高训练稳定性**。

其推理是：权重衰减会导致嵌入层范数在训练过程中逐渐减小，这可能导致早期层中的**梯度变大**，因为层归一化（Layer Normalization）的雅可比矩阵与输入范数成**反比** (Takase et al., 2025)。

我们通过训练三种配置来测试这种方法：

1.  **基线模型：** 使用标准权重衰减。
2.  **变体模型：** 嵌入层无权重衰减。
3.  **组合模型：** 结合我们所有已采纳的更改（嵌入层无权重衰减 + NoPE + 文档掩码），以确保技术之间没有负面相互作用。

Loss 曲线和评估结果在所有三种配置中**几乎相同**。因此，我们在 SmolLM3 训练中**采用了所有这 3 项更改**。

##### 4.1.4.3 QK-norm

**QK-norm** (Dehghani et al., 2023) 在计算注意力之前，**对查询（Query）和键（Key）向量同时应用层归一化（Layer Normalization）**。这项技术有助于防止注意力 Logits 变得过大，并被许多最近的模型用于提高稳定性。

然而，B. Yang et al. (2025) 发现 **QK-norm 会损害长上下文任务**。他们的分析显示，QK-norm 导致相关 Token（即“针”）上的注意力质量降低，而**不相关上下文上的注意力质量增高**。他们认为发生这种情况是因为归一化操作消除了`query-key`点积中的**幅度信息（magnitude information）**，这使得注意力 Logits 的幅度更接近。

由于这个原因，我们**没有在 SmolLM3 中使用 QK-norm**。此外，作为一个小的 30 亿参数模型，与那些 QK-norm 已被证明最有益处的更大模型相比，它面临的训练不稳定性风险也较小。

#### 4.1.5 其他核心组件（Other Core Components）

除了我们已经涵盖的组件之外，还有一些其他值得一提的架构决策，以求完整性。

*   **参数初始化（Initialization）：** 现代模型通常使用**截断正态分布初始化**（均值=0，标准差 $\text{std}=0.02$ 或 $\text{std}=0.006$），或者像 **$\mu\text{P}$** (G. Yang & Hu, 2022) 这样的初始化方案，例如 Cohere 的 Command A (Cohere et al., 2025)。这也可以是另一个消融实验的主题。

*   **激活函数（Activation Functions）：** **SwiGLU** 已成为现代 LLM 中的事实标准（除了使用 GeGLU 的 Gemma2 和使用 $\text{relu}^2$ 的 Nvidia (Nvidia et al., 2024; NVIDIA et al., 2025)），取代了像 ReLU 或 GELU 这样较旧的选择。

*   **架构布局（Architectural Layout）：** 从更宏观的层面看，架构布局的选择也对模型的行为起着作用。尽管**总参数数量**在很大程度上决定了语言模型的容量，但这些参数如何分布在**深度（depth）和宽度（width）**上也至关重要。Petty et al. (2024) 发现，在语言建模和组合性任务上，**更深的模型**比参数量相等的**更宽的模型**表现更优，直到这种益处达到饱和。**“深而薄”**的策略在 MobileLLM 的子十亿参数 LLM 消融实验中表现良好 (Z. Liu et al., 2024)，而**更宽的模型**由于具有更大的并行性，往往能提供更快的推理速度。现代架构以不同的方式体现了这种权衡取舍，正如本文所指出的。

至此，我们已经涵盖了值得为你的训练运行进行优化的**密集型 Transformer 架构**的**最重要方面**。

然而，最近也出现了涉及模型整体的其他架构干预，即 **MoE（专家混合）模型**和**混合（Hybrid）模型**。让我们来看看它们能提供什么，从 MoE 开始。

#### 4.1.6 走向稀疏：专家混合模型（MoE）

专家混合模型（MoE）的直觉是：我们不需要为了每一个 Token 的预测都用到整个模型，这类似于我们的大脑会根据手头的任务激活不同的区域（例如视觉或运动皮层）。对于一个 LLM 来说，这意味着模型在执行翻译任务时，那些学习了代码语法的组件就不需要被使用。如果能做好这一点，就意味着我们可以节省大量的计算资源，因为在推理时我们只需要运行**部分模型**。

在技术层面上，MoE 的目标很简单：**增加总参数量，同时不增加每个 Token 的“活跃”参数量**。简化来说，**总参数量**影响模型的总体学习容量，而**活跃参数量**决定了训练成本和推理速度。这就是为什么现在许多前沿系统（例如 DeepSeek V3、K2，以及 Gemini、Grok 等闭源模型）都在使用 MoE 架构。如果你是第一次接触 MoE，不用担心，其机制并不复杂。让我们从标准的密集型架构开始，看看 MoE 所需的必要改变。在 MoE 中，我们将单个 MLP 替换为多个 MLP（**“专家 Experts”**），并在这些 MLP 之前添加一个**可学习的路由器（Router）**。对于每个 Token，路由器会选择一小部分专家来执行计算。**总参数量**和**活跃参数量**的区别就源于此：模型拥有很多专家，但任何给定的 Token 只使用其中少数几个。

设计 MoE 层会引出几个核心问题：

1.  **专家形态与稀疏度：** 你应该使用**许多小型专家**还是**少数大型专家**？每个 Token 应该有多少专家是**活跃的**？你总共需要多少专家（即**稀疏度**或“top-k”）？是否应该让某些专家成为 **通用型（universal）** 专家而始终保持活跃？
2.  **利用率与专业化：** 如何选择路由的专家，并确保它们被充分利用（避免闲置容量），同时鼓励它们实现**专业化**？在实践中，这是一个 **负载均衡（load-balancing）** 问题，对训练和推理效率有着重要影响。

在这里，我们专注于一个目标：**给定固定的计算预算，我们如何选择一个能将 Loss 降到最低的 MoE 配置？** 这是一个不同于纯系统效率（吞吐量/延迟）的问题，我们稍后会再讨论后者。本节的大部分内容遵循蚂蚁集团 MoE 扩展定律论文 (Tian et al., 2025) 中的分析。我们将使用他们提出的**效率杠杆（Efficiency Leverage, EL）**的概念。简单来说，EL 衡量了你需要多少**密集型计算**才能匹配 MoE 设计所达到的 Loss，衡量单位是 **FLOPs**。**更高的 EL** 意味着与密集型训练相比，该 MoE 配置每单位计算能带来更多的 Loss 改进。让我们仔细看看如何设置 MoE 的稀疏度来提高效率杠杆。

##### 4.1.6.1 稀疏度/激活比率（Sparsity / Activation Ratio）

在本节中，我们想找出哪种 MoE 设置是最佳的。渐进地来看，很容易看出两个极端情况都不是理想的设置：

*   一方面，始终激活所有专家会使我们回到**密集型设置**，即所有参数始终被使用。
*   另一方面，如果活跃参数非常少（极端情况下只激活 1 个参数），显然不足以解决任务，即使是在一个狭窄的领域。

因此，我们显然需要找到一个**中间点**。在我们深入寻找最佳设置之前，定义两个量是有用的：**激活比率（activation ratio）**及其倒数**稀疏度（sparsity）**：

$$\text{激活比率} = \frac{\text{激活的专家数量}}{\text{总专家数量}}$$

$$\text{稀疏度} = \frac{\text{总专家数量}}{\text{激活的专家数量}} = \frac{1}{\text{激活比率}}$$

从计算的角度来看，成本仅由**活跃参数**驱动。如果你保持活跃专家的数量（和大小）固定，并增加专家的总数，你的推理/训练 FLOPs 预算大致保持不变，但你增加了模型的容量，因此只要训练时间足够长，模型通常会变得更好。

如果你对最近的 MoE 论文进行调查，会发现一些有趣的经验性结论：**在固定活跃专家的数量和大小的情况下，增加专家的总数（即降低激活比率/增加稀疏度）可以改善 Loss，但当稀疏度变得非常高时，回报会递减。**

两个例子：

*   **Kimi K2** (K. Team et al., 2025)：显示了两种效应：更高的稀疏度提高了性能，但随着稀疏度的增长，收益会逐渐减少。
*   **蚂蚁集团** (Tian et al., 2025)：与 K2 得出相同结论，并额外指出**稀疏度越高的 MoE 从增加计算中获得的益处越多**。

下面是一些 MoE 模型的稀疏度表格：

| 模型 | 总专家数量 | 每个 Token 激活数量（含共享） | 稀疏度 |
| :--- | :--- | :--- | :--- |
| Mixtral-8x7B | 8 | 2 | 4.0 |
| Grok-1 | 8 | 2 | 4.0 |
| Grok-2 | 8 | 2 | 4.0 |
| OLMoE-1B-7B-0924 | 64 | 8 | 8.0 |
| gpt-oss 20b | 32 | 4 | 8.0 |
| Step-3 | 48 路由 + 1 共享 = 49 | 3 路由 + 1 共享 = 4 | 12.25 |
| GLM-4.5-Air | 128 路由 + 1 共享 = 129 | 8 路由 + 1 共享 = 9 | 14.3 |
| Qwen3-30B-A3B | 128 | 8 | 16.0 |
| Qwen3-235B-A22B | 128 | 8 | 16.0 |
| GLM-4.5 | 160 路由 + 1 共享 = 161 | 8 路由 + 1 共享 = 9 | 17.8 |
| DeepSeek-V2 | 160 路由 + 2 共享 = 162 | 6 路由 + 2 共享 = 8 | 20.25 |
| DeepSeek-V3 | 256 路由 + 1 共享 = 257 | 8 路由 + 1 共享 = 9 | 28.6 |
| gpt-oss 120b | 128 | 4 | 32.0 |
| Kimi K2 | 384 路由 + 1 共享 = 385 | 8 路由 + 1 共享 = 9 | 42.8 |
| Qwen3-Next-80B-A3B-Instruct | 512 路由 + 1 共享 = 513 | 10 总激活 + 1 共享 = 11 | 46.6 |

最近的趋势很明显：**MoE 模型正变得越来越稀疏**。尽管如此，最佳稀疏度仍然取决于硬件和端到端效率。例如，Step-3 旨在达到峰值效率，并有意不将稀疏度最大化，以适应其特定的硬件和带宽约束，而 gpt-oss-20b 的稀疏度较低则是由于设备上的内存限制（被动专家仍然占用一些内存）。

##### 4.1.6.2 粒度（Granularity）

除了稀疏度之外，我们还需要决定**每个专家应该有多大**。这由蚂蚁集团引入的 **粒度（Granularity）** 来捕获。让我们明确这个术语的含义。术语在不同论文中有所不同，有些使用略有不同的公式。在这里，我们将使用与我们引用的图表相匹配的定义：

$$G = \alpha \times \frac{d_{model}}{d_{expert}}$$

**更高的粒度值**对应于拥有 **更多具有更小维度（$d_{expert}$）的专家**（给定固定的参数数量）。该指标是 **专家维度（$d_{expert}$）** 与 **模型维度（$d_{model}$）** 之间比值的倒数（乘上归一化系数 $\alpha$）。

在密集型模型中，一个常用的经验法则是将 MLP 的维度设置为 $d_{intermediate} = 4 \times d_{model}$。如果 $\alpha=4$（像 Krajewski et al. (2024)），你可以大致将粒度视为**匹配密集型 MLP 宽度所需的专家数量**（$4 \times d_{model} = d_{intermediate} = G \times d_{expert}$）。

这种解释只是一个粗略的启发式：现代 MoE 设计通常分配比单个密集型 MLP 大得多的总容量，因此一对一匹配在实践中会失效。蚂蚁团队的设置选择了 $\alpha=2$，这只是一种不同的归一化选择。为了保持一致性，我们将采用这种约定并坚持下去。

以下是一些 MoE 版本的不同粒度值表格：

| 模型 | $d_{model}$ | $d_{expert}$ | $G = 2 \times d_{model} / d_{expert}$ | 年份 |
| :--- | :--- | :--- | :--- | :--- |
| Mixtral-8x7B | 4,096 | 14,336 | 0.571 | 2023 |
| gpt-oss-120b | 2,880 | 2,880 | 0.5 | 2025 |
| gpt-oss-20b | 2,880 | 2,880 | 0.5 | 2025 |
| Grok 2 | 8,192 | 16,384 | 1.0 | 2024 |
| StepFun Step-3 | 7,168 | 5,120 | 2.8 | 2025 |
| OLMoE-1B-7B | 2,048 | 1,024 | 4.0 | 2025 |
| Qwen3-30B-A3B | 2,048 | 768 | 5.3 | 2025 |
| Qwen3-235B-A22B | 4,096 | 1,536 | 5.3 | 2025 |
| GLM-4.5-Air | 4,096 | 1,408 | 5.8 | 2025 |
| DeepSeek V2 | 5,120 | 1,536 | 6.6 | 2024 |
| GLM-4.5 | 5,120 | 1,536 | 6.6 | 2025 |
| Kimi K2 | 7,168 | 2,048 | 7.0 | 2025 |
| DeepSeek V3 | 7,168 | 2,048 | 7.0 | 2024 |
| Qwen3-Next-80B-A3B | 2,048 | 512 | 8.0 | 2025 |

让我们谈谈**粒度如何影响行为**（来自蚂蚁集团的论文）：

粒度看起来**不是 EL 的主要驱动因素**——它有帮助，尤其是当粒度超过 2 时，但它不是决定 Loss 的主导因素。不过，存在一个**最佳点**：提高粒度到一定程度会有帮助，然后收益就会趋于平稳。因此，粒度是一个有用的调整旋钮，最近发布的趋势明显倾向于更高的值，但**不应该孤立地进行优化**。

另一种广泛用于改进 MoE 的方法是 **共享专家（Shared Experts）** 的概念。让我们来看看！

##### 4.1.6.3 共享专家（Shared Experts）

**共享专家**设置将**每个 Token** 路由到**一组始终处于激活状态**的专家。这些共享专家吸收数据中**基本、重复的模式**，以便其余的专家可以更积极地进行**专业化**。

在实践中，你通常不需要很多共享专家；模型设计者通常选择**一个**，最多**两个**。随着粒度的增加（例如，从 Qwen3 风格的设置转向更接近 Qwen3-Next 的设置），共享专家往往变得更有用。从下方的图表来看，总体影响是**适度的**，它不会戏剧性地改变 EL。一个简单的经验法则是**只使用一个共享专家**，这与 DeepSeek V3、K2 和 Qwen3-Next 等模型的选择相符，并且倾向于在不增加不必要复杂性的情况下最大化效率。

那么，共享专家就是**所有 Token 都会始终路由通过**的专家。那其他专家呢？我们如何学习何时路由到每个专家，并确保我们不会只使用少数几个专家？接下来我们将讨论**负载均衡**，它正是解决这个问题的。

##### 4.1.6.4 负载均衡（Load Balancing）

**负载均衡是 MoE 中的关键环节。** 如果设置不当，它可能会破坏所有其他设计选择。通过以下示例，我们可以看到为什么糟糕的负载均衡会给我们带来很多痛苦。

考虑一个非常简单的分布式训练设置，我们有 4 块 GPU，并将模型的 4 个专家均匀地分布在这些 GPU 上。如果路由崩溃，所有 Token 都被路由到专家 1，这意味着**只有 1/4 的 GPU 被利用**，这对训练和推理效率来说非常糟糕。除此之外，这也意味着模型的**有效学习容量**也降低了，因为并非所有专家都被激活。

为了解决这个问题，我们可以**向路由器添加一个额外的损失项**。你可以在下方看到标准的 **基于辅助损失（auxiliary loss–based）** 的负载均衡 (LBL)：

$$\text{Loss}_{LBL} = \alpha \times \sum_{i} f_i \times P_i$$

这个简单的公式只使用了三个因子：

*   系数 $\alpha$ 决定了损失的强度。
*   $f_i$ 是**流量分数（traffic fraction）**，即流经专家 $i$ 的 Token 分数。
*   $P_i$ 是**概率质量（probability mass）**，简单地是流经该专家的 Token 概率总和。

两者都是必需的：$f_i$ 对应于**实际的均衡**，而 $P_i$ 是**平滑且可微分的**，允许梯度流动。如果我们实现了完美的负载均衡，我们会得到 $f_i = P_i = 1/N_r$。然而，我们需要小心如何调整 $\alpha$，因为 $\alpha$ 值太小，我们对路由的引导不够；而如果 $\alpha$ 太大，路由的**均匀性**就变得比主要的语言模型损失更重要。

一个关键的细节是**计算路由统计信息的范围**：$f_i$ 和 $P_i$ 是**按局部批次**（每个 Worker 的 Mini-Batch）计算，还是**按全局**（跨 Worker/设备聚合）计算？Qwen 团队的分析 (Qiu et al., 2025) 表明，当每个局部批次中没有足够的 Token 多样性时，局部计算可能会**损害专家专业化**（路由健康状况的良好代理）和**整体模型性能**。专家专业化是一种现象，即一个或多个专家在特定领域被激活得比其他专家更频繁。换句话说，如果一个局部批次很窄，它的路由统计数据就会嘈杂/有偏差，不会带来好的均衡。这表明，**只要可行，我们就应该使用全局统计信息**（或至少是跨设备的聚合）。值得注意的是，在那篇论文发表时，许多框架——包括 Megatron——默认都是局部计算这些统计信息的。

下方的图表（*注：指代原文图表*）来自 Qwen 的论文，说明了 **Mini-Batch 与全局批次聚合**之间的差异及其对性能和专业化的影响。

通常，围绕 MoE 进行架构选择的消融实验是棘手的，因为它涉及许多方面的相互作用。例如，共享专家的有效性可能取决于模型的**粒度**。因此，值得花一些时间来确保你有一套好的实验，才能真正获得你正在寻找的洞察！

我们现在已经涵盖了 MoE 的基础知识，但仍有更多内容有待发现。以下是一些非详尽的、可供进一步研究的项目清单：

*   零计算专家、MoE 层重新缩放和训练监控（来自 LongCat-Flash 论文）。
*   正交损失负载均衡（如 ERNIE 4.5 中所示）。
*   在训练过程中**调度负载均衡系数**。
*   架构/优化与 MoE 的相互作用，例如：
    *   优化器排名是否会因 MoE 而改变。
    *   如何将 $\mu\text{P}$ 应用于 MoE。
    *   如何为 MoE 调整学习率（因为它们在每个批次中看到的 Token 数量不同）。
*   起始处的密集层数量。
*   ...更多。

我们把进一步深入探索的重任留给你们这些求知若渴的读者，现在我们将转向最后一个主要的架构选择：**混合模型（Hybrid Models）**！

#### 4.1.7 混合模型（Hybrid Models）

最近的一个趋势是使用 **状态空间模型（State Space Models, SSM）** 或 **线性注意力机制（Linear Attention）** 来增强标准的密集型或 MoE 架构 (MiniMax et al., 2025; Zuo et al., 2025)。这些新型模型试图解决 Transformer 的一些根本性弱点：**高效处理极长上下文**。

它们采取了一种**中间路线**：

*   **循环模型（Recurrent Models）** 可以高效处理**任意长度的上下文**并**线性扩展**，但可能难以充分利用上下文中的信息。
*   **Transformer** 在**长上下文**下变得**极其昂贵**，但可以很好地利用上下文中的模式。

混合模型则试图**兼得两者之长**，这也是它们得名的原因。

有一些研究 (Waleffe et al., 2024) 试图了解 Mamba 模型（一种 SSM 形式）的弱点，发现这类模型在许多基准测试中表现良好，但例如在 MMLU 上表现不佳，并推测是 **缺乏上下文学习能力（in-context learning）** 导致了这种差距。这就是为什么它们会与密集型或 MoE 模型中的模块结合使用的原因。

这些线性注意力方法背后的核心思想是**重新排序计算**，使注意力不再需要 $O(n^2d)$ 的成本，因为在长上下文时这是无法处理的。这是如何工作的呢？

首先，回顾推理时的注意力公式。为 Token $t$ 生成输出 $o_t$ 看起来像这样（带有 Softmax）：

$$o_t = \frac{\sum_{j=1}^t \exp(q_t^\top k_j) v_j}{\sum_{l=1}^t \exp(q_t^\top k_l)}$$

现在，**去掉 Softmax**（朴素线性注意力）：

$$o_t = \sum_{j=1}^t (q_t^\top k_j) v_j$$

重新排序后得到（交换了 $q_t$ 的位置）：

$$\sum_{j=1}^t (q_t^\top k_j) v_j = \left(\sum_{j=1}^t v_j k_j^\top\right) q_t$$

定义**运行状态（Running State）** $S_t$：

$$S_t \triangleq \sum_{j=1}^t k_j v_j^\top = K_{1:t}^\top V_{1:t} \in \mathbb{R}^{d \times d}$$

通过简单的更新公式：

$$S_t = S_{t-1} + k_t v_t^\top$$

因此我们可以写成：

$$o_t = S_t q_t = S_{t-1} q_t + v_t (k_t^\top q_t)$$

**为什么重新排序很重要？**

左边的形式 $\sum_{j \leq t} (q_t^\top k_j) v_j$ 意味着：“对于每个过去的 Token $j$，计算一个点积 $q_t^\top k_j$（一个标量），用它来缩放 $v_j$，并将这 $t$ 个向量相加”——这在第 $t$ 步的成本约为 $O(t d)$。

右边的形式将其重写为 $(\sum_{j \leq t} v_j k_j^\top) q_t$：你保留了一个**单一的运行状态矩阵 $S_t = \sum_{j \leq t} v_j k_j^\top \in \mathbb{R}^{d \times d}$**，它已经总结了所有过去的 $(k_j, v_j)$。

*   每个新 Token 用一个外积 $v_t k_t^\top$ 更新 $S_t$，成本是 $O(d^2)$。
*   然后输出只是一个矩阵-向量乘法 $S_t q_t$（另一个 $O(d^2)$）。

因此，通过左边的形式从头生成 $T$ 个 Token 是 $O(T^2 d)$；而通过维护 $S_t$ 并使用右边的形式是 $O(T d^2)$。**当 $T$（序列长度）远大于 $d$（维度）时，右边的形式就具备了显著的效率优势，实现了** $O(T)$ 级别的线性缩放**。

*（直觉上：左边 = “每一步有多次小规模点积-缩放-相加”；右边 = “一个预先总结好的矩阵乘以查询”，用对**序列长度**的依赖换取对**维度**的依赖。）*

我们在这里关注推理和循环形式，但它在训练中也更高效，重新排序就像下面的等式一样简单：

$$(QK^\top)_{n \times n} V = Q (K^\top V)_{d \times d}$$

因此，我们可以看到这现在看起来与 RNN 类似的结构非常相似。这解决了我们的问题，对吗？**几乎如此。**

在实践中，Softmax 扮演着重要的**稳定作用**，而朴素的线性形式在没有某种归一化的情况下可能**不稳定**。这就催生了一种实用的变体，称为 **Lightning 或 Norm Attention**！

##### **Lightning 和 Norm Attention**

这一系列方法出现在 Minimax01 (MiniMax et al., 2025) 中，最近又出现在 Ring-linear (L. Team, Han, et al., 2025) 中，它们建立在 **Norm Attention** (Qin et al., 2022) 的思想之上。关键步骤很简单：**对输出进行归一化。** “Lightning”变体专注于使实现快速高效，并使公式略有不同。这是两者的公式：

*   **NormAttention:** $$\text{RMSNorm}(Q(K^T V))$$
*   **LightningAttention:**
    $$Q=\text{Silu}(Q), \quad K=\text{Silu}(K), \quad V=\text{Silu}(V)$$
    $$O=\text{SRMSNorm}(Q(K V^T))$$

根据 Minimax01 的说法，经验上，采用 Norm Attention 的混合模型在大多数任务上**与 Softmax 匹配**。

有趣的是，在像 **Needle in a Haystack (NIAH)** 这样的检索任务上，它比完整的 Softmax 注意力**做得好得多**，这似乎令人惊讶，但可能表明 Softmax 和线性层协同工作时存在某种**协同效应**！

现在，让我们看看更多这些方法，以及如何用一个统一的框架来理解它们。

##### **进阶线性注意力（Advanced Linear Attention）**

从循环模型中得到的一个有益教训是：**让状态偶尔能够“放下过去”**。在实践中，这意味着为先前的状态引入一个**门控（Gate）** $G_t$：

$$S_t = G_t \odot S_{t-1} + v_t k_t^\top$$

几乎所有最近的线性注意力方法都有这个**门控组件**，只是 $G_t$ 的实现不同。以下是一些论文中提出的不同变体及其对应的架构：

| 模型 | 参数化（Parameterization） | 可学习参数（Learnable parameters） |
| :--- | :--- | :--- |
| **Mamba** (A. Gu & Dao, 2024) | $G_t = \exp(-(1_t^\top \alpha_t) \odot \exp(\mathbf{A}))$, $\alpha_t = \text{softmax}(x_t W_{\alpha 1} W_{\alpha 2})$ | $\mathbf{A} \in \mathbb{R}^{d_k \times d_k}$, $W_{\alpha 1} \in \mathbb{R}^{d \times d_{\alpha}}$, $W_{\alpha 2} \in \mathbb{R}^{d_{\alpha} \times d_k}$ |
| **Mamba-2** (Dao & Gu, 2024) | $G_t = \gamma_t 1_t^\top 1$, $\gamma_t = \exp(-\text{softmax}(x_t W_\gamma) \exp(a))$ | $W_\gamma \in \mathbb{R}^{d \times 1}$, $a \in \mathbb{R}$ |
| **mLSTM** (Beck et al., 2025; H. Peng et al., 2021) | $G_t = \gamma_t 1_t^\top 1$, $\gamma_t = \sigma(x_t W_\gamma)$ | $W_\gamma \in \mathbb{R}^{d \times 1}$ |
| **Gated Retention** (Sun et al., 2024) | $G_t = \gamma_t 1_t^\top 1$, $\gamma_t = \sigma(x_t W_\gamma)^{\frac{1}{2}}$ | $W_\gamma \in \mathbb{R}^{d \times 1}$ |
| **DFW** (Mao, 2022; Pramanik et al., 2023) | $G_t = \alpha_t^\top \beta_t$, $\alpha_t = \sigma(x_t W_\alpha)$, $\beta_t = \sigma(x_t W_\beta)$ | $W_\alpha \in \mathbb{R}^{d \times d_k}$, $W_\beta \in \mathbb{R}^{d \times d_k}$ |
| **GateLoop** (Katsch, 2024) | $G_t = \alpha_t^\top 1$, $\alpha_t = \sigma(x_t W_{\alpha 1}) \exp(x_t W_{\alpha 2} i)$ | $W_{\alpha 1} \in \mathbb{R}^d$, $W_{\alpha 2} \in \mathbb{R}^d$ |
| **HGRN-2** (Qin et al., 2024) | $G_t = \alpha_t^\top 1$, $\alpha_t = \gamma + (1-\gamma) \sigma(x_t W_\alpha)$ | $W_\alpha \in \mathbb{R}^{d \times d_k}$, $\gamma \in (0, 1)^d$ |
| **RWKV-6** (B. Peng et al., 2024) | $G_t = \alpha_t^\top 1$, $\alpha_t = \exp(-\exp(x_t W_\alpha))$ | $W_\alpha \in \mathbb{R}^{d \times d_k}$ |
| **Gated Linear Attention (GLA)** | $G_t = \alpha_t^\top 1$, $\alpha_t = \sigma(x_t W_{\alpha 1} W_{\alpha 2})^{\frac{1}{2}}$ | $W_{\alpha 1} \in \mathbb{R}^{d \times 16}$, $W_{\alpha 2} \in \mathbb{R}^{16 \times d_k}$ |

一个值得注意的变体是列表中的 **Mamba-2** (Dao & Gu, 2024)。它被用于许多混合模型中，如 Nemotron-H (NVIDIA, :, Blakeman, et al., 2025)、Falcon H1 (Zuo et al., 2025) 和 Granite-4.0-h (IBM Research, 2025)。

然而，现在仍处于早期阶段，在扩展到大型混合模型时，需要考虑重要的细微差别。虽然它们显示出希望，但 MiniMax 对 M2 的经验突出表明，**小规模的益处并不总是能转化为大规模生产系统**，尤其是在复杂的推理任务、RL 训练稳定性和基础设施成熟度方面。

话虽如此，**混合模型正在迅速发展**，并且仍然是前沿训练的一个可靠选择。Qwen3-Next (采用门控 DeltaNet 更新) (Qwen Team, 2025) 报告称，它们在**长上下文推理时更快、训练更快，并且在常规基准上更强**。我们也期待 Kimi 的下一个模型，它很可能会使用他们新的**“Kimi Delta Attention”**。

最后，我们还要提一下**稀疏注意力（Sparse Attention）**，它通过选择块或查询来计算注意力，解决了与线性注意力相同的长上下文问题。一些例子包括 Native Sparse Attention (Yuan et al., 2025)、DeepSeek Sparse Attention (DeepSeek-AI, 2025) 和 InfLLM v2 (M. Team, Xiao, et al., 2025)。

在转向分词器之前，我们将通过构建一个**小的决策树**来决定是训练**密集型、MoE 还是混合模型**，从而结束架构选择的讨论。

#### 4.1.8 要不要 MoE ：如何选择基础架构？

我们现在已经见识了密集型（Dense）、MoE（专家混合）和混合（Hybrid）模型，你自然会好奇到底应该使用哪一种？

你的架构选择通常取决于**模型的部署环境、团队的专业知识和项目时间线**。让我们简要回顾每种架构的优缺点，并得出一个简单的指导流程，以帮助你找到最适合你的架构。

| 架构类型 | 描述 | 优点 (Pros) | 缺点 (Cons) |
| :--- | :--- | :--- | :--- |
| **密集型 (Dense)** | 标准的仅解码器 Transformer，每个参数都为每个 Token 激活。 | 广泛支持、成熟易懂、训练稳定、每参数性能良好。 | 计算成本与模型大小线性扩展，一个 70B 模型比 3B 模型贵约 23 倍。 |
| **专家混合 (MoE)** | 用多个“专家”替代 Transformer 中的前馈层。一个门控网络只将每个 Token 路由到少数专家。 | **每单位计算**的训练和推理性能更好。 | 内存占用高（所有专家都必须加载）。训练比密集型更复杂。框架支持不如密集型成熟。分布式训练的专家放置、负载均衡和全对全通信（All-to-all）是梦魇。 |
| **混合型 (Hybrid)** | 结合 Transformer 与状态空间模型（SSM，如 Mamba），为部分操作提供线性复杂度，以对抗注意力的二次方缩放。 | **潜在地更好的长上下文处理能力。** 对于极长序列更高效。 | 成熟度低于密集型和 MoE，经过验证的训练“配方”更少。框架支持有限。 |

**简而言之，如何做决定：**

首先，问问你的模型将**部署在哪里**。然后，考虑你团队的**专业知识**和**训练时间线**，以评估你可以承受多少探索成本：

1.  **部署场景决定了内存约束：**
    *   **设备端/边缘设备/低显存要求：** 内存是瓶颈，基本排除 **MoE** (MoE 必须加载所有参数)。首选 **密集型** 或考虑极致优化的 **混合型**。
    *   **云计算/数据中心/追求 SOTA 性能/算力充裕：** 内存不是主要瓶颈，优先考虑 **MoE**（最佳的性能/计算比）或 **混合型**（极致的长上下文）。

2.  **团队经验和时间线决定了技术风险：**
    *   **时间紧迫/团队经验相对较少：** 选择**密集型**架构，这是最稳健、文档最丰富的路径。
    *   **有时间探索/拥有资深分布式/系统工程师：** 可以在 **MoE** 或 **混合型** 上进行探索。

**以 SmolLM3 为例：**

我们想构建一个强大的**设备端部署**小型模型，时间线大约是 **3 个月**，并且过去主要训练的是**密集型模型**。

*   这排除了 **MoE**（设备内存约束）。
*   这也排除了 **混合型**（探索新架构的时间太短，而且密集型模型也能达到我们目标的最大 128k Token 长上下文）。

因此，我们选择了 **Llama 风格的密集型模型**。

现在我们已经研究了模型架构的内部细节，接下来让我们看看**分词器（Tokenizer）**，它是连接数据和模型的桥梁。

#### 4.1.9 分词器（Tokenizer）：被低估的“语言翻译官”

虽然分词方案很少能像架构创新那样引人注目，但它很可能是任何语言模型中**最被低估的组件之一**。

你可以将分词器视为**人类语言和模型所处的数学世界之间的翻译官**，而正如任何翻译官一样，**翻译的质量至关重要**。那么，我们如何为自己的需求构建或选择一个合适的分词器呢？

##### **分词器的基本原理**

分词器的核心作用是将**原始文本**转换为模型可以处理的**数字序列**，它通过将一段运行文本分割成称为 **Token** 的单个可处理单元来实现。在深入技术细节之前，我们应该首先回答一些将指导我们分词器设计的**根本问题**：

1.  **我们想要支持哪些语言？** 如果我们正在构建一个多语言模型，但我们的分词器只见过英语，那么当遇到非英语文本时，模型将是低效的，文本会被分割成比必要多得多的 Token。这直接影响**性能、训练成本和推理速度**。
2.  **哪些领域对我们很重要？** 除了语言之外，像**数学和代码**这样的领域需要仔细表示数字。
3.  **我们知道目标数据混合配比吗？** 如果我们计划从头开始训练我们的分词器，理想情况下，我们应该用一个**反映我们最终训练混合配比**的样本来训练它。

一旦我们回答了这些问题，我们就可以研究主要的设计决策：

##### **词汇表大小（Vocabulary Size）**

**词汇表**本质上是一个列出了模型识别的所有 Token（最小文本单元，如词语、子词或符号）的字典。

**更大的词汇表**能更有效地压缩文本，因为每个句子生成的 Token 更少，但这涉及到一个**计算上的权衡**。词汇表大小直接影响我们的**嵌入矩阵**的大小。如果词汇表大小为 $V$，隐藏层维度为 $h$，则**输入嵌入**有 $V \times h$ 个参数，**输出层**也有 $V \times h$ 个参数。正如我们在“嵌入层共享”一节中看到的，对于较小的模型，这占了总参数量的很大一部分，但随着模型的扩展，相对成本会缩小。

最佳点取决于我们的**目标覆盖范围**和**模型大小**。对于**仅英语模型**，大约 50k 个 Token 通常就足够了，但**多语言模型**通常需要 **100k+** 才能有效处理不同的书写系统和语言。像 Llama 3 这样的现代 SOTA 模型已经采用了 **128k+** 的词汇表范围，以提高跨不同语言的 Token 效率。同一家族中较小的模型则通过应用**嵌入层共享**来减少嵌入参数的百分比，同时仍受益于更大的词汇表。Dagan et al. (2024) 分析了词汇表大小对压缩、推理和内存的影响。他们观察到，更大的词汇表带来的**压缩收益呈指数级下降**，暗示存在一个最优大小。对于推理，更大的模型受益于更大的词汇表，因为压缩节省的前向传播成本高于额外的嵌入 Token 在 Softmax 中的成本。对于内存，最优大小取决于序列长度和批次大小：更长的上下文和更大的批次受益于更大的词汇表，因为 Token 减少节省了 KV 缓存空间。

##### **分词算法（Tokenization Algorithm）**

**BPE (Byte-Pair Encoding)** (Sennrich et al., 2016) 仍然是最流行的选择，WordPiece 或 SentencePiece 等其他算法也存在，但采用较少。人们对**无分词器方法**（直接在字节或字符上工作）的研究兴趣也在增长，这有可能完全消除分词过程。

既然我们已经了解了定义分词器的关键参数，我们面临一个实际的决定：**我们应该使用现有的分词器还是从头开始训练？** 答案取决于**覆盖范围**：具有我们目标词汇表大小的现有分词器是否能很好地处理我们的语言和领域。

下方的图表（*注：原文指代的图表*）比较了 GPT-2 的**仅英语分词器** (Radford et al., 2019) 和 Gemma 3 的**多语言分词器** (G. Team, Kamath, et al., 2025) 对同一句英语和阿拉伯语句子的分段方式。

虽然这两种分词器在英语上的表现似乎相似，但**在阿拉伯语上差异显著**：GPT2 将文本分解成一百多个片段，而 Gemma 3 由于其多语言训练数据和更大、更具包容性的词汇表，生成的 Token 数量要少得多。

但是，要衡量一个分词器的质量，我们不能仅仅凭直觉看几个分词示例就断定好坏，就像我们不能在没有运行消融实验的情况下凭直觉进行架构更改一样。我们需要**具体的指标**来评估分词器的质量。

##### **衡量分词器质量**

为了评估分词器的表现，我们可以使用 **FineWeb2** 中使用的两个关键指标 (Penedo et al., 2025)。

1.  **Token 消耗比（Fertility）：**
    它衡量了**编码一个词平均所需的 Token 数量**。**较低的Token 消耗比意味着更好的压缩**，这转化为更快的训练和推理。你可以这样想：如果一个分词器需要一个或两个以上 Token 来编码大多数词语，而另一个分词器需要的 Token 更少，那么后者显然更高效。

    衡量Token 消耗比的标准方法是计算**词语-Token 比率（word fertility）**，它衡量平均每个词需要多少 Token。这个指标是围绕**词语**的概念定义的，因为它在有合适的词语分词器可用时（例如 Spacy 和 Stanza），能提供有意义的**跨语言比较** (Penedo et al., 2025)。

    在比较**单一语言**的分词器时，你也可以使用**字符或字节**而不是词语来获得**字符-Token 比率**或**字节-Token 比率** (Dagan et al., 2024)。然而，这些指标在跨语言比较方面存在局限性。字节可能会产生偏差，因为不同脚本中的字符需要不同的字节表示（例如，中文汉字在 UTF-8 中使用三个字节，而拉丁字符使用一到两个）。同样，使用字符数并没有考虑到词语长度在不同语言之间差异巨大的事实。例如，中文词语通常比德语复合词短得多。

2.  **连续词比例（Proportion of continued words）：**
    这个指标告诉我们**有多少百分比的词语被分割成多个片段**。**较低的百分比更好**，因为它意味着被片段化的词语更少，从而实现更高效的分词。

*(此处省略了原文提供的 Python 函数代码，以保持文章的可读性，读者可以直接查阅原文代码。)*

对于像**代码和数学**这样的专业领域，除了Token 消耗比之外，我们还需要更深入地研究分词器处理**领域特定模式**的能力。大多数现代分词器都进行**单数字分割**（例如，“123”被分割为 [“1”, “2”, “3”]）(Chowdhery et al., 2022; DeepSeek-AI et al., 2024)。将数字拆开可能看起来违反直觉，但它实际上有助于模型更有效地学习**算术模式**。如果“342792”被编码为一个不可分割的 Token，模型必须记住将这个特定 Token 与每个其他数字 Token 相加、相减或相乘时会发生什么。但如果它被分割，模型就会学习**数字级别的运算**是如何工作的。像 Llama 3 (Grattafiori et al., 2024) 这样的分词器将数字 1 到 999 编码为独特的 Token，其余的数字则由这些 Token 组成。

因此，我们可以衡量在我们的目标领域上的**Token 消耗比**，以评估一个分词器的弱点和优势。下表比较了流行分词器在不同语言和领域的Token 消耗比。

##### **评估分词器**

为了比较不同语言的分词器，我们将使用 **FineWeb2 分词器分析**中的设置，使用 **Wikipedia 文章**作为我们的评估语料库。对于每种语言，我们将抽取 100 篇文章以获得有意义的样本，同时保持计算可控。

结果揭示了一些赢家和权衡取舍，具体取决于你的优先事项：

**Token 消耗比（每个词的 Token 数）**

| 分词器 (词汇表大小) | 英语 | 中文 | 法语 | 阿拉伯语 |
| :--- | :--- | :--- | :--- | :--- |
| **Llama3 (128k)** | 1.48 | 1.60 | 1.73 | 2.35 |
| **Mistral Small (131k)** | 1.59 | 1.78 | 1.69 | 2.15 |
| **Qwen3 (151k)** | 1.54 | **1.45** | 1.75 | 2.26 |
| **Gemma3 (262k)** | **1.41** | 1.47 | **1.56** | **2.25** |

**连续词比例 (%)**

| 分词器 (词汇表大小) | 英语 | 中文 | 法语 | 阿拉伯语 |
| :--- | :--- | :--- | :--- | :--- |
| **Llama3 (128k)** | 32.2% | 42.6% | 48.2% | 71.8% |
| **Mistral Small (131k)** | 36.8% | 47.1% | 46.5% | 66.0% |
| **Qwen3 (151k)** | 32.8% | **30.7%** | 47.8% | **66.0%** |
| **Gemma3 (262k)** | **26.0%** | 33.1% | **39.9%** | 70.1% |

**Gemma 3 分词器**在多种语言上实现了较低的Token 消耗比和分词率，尤其在英语、法语和西班牙语上，这可以通过其分词器训练数据和**非常庞大的 262k 词汇表**（大约是 Llama 3 的 128k 的两倍）来解释。**Qwen 3 分词器**在**中文上表现出色**，但在英语、法语和西班牙语上落后于 Llama 3 分词器。**Mistral Small 分词器** (Mistral AI, 2025) 在阿拉伯语上表现最佳，但在英语和中文上落后于其他分词器。

##### **选择现有分词器还是自定义分词器**

目前，市面上有一系列不错的强大分词器可供选择。许多最近的模型都是从 GPT4 的分词器 (OpenAI et al., 2024) 开始，然后通过额外的多语言 Token 进行增强。正如我们在上表中所看到的，Llama 3 的分词器在多语言文本和代码上的**平均分词质量表现良好**，而 Qwen 2.5 在**中文**和一些**低资源语言**上表现尤为出色。

*   **何时使用现有分词器：** 如果我们的目标用例与上表最佳分词器（Llama、Qwen、Gemma）的语言或领域覆盖范围匹配，那么它们是经过实战检验的可靠选择。对于 **SmolLM3 训练**，我们选择了 **Llama 3 的分词器**：它在我们的目标语言（英语、法语、西班牙语、葡萄牙语、意大利语）上提供了有竞争力的分词质量，同时词汇表大小适中，这对于我们的小模型规模是合理的。对于嵌入层只占总参数量一小部分的大型模型，Gemma 3 的效率增益变得更具吸引力。

*   **何时训练我们自己的分词器：** 如果我们正在为**低资源语言**训练，或者我们的**数据混合配比非常不同**，我们可能需要训练自己的分词器以确保良好的覆盖范围。在这种情况下，重要的是我们要用一个**接近我们认为最终训练混合配比**的数据集来训练分词器。这会产生一个 **“先有鸡还是先有蛋”** 的问题，因为我们需要一个分词器才能运行数据消融实验并找到最佳混合配比。但我们可以在启动最终运行之前重新训练分词器，并验证下游性能有所改善且Token 消耗比仍然良好。

分词器的选择可能看起来像一个技术细节，但它会**影响你模型性能的方方面面**。所以，不要害怕投入时间去确保它的正确性。

#### 4.1.10 SmolLM3：将所有洞察付诸实践

既然我们已经探索了架构格局并运行了系统性的消融实验，那么让我们看看这一切是如何在 **SmolLM3** 这样的模型中付诸实践的。

SmolLM 家族旨在**突破小型模型能力的边界**。SmolLM2 交付了 135M、360M 和 1.7B 参数的三个有能力高效运行在设备端的模型。对于 SmolLM3，我们希望在**提升性能**的同时保持**手机上运行所需的小巧规模**，并解决 SmolLM2 的弱点：**多语言能力、极长上下文处理和强大的推理能力**。我们选择了 **30 亿参数**作为实现这种平衡的最佳点。

由于我们是在扩大一个经过验证的“配方”，我们自然而然地倾向于**密集型 Transformer**。当时 nanotron 中尚未实现 MoE，而且我们已经拥有训练强大的小型密集型模型的专业知识和基础设施。更重要的是，对于**边缘设备部署**，我们受到**内存限制**，一个即使只有少量参数活跃但总参数量很大的 MoE 模型也会受到限制，因为我们仍然需要将所有专家加载到内存中。这使得**密集型模型**对于我们的边缘部署目标**更为实用**。

**消融实验：** 我们以 SmolLM2 的 1.7B 架构作为基础，然后在 **1000 亿 Token** 上训练了一个 **3B 的消融模型**，使用了 **Qwen 2.5-3B 的布局**。这为我们提供了坚实的基线，以便单独测试每个修改。**每一次架构更改都需要要么提升英语基准上的 Loss 和下游性能，要么在不降低质量的情况下提供可衡量的益处**，例如推理速度。

以下是我们进行最终运行之前测试并通过的关键修改：

1.  **分词器（Tokenizer）：** 在深入架构修改之前，我们需要选择一个分词器。我们找到了一个很好的分词器集合，它们覆盖了我们的目标语言和领域。根据我们的**词元化效率（Fertility）**分析，**Llama 3.2 的分词器**在我们的 6 种目标语言之间提供了**最佳的权衡**，同时将词汇表保持在 128k——足够大以实现多语言效率，但又不会大到因嵌入权重而使我们 3B 参数模型的参数量过度膨胀。
2.  **分组查询注意力 (GQA)：** 我们再次确认了我们早期的发现，即**分组数量为 4 的 GQA 匹配多头注意力的性能**，但这一次是在 3B 规模和 1000 亿 Token 上进行的。KV 缓存的效率提升实在太诱人，尤其对于内存珍贵的设备端部署。
3.  **用于长上下文的 NoPE：** 我们实施了 **NoPE**，通过**每隔 4 层移除 RoPE**。我们的 3B 消融实验证实了前面章节的发现：**NoPE 在不牺牲短上下文性能的情况下改进了长上下文处理**。
4.  **文档内注意力掩码 (Intra-document attention masking)：** 我们阻止了训练期间的跨文档注意力，以帮助在训练非常长的序列时提高**训练速度和稳定性**，我们再次发现这**不影响下游性能**。
5.  **模型布局优化：** 我们比较了文献中最近 3B 模型的布局，有些偏向深度，有些偏向宽度。我们在我们的训练设置中测试了 Qwen 2.5-3B (3.1B)、Llama 3.2-3B (3.2B) 和 Falcon 3-H1-3B (3.1B) 的布局，它们的深度和宽度各不相同。结果很有趣：**所有布局都达到了几乎相同的 Loss 和下游性能**，尽管 Qwen 2.5-3B 的参数量实际上更少。但 **Qwen 2.5-3B 更深的架构**与研究结果一致，即**网络深度有助于泛化** (Petty et al., 2024)。因此，我们选择了更深的布局，赌它会随着训练的进展而带来帮助。
6.  **稳定性改进：** 我们保留了 SmolLM2 的**绑定嵌入（Tied Embeddings）**，并添加了一个受 **OLMo 2 启发的新技巧：移除嵌入层的权重衰减**。我们的消融实验显示，这在**不损害性能**的同时降低了嵌入层范数，有助于**防止训练发散**。

这种系统性消融方法的美妙之处在于，我们可以**自信地将所有这些修改结合起来**，因为我们知道**每一个修改都已经得到了验证**。

#### 4.1.11 架构设计准则 (Rules of Engagement)

**一句话总结：你的用例驱动你的选择。**

1.  **让你的部署目标指导架构决策。** 在评估新的架构创新时，请考虑你的模型**最终将在何处以及如何运行**。
2.  **在创新与实用主义之间取得恰当的平衡。** 我们不能忽略重大的架构进步——在 GQA 和更好的替代方案存在的情况下，今天仍使用多头注意力（MHA）将是一个糟糕的技术选择。要随时了解最新研究，并采纳那些在**大规模上提供清晰、经过验证的益处**的技术。但要抵抗住追逐每一篇承诺微小收益的新论文的诱惑（除非你有足够的资源或你的目标就是架构研究）。
3.  **系统性胜过直觉。** 验证每一个架构更改，无论它在论文上看起来多么有前景。然后**先单独测试修改**，再将它们结合起来以理解它们的相互作用。
4.  **规模效应是真实存在的 — 尽可能在目标规模上重新消融。** 不要假设你的小规模消融结果在你的目标模型规模上会完美成立。如果你有算力，请尝试重新确认它们。
5.  **在你的实际领域上验证分词器效率。** 你的目标语言和领域上的**Token 消耗比（Fertility）**指标，比追随最新的模型使用了什么更重要。一个 50k 的英语分词器无法胜任严肃的多语言工作，但如果你没有覆盖那么多语言，你也不需要一个 256k 的词汇表。

现在，模型架构已经确定，是时候着手解决将驱动学习过程的**优化器和超参数**了。

### 4.2  优化器与训练超参数

一切正在逐步就位。我们已经运行了消融实验，确定了架构，并选择了分词器。但在我们真正启动训练之前，仍然缺少一些**至关重要的组件**：我们应该使用哪个**优化器（Optimizer）**？应该设置多大的**学习率（Learning Rate）**和**批次大小（Batch Size）**？我们应该如何**调度**学习率在训练过程中的变化？

在这里，最诱人的方法是**直接借用文献中另一个强大模型的参数值**。毕竟，如果它对大实验室有效，对我们也应该有效，对吗？

在许多情况下，如果我们选取的是来自相似架构和模型大小的值，这样做确实可行。

然而，通过不对这些值针对我们的**特定设置**进行调优，我们**有可能会牺牲掉潜在的性能**。文献中的超参数是为特定的数据和约束条件优化的，而这些约束有时甚至与性能无关。也许某个学习率是在开发初期就选定的，从未被重新审视。即使模型作者进行了彻底的**超参数扫描（Hyperparameter Sweeps）**，那些最优值也是针对**他们精确的架构、数据和训练方案组合**而找到的，而不是针对我们的。

**文献中的值永远是一个很好的起点**，但探索一下我们能否在附近找到更好的值，总是一个好主意。

在本章中，我们将探索**最新的优化器**（并看看**老当益壮的 AdamW** (Kingma, 2014) 是否依然经得起时间的考验 🎉）、深入研究**超越标准余弦衰减的学习率调度**，并找出如何针对给定的模型和数据大小来**调优学习率和批次大小**。

让我们从**优化器之战**开始吧。

#### 4.2.1 优化器：AdamW 以及其他

**优化器（Optimizer）** 是整个 LLM 训练操作的核心。它根据过去的更新、当前的权重以及从损失函数中导出的梯度，来决定每个参数的**实际更新步骤**。同时，它也是一个**吞噬内存和计算资源的野兽**，因此会影响你需要多少 GPU 以及你的训练速度有多快。

我们总结了当前用于 LLM 预训练的优化器：

| 模型 | 优化器 |
| :--- | :--- |
| Kimi K2, GLM 4.5 | **Muon** |
| 其他所有人 | **AdamW** |

所以，你可能会好奇为什么大家都在用 AdamW？

撰写这篇博客的人认为这是因为 **“人们很懒”**，但更现实的说法可能是：**AdamW 在不同规模上长期以来一直表现良好/更优**，而且改变这样一个核心组件总是有点可怕，尤其是当测试它在非常长的训练中的表现有多好**既困难又昂贵**时。

此外，公平地比较优化器**比看起来更难**。规模的变化会以一种难以在小型消融实验中模拟的方式改变动态，使得**超参数调优变得复杂**。你可能会说：“没关系，我的 AdamW 已经调优了好几周，我可以直接重用相同的超参数进行比较！” 我们多么希望这是真的。但不幸的是，对于每种优化器，你都需要进行**适当的超参数搜索**（1D？2D？3D？），这使得优化器研究既困难又昂贵。

因此，让我们从经典且成就了 Durk Kingma 令人恐惧的 Google Scholar 统治地位的基础开始：**AdamW**。

##### **AdamW**

**Adam（Adaptive Momentum Estimation，自适应矩估计）**是一种**一阶优化技术**。这意味着，除了单独查看梯度之外，我们还会考虑权重在**先前步骤中改变了多少**。这使得每个参数的**学习率根据动量进行自适应**。

细心的读者可能会问：嘿，你是不是漏了一个 **W**？没错！我们特意加上 W（=权重衰减 Weight Decay）的原因如下。在标准的 SGD 中，我们可以简单地向 Loss 添加一个 $\lambda\theta^2$ 项（其中 $\theta$ 是权重）来应用 L2 正则化。然而，如果我们对 Adam 做同样的事情，**自适应学习率也会影响 L2 正则化**。这意味着正则化强度变得**依赖于梯度幅度**，从而削弱了它的效果。这不是我们想要的，这就是为什么 **AdamW 将 L2 正则化与主优化循环解耦应用**来解决这个问题。

有趣的是，在过去几年中，AdamW 的超参数几乎没有变动：

*   $\beta_1 = 0.9, \beta_2 = 0.95$
*   梯度范数裁剪（grad norm clipping）$= 1.0$
*   权重衰减（weight decay）$= 0.1$（Llama-3-405B 将其降至 $0.01$）

从 Llama 1, 2, 3 到 DeepSeek-V1, 2, 3 671B，几乎都重复使用了这三个值，没有变化。**Durk Kingma 从头到尾都是对的吗？或者我们能做得更好？**

##### **一句话理解 Muon**

Adam 是一种**一阶方法**，因为它只使用了梯度。**Muon** 则是一种**二阶优化器**，它作用于参数张量的**矩阵视图**。

$$\begin{aligned} G_t &= \nabla_\theta L_t(\theta_{t-1}) \\ B_t &= \mu B_{t-1} + G_t \\ O_t &= \text{NewtonSchulz5}(B_t) \approx UV^\top \quad \text{if } B_t = U\Sigma V^\top \quad (\text{SVD}) \\ \theta_t &= \theta_{t-1} - \eta O_t G_t \end{aligned}$$

看着这些方程，你可能会好奇为什么这是一种**二阶方法**，我只看到了梯度，没有更高阶的项。二阶优化实际上发生在 **Newton Schulz 步骤**内部，但我们在此不做进一步的详细解释。

已经有一些高质量的博客深入解释了 Muon，所以在这里我们只列出 **Muon 的三个关键思想**：

1.  **矩阵级几何 vs. 参数级更新：** AdamW 是**每参数**预处理（对角二阶矩）。Muon 将每个权重矩阵视为一个**单一对象**，并沿着 $G=UV^\top$ 进行更新，从而捕获**行/列子空间结构**。
2.  **通过正交化实现各向同性步骤：** 使用奇异值分解（SVD）将 $G=U\Sigma V^\top$ 分解，将**幅度** ($\Sigma$) 与**方向**（左右子空间 $U, V$）分离。用 $UV^\top$ 替换 $G$ 会**丢弃奇异值**，使步骤在活跃子空间中**各向同性（Isotropic）**。这乍一看有点反直觉——因为丢弃 $\Sigma$ 看起来像是丢失信息——但它**减少了轴对齐的偏差**，并鼓励探索那些原本会被非常小的奇异值抑制的方向。这种探索是否会为模型带来不同的能力（而不仅仅是看 Loss）仍然是一个悬而未决的问题。
3.  **对更大批次大小的经验容忍度：** 在实践中，Muon 通常能**容忍更高的批次大小**。我们将在批次大小部分更深入地讨论这一点，但这可能是 Muon 被采纳的关键点！

多年来，社区大多满足于 AdamW，并且前沿实验室的优化器“配方”通常是保密的（例如 Qwen 就没有公开谈论他们的优化器）。但最近，Muon 在高知名度的发布中得到了应用（例如 Kimi K2、GLM-4.5）。希望我们能看到更多开放和稳健的使用配方。

优化器的世界是一个**狂野的动物园**，研究人员在组合所有可能的动量和导数方面所展现的创造力，唯一能与之匹敌的就是为它们命名：Shampoo、SOAP、PSGD、CASPR、DION、Sophia、Lion……就连 AdamW 也有自己的变体，如 NAdamW、StableAdamW 等。深入探讨所有这些优化器值得单独写一篇博客，但我们将其留待下次。同时，我们推荐斯坦福/Marin 团队的这篇精彩论文 (Wen et al., 2025)，他们对许多不同的优化器进行了基准测试，以展示在进行比较时**超参数调优是多么重要**。

与几乎所有优化器密切相关的问题是：**我们应该以多大的强度更新权重？** 这由通常出现在优化器方程中的**学习率**这个标量值决定。让我们看看这个看似简单的话题是如何仍然拥有许多面向的。

#### 4.2.2 学习率（Learning Rate）

学习率是我们将要设置的最重要的超参数之一。在每个训练步骤中，它控制着我们根据计算出的梯度来**调整模型权重的幅度**。

*   如果学习率设置得**太低**，我们的训练将变得**慢得痛苦**，并且我们可能会陷入一个糟糕的局部最小值。Loss 曲线将看起来平坦，我们会在没有取得有意义进展的情况下烧完算力预算。
*   另一方面，如果学习率设置得**太高**，我们会导致优化器采取**巨大的步骤**，从而**越过最佳解决方案**，永远无法收敛；或者发生无法想象的事情：**Loss 发散，直接冲向云霄**。

但最佳的学习率甚至不是恒定的，因为**学习动态在训练过程中会发生变化**。当我们离好的解决方案很远时，**高学习率**是有效的，但在接近收敛时，它们会导致**不稳定性**。这就是 **学习率调度（Learning Rate Schedules）** 发挥作用的地方：从零开始 **预热（Warmup）** 以避免早期的混乱，然后 **衰减（Decay）** 以稳定到一个好的最小值。这些模式（例如，Warmup + 余弦衰减）已被验证适用于神经网络训练多年。

让我们看看常见的调度方案，然后讨论如何选择峰值。

##### **学习率调度：不止余弦衰减**

多年来，人们就知道改变学习率有助于收敛 (Smith & Topin, 2018)，而**余弦衰减（Cosine Decay）** (Loshchilov & Hutter, 2017) 是训练 LLM 的首选调度方案：在预热后以峰值学习率开始，然后沿着余弦曲线平稳下降。它简单且效果好。

但它的主要缺点是**缺乏灵活性**：我们需要**预先知道总训练步数**，因为余弦周期的长度必须匹配你的总训练时长。这在常见的场景中会成为问题：你的模型尚未达到稳定期，或者你获得了更多算力想要训练更久，或者你正在运行扩展定律实验，需要用不同的 Token 数量训练相同的模型。**余弦衰减会迫使你从头开始重启。**

现在，许多团队使用**不需要在预热后立即衰减**的调度方案。这就是下方图表所示的 **Warmup-Stable-Decay (WSD)** (Hu et al., 2024) 和 **Multi-Step（多步骤）**变体 (DeepSeek-AI, :, et al., 2024) 的情况。你可以在大部分训练中保持一个**恒定的高学习率**，然后在**最后阶段**（通常是最后 10%-20% 的 Token）急剧衰减（WSD），或者进行**离散的下降（步骤）**来降低学习率，例如在 80% 训练之后，然后在 90% 之后再进行一次（如 DeepSeek LLM 的 Multi-Step 调度所做）。

这些调度方案提供了优于余弦衰减的**实用优势**。我们可以**在运行中途延长训练而无需重启**，无论是我们想训练更久，还是想提前衰减以更准确地衡量训练进度，或者我们可以用一次主训练运行来运行跨不同 Token 数量的扩展定律实验。此外，研究表明 **WSD 和 Multi-Step 都与余弦衰减相匹配** (DeepSeek-AI, :, et al., 2024; Hägele et al., 2024)，同时对于实际训练场景**更具实用性**。

但你可能注意到了，与余弦相比，这些调度引入了新的超参数：WSD 中**衰减阶段应该持续多久**？Multi-Step 变体中**每个步骤应该持续多久**？

*   **对于 WSD：** 匹配余弦性能所需的冷却时间随着训练运行时间的延长而缩短，建议将**总 Token 的 10%-20%** 分配给衰减阶段 (Hägele et al., 2024)。我们将在下面的消融实验中确认这种设置是否匹配余弦。
*   **对于 Multi-Step：** DeepSeek LLM 的消融实验发现，虽然他们的基线 80/10/10 分割（稳定直到 80%，第一次下降从 80-90%，第二次从 90-100%）匹配余弦，但调整这些比例甚至可以**超越它**，例如使用 70/15/15 和 60/20/20 的分割。

但我们可以对这些调度方案变得更有创意。让我们看看 DeepSeek 模型家族中使用的调度方案：

*   **DeepSeek LLM** 使用了基线 Multi-Step 调度（80/10/10）。
*   **DeepSeek V2** 将比例调整为 60/30/10，给第一次衰减步骤**更多的时间**。
*   **DeepSeek V3** 采取了最具创意的方法：他们**用余弦衰减**从恒定阶段过渡（从 67% 到 97% 的训练），然后应用一个简短的**恒定阶段**，最后是**最终的急剧下降**。

让我们在这里结束对奇特学习率调度的调查，并**烧掉一些 GPU 小时**来确定在实践中什么有效！

##### **消融实验 - WSD 匹配余弦**

现在是进行消融实验的时候了！让我们测试一下 WSD 在实践中是否真的匹配余弦的性能。我们在这里不会展示 Multi-Step 的消融实验，但我们推荐 DeepSeek LLM 的消融实验，他们在其中展示了 Multi-Step 在不同的阶段分割下与余弦相匹配。在本节中，我们将比较**余弦衰减**与具有**两种衰减窗口（10% 和 20%）的 WSD**。

评估结果显示，所有三种配置的**最终性能相似**。观察 Loss 和评估曲线（特别是 HellaSwag），我们看到一个有趣的模式：**余弦在稳定阶段（WSD 衰减开始之前）实现了更好的 Loss 和评估分数**。然而，一旦 WSD 进入其衰减阶段，Loss 和下游指标都出现了**近乎线性的改进**，使 WSD 在训练结束时追赶上余弦。

这证实了 **WSD 的 10%-20% 衰减窗口足以匹配余弦的最终性能**，同时保持了在训练中途延长的灵活性。**我们为 SmolLM3 选择了具有 10% 衰减的 WSD。**

既然我们对流行的学习率调度有了很好的概述，下一个问题是：**峰值学习率到底应该是什么？**

##### **寻找最优学习率**

我们如何为我们特定的学习率调度和训练设置挑选正确的学习率？

我们可以像对架构选择一样，在短消融实验中运行**学习率扫描（Learning Rate Sweeps）**。但**最优学习率取决于训练时长**：在短消融实验中收敛最快的学习率，可能不是完整运行的最佳学习率。而且我们负担不起仅仅为了测试不同的学习率而多次运行昂贵的多周训练。

让我们首先看看我们可以快速运行的简单扫描，帮助我们排除那些**明显太高或太低**的学习率，然后我们将讨论用于超参数的**扩展定律**。

##### **消融实验 - 学习率扫描**

为了说明不同学习率的影响，让我们看一下在我们 1B 消融模型上进行的扫描，该模型在 45B Token 上训练。我们使用 4 个不同的学习率训练相同的模型，使用相同的设置：$1\text{e-}4, 5\text{e-}4, 5\text{e-}3, 5\text{e-}2$。结果清楚地显示了两个极端的危险：

*   **$5\text{e-}2$** 几乎立即发散，Loss 早期飙升，从未恢复，使得模型无法使用。
*   **$1\text{e-}4$** 过于保守，虽然训练稳定，但收敛速度比其他学习率慢得多。
*   中值 $5\text{e-}4$ 和 $5\text{e-}3$ 显示出更好的收敛性，并且性能具有可比性。

但为每种模型尺寸运行扫描会很快变得昂贵，更重要的是，正如我们前面所述，它**没有考虑到计划的训练 Token 数量**。这就是 **扩展定律（Scaling Laws）** 变得无价的地方。

对于 SmolLM3，我们使用 WSD 调度和 AdamW 在 100B Token 上训练 3B 模型，比较了几个学习率。我们发现 $2\text{e-}4$ 在 Loss 和下游性能上的收敛速度都比 $1\text{e-}4$ 快得多，而 $3\text{e-}4$ 仅略优于 $2\text{e-}4$。$3\text{e-}4$ 带来的边际收益伴随着在长期训练中**不稳定性增加的风险**，因此我们将 $2\text{e-}4$ 选为我们的**甜蜜点**。

这些扫描有助于我们排除那些**明显太高**（发散）或**太低**（收敛缓慢）的学习率，但为每种模型尺寸运行扫描会很快变得昂贵，更重要的是，正如我们前面所述，它**没有考虑到计划的训练 Token 数量**。这就是 **扩展定律（Scaling Laws）** 变得无价的地方。

但在我们深入研究超参数的扩展定律之前，让我们讨论另一个与学习率相互作用的关键超参数：**批次大小（Batch Size）**。

#### 4.2.3 批次大小（Batch Size）：效率与数据效率的平衡艺术

**批次大小**是在更新模型权重之前处理的样本数量。它直接影响**训练效率**和**最终模型性能**。如果你的硬件和训练堆栈能够很好地跨设备扩展，增加批次大小可以提高**吞吐量**。

但是，超过某个点后，更大的批次开始**损害数据效率**：模型需要**更多的总 Token** 才能达到相同的 Loss。发生这种情况的临界点被称为**临界批次大小（Critical Batch Size）** (McCandlish et al., 2018)。

*   **低于临界批次大小增加批次：** 在增加批次大小并重新调整学习率后，你可以用**相同数量的 Token** 达到与较小批次运行相同的 Loss，**没有数据浪费**。
*   **高于临界批次大小增加批次：** 更大的批次开始**牺牲数据效率**；现在需要**更多的总 Token** 才能达到相同的 Loss（因此需要更多的钱），即使由于更多的芯片处于繁忙状态，**挂钟时间（Wall-Clock Time）下降了**。

让我们试着直观地理解为什么我们需要重新调整学习率，以及如何计算临界批次大小的估算值。

当批次大小增长时，每个 Mini-Batch 的梯度是**真实梯度**的更好估计，因此你可以安全地采取**更大的步骤**（即增加学习率），并在更少的更新次数内达到目标 Loss。问题是如何进行缩放。

**对 $B$ 个样本进行平均**

*   **批次梯度：** $\tilde{g}_B = \frac{1}{B} \sum_{i=1}^B \tilde{g}^{(i)}$
*   **均值保持不变：** $E[\tilde{g}_B] = g$
*   **协方差缩小：** $Cov(\tilde{g}_B) = \frac{\Sigma}{B}$

SGD 参数更新是：

$$\Delta w = - \eta \tilde{g}_B$$

这个更新的**方差**与 $\text{Var}(\Delta w) \propto \eta^2 \frac{\Sigma}{B}$ 成正比。因此，为了保持更新方差大致恒定，如果你将批次大小缩放 $k$ 倍，你就需要将学习率缩放 $\sqrt{k}$ 倍。

所以，假设你已经计算出了你的最优批次大小和学习率，并且你发现增加到临界批次大小是可行的并且提高了吞吐量，你还需要**相应地调整最优学习率**。

$$B_{\text{critical}} \rightarrow k B_{\text{optimal}} \implies \eta_{\text{critical}} \rightarrow \sqrt{k} \eta_{\text{optimal}}$$

对于 AdamW 或 Muon 这样的优化器来说，随着批次大小增长，**学习率进行平方根缩放**是一个有用的经验法则，但这也会取决于优化器。例如，使用 AdamW 时，它与 $\beta_1 / \beta_2$ 存在相互作用，可能引入非常不同的行为。

一个**实用的替代方案**是**短期分支训练**：保持一次运行使用原始批次，开始第二次运行使用更大的批次和**重新缩放后的学习率（Rescaled LR）**，并且只有当重新缩放后两个 Loss 曲线对齐时才采用更大的批次 (Merrill et al., 2025)。在这篇论文中，他们在切换批次大小时**重新预热学习率并重置优化器状态**。他们还设置了一个容忍度和时间窗口来决定 Loss 是否“匹配”，这两个旋钮都是凭经验选择的。他们发现 $B_{\text{simple}}$ 估算（它也有噪音）**低估了“实际”的临界批次大小**。这为你提供了一个快速、低风险的检查，确保新的批次/学习率对保持了训练动态。

**临界批次大小不是固定的，它会随着训练的进展而增长。**

*   在训练早期，模型正在进行**大梯度步骤**，所以 $\|g\|^2$ 很大，这意味着 $B_{\text{simple}}$ 很小，因此模型有一个**较小的临界批次大小**。
*   随后，随着模型更新的稳定，**更大的批次变得更有效**。

这就是为什么一些大规模训练不保持批次大小恒定，而是使用我们称之为**批次大小预热（Batch Size Warmup）**的原因。例如，DeepSeek V3 在前约 4690 亿 Token 中以 12.6M 的批次大小开始，然后增加到 62.9M，用于训练的剩余部分。像这样的批次大小预热调度与学习率预热的目的一样：它使模型在**梯度噪声规模增长时保持在效率前沿**，从而在整个过程中保持稳定和高效的优化。

另一种有趣的方法是将 **Loss 作为临界批次大小的代理** 。Minimax01 使用了这种方法，在最后阶段他们用 128M 的批次大小进行训练！这有点不同，因为他们**没有增加学习率**，所以他们的批次大小调度**充当了学习率衰减调度**。

正如上面所提到的，选择批次大小和学习率的起始点的一种方法是通过**扩展定律（Scaling Laws）**。让我们看看这些扩展定律是如何工作的，以及它们如何根据你的计算预算来预测这两个超参数。

#### 4.2.4 超参数的扩展定律（Scaling Laws for Hyperparameters）

最优学习率和批次大小不仅仅与模型架构和大小有关，它们还取决于我们的**计算预算（Compute Budget）**，这结合了**模型参数数量**和**训练 Token 数量**两个因素。在实践中，这两个因素相互作用，共同决定了我们的更新应该有多激进或多保守。这就是 **扩展定律（Scaling Laws）** 发挥作用的地方。

扩展定律建立了**经验关系**，描述了当我们增加训练规模时（无论是通过更大的模型还是更多的数据），模型性能如何演变（有关完整历史，请参见本章末尾的“扩展定律”部分）。

但扩展定律也可以帮助我们**预测**如何随着训练规模的扩大而调整关键超参数，例如学习率和批次大小，正如 DeepSeek 和 Qwen 2.5 最近的工作所做的那样。这为我们提供了**有原则的默认值**，而不是完全依赖于超参数扫描。

为了在这种情况下应用扩展定律，我们需要一种量化训练规模的方法。标准指标是**计算预算**，表示为 $C$，以 FLOPs（浮点运算次数）为单位进行测量，可以近似为：

$$C \approx 6 \times N \times D$$

其中，$N$ 是模型参数的数量（例如，1B = 1e9），$D$ 是训练 Token 的数量。这通常以 **FLOPs（浮点运算次数）**来衡量，这是一种**硬件无关**的方式来量化实际完成了多少计算。但如果 FLOPs 感觉太抽象，你可以这样想：训练一个 1B 参数模型在 100B Token 上消耗的 FLOPs，大约是训练一个 2B 模型在 100B Token 上，或一个 1B 模型在 200B Token 上的 **1/2**。

常数 6 来自于经验估计，即训练一个 Transformer 大约需要**每参数每 Token 6 个 FLOPs**。

现在，这与学习率有什么关系？我们可以推导出**将最优学习率和批次大小预测为总计算预算 ($C$) 的函数**的扩展定律。它们有助于回答以下问题：

*   当我从 1B 扩展到 7B 参数时，学习率应该如何改变？
*   如果我将训练数据加倍，我应该调整学习率吗？

让我们通过 DeepSeek 使用的方法来看看这是如何工作的：

首先，我们选择我们的学习率调度方案，理想情况下是 **WSD**，因其灵活性。然后，我们在**一系列计算预算**下（例如，1e17, 5e17, 1e18, 5e18, 1e19, 2e19 FLOPs）用**不同的批次大小和学习率组合**来训练模型。简单来说：我们用不同的 Token 数量训练不同的模型大小，并测试不同的超参数设置。这就是 WSD 调度方案的优势所在：我们可以将同一次训练运行扩展到不同的 Token 数量，而无需重启。

对于每种设置，我们对学习率和批次大小进行**扫描**，并识别出那些能带来**近乎最优性能**的配置，这通常定义为在最佳验证 Loss（在独立的验证集上计算，其分布与训练集相似）的**一个很小的裕度内**（例如，0.25%）。

每个近乎最优的配置都为我们提供了一个数据点——一个 **（计算预算 $C$，最优学习率 $\eta$）** 或 **（$C$，最优批次大小 $B$）** 的元组。当在**对数-对数（log-log）**尺度上绘制时，这些关系通常遵循**幂律（Power-Law）**行为，呈现为近似的直线（如下图所示）。通过拟合这些数据点，我们可以提取出描述**最优超参数如何随计算预算演变**的扩展定律。

这个过程中的一个重要发现是：对于固定的模型大小和计算预算，**性能在广泛的超参数范围内保持稳定**。这意味着存在一个**宽泛的最佳点**，而不是一个狭窄的最优点。我们不需要找到完美的值，只需一个**足够接近的值**即可，这使得整个过程更加实用。

下方（*注：指代原文图表*）你可以看到 DeepSeek 推导出的扩展定律的结果，其中每个点代表一个近乎最优的设置。

这些结果背后的核心直觉是：随着训练变得**更大、更长**，我们希望**更稳定的更新**（因此，**学习率更小**），以及**更高效的梯度估计**（因此，**批次大小更大**）。

这些扩展定律为我们提供了**学习率和批次大小的起点**。但我们的目标不是“每个梯度的最优样本数”，而是“在我们时间和 GPU 天数约束下可达到的**更低 Loss**”，同时仍然从每个 Token 中提取出**完整的信号**。

在实践中，你可能能够将批次大小增加到**超出预测的最优批次大小**，以显著提高吞-吐量而不会明显损害数据效率，直到我们前面讨论的**临界批次大小**。

#### 4.2.5 SmolLM3 的最终选择

那么，我们最终为 SmolLM3 使用了什么呢？

在启动 SmolLM3 之前的消融实验阶段，我们在一个 1B 模型上用 1000 亿 Token 比较了 **AdamW、AdEMAMix 和 Muon**。

*   **Muon** 在适当调优后可以**超越 AdamW**，但对学习率**敏感**，且容易**发散**。
*   **AdeMaMix** 不那么敏感，并且达到了与 Muon **相似的 Loss**。
*   **AdamW** 是**最稳定**的，但达到的最终 Loss 比经过调优的替代方案要**高**。

然而，当我们扩展到 **30 亿参数**时，我们遇到了 Muon 和 AdeMaMix **更频繁的发散**。这可能是由于我们在完成消融实验后发现的一个**并行化 Bug**，尽管我们尚未证实这一点。最终，我们决定使用 **AdamW**（beta1: 0.9, beta2: 0.95），权重衰减为 0.1，梯度裁剪为 1。总而言之，是一个**非常普通（Vanilla）的设置**。

对于学习率调度，我们选择了 **WSD**。我们在 SmolLM2 中成功使用过它，并且它被证明是我们在**易用性、总训练时长灵活性以及运行中途衰减实验能力**方面做出的最佳决策之一。我们运行了学习率扫描，并最终确定为 **2e-4**。

对于全局批次大小，我们测试了从 **200 万到 400 万 Token** 的值，但发现对 Loss 或下游性能的影响**微乎其微**，因此我们选择了 **236 万 Token**——这个大小为我们带来了**最佳的吞吐量**。

#### 4.2.6 优化器选择准则 (Rules of Engagement)

**一句话总结：平衡探索与执行，“完成”比“完美”更重要。**

我们已经讨论了很多关于“做什么”（优化器、学习率、批次大小）的问题，但 **“怎么做”** 同样重要。我们如何决定什么值得实验？我们如何安排时间？我们何时应该停止探索，直接开始训练？

1.  **明智地分配你在探索和执行之间的时间。** 花费数周时间去完善一种新方法带来的微小改进，远不如将同样的算力投入到**更好的数据策划**或更**彻底的架构消融**中有价值。根据我们的经验，尽管这可能会让架构爱好者失望，但**最大的性能提升通常来自数据策划**。
2.  **当有疑问时，选择灵活性和稳定性，而非峰值性能。** 如果两种方法表现同样出色，选择**提供更多灵活性**或具有**更好实现成熟度和稳定性**的那一种。像 WSD 这样允许我们延长训练或运行中途实验的学习率调度，比一个可能收敛得略好但僵化的调度更有价值。
3.  **知道何时停止优化并开始训练。** 总有下一个超参数需要调优，总有下一个优化器可以尝试。为**探索设定一个截止日期并遵守它**——我们**真正完成训练**的模型，永远胜过我们**从未开始**的那个**完美模型**。
4.  **完美是优秀的敌人（Perfect is the enemy of good）**，尤其是在我们面对有限的计算预算和截止日期时。

### 4.3 扩展定律：多少参数？多少数据？

在深度学习的早期，当语言模型（以及训练它们的集群）还不够“大”时，训练运行通常不会受到计算资源的严重限制。训练模型时，你只需选择能装入硬件的**最大模型**和**最大批次大小**，然后一直训练，直到模型开始过拟合或你用完了数据。然而，即使在早期，人们也已经意识到**规模是有益的**——例如，Hestness et al. 在 2017 年提供了一套全面的结果，表明训练**更大、更久**的模型能产生**可预测的收益**。

在大型语言模型时代，我们**总是受到计算资源的限制**。为什么？

Kaplan et al. 的《神经网络语言模型的扩展定律》（Scaling Laws for Neural Language Models）论文将早期关于可扩展性的概念形式化了，该研究表明，语言模型性能在**多个数量级的规模**上具有**惊人的可预测性**。这引发了语言模型在**大小和训练时长**上的爆炸式增长，因为它提供了一种**准确预测**通过增加规模能带来多少性能提升的方法。因此，构建更好语言模型的竞赛变成了用**不断增长的计算预算**，在**更大量的数据**上训练**更大模型**的竞赛，语言模型的开发也迅速变得**受计算资源限制**。

当面临计算限制时，最重要的问题是：**应该训练更大的模型，还是用更多的数据进行训练？**

令人惊讶的是，Kaplan et al. 的扩展定律表明，将**更多计算资源分配给模型规模**比之前的最佳实践更有优势——这激励了例如用相对适度的 Token 预算（3000 亿 Token）来训练庞大的 GPT-3 模型（1750 亿参数）。

在重新审视时，Hoffman et al. 发现了 Kaplan et al. 方法中的一个方法论问题，最终重新推导了扩展定律，建议将**更多计算资源分配给训练时长**。这表明，例如，GPT-3 的 1750 亿参数模型的**计算最优训练**应该消耗 **3.7 万亿 Token**！

这一发现（通常被称为 **Chinchilla 定律**）将该领域从 **“让模型更大”** 转向了 **“将它们训练得更久、更好”**。

然而，大多数现代训练仍然没有严格遵循 Chinchilla 定律，因为它们有一个缺点：它们旨在预测在**给定计算预算**下能获得**最佳性能**的模型大小和训练时长，但它们未能考虑到**更大的模型在训练后更昂贵**的事实。换句话说，我们实际上可能更愿意用给定的计算预算来**将一个较小的模型训练更长时间**——即使这在计算上不是“最优”的——因为这将使**推理成本更便宜** (Sardana et al., de Vries)。如果我们预计一个模型将会有大量的推理使用（例如，因为它将作为开源模型发布 🤗），情况就可能如此。

最近，这种 **“过度训练（Overtraining）”** 模型，即超过扩展定律建议的训练时长的做法，已经成为标准实践，也是我们在开发 SmolLM3 时采取的方法。

虽然扩展定律为给定特定计算预算的模型大小和训练时长提供了建议，但选择过度训练意味着你必须**自己决定这些因素**。对于 SmolLM3，我们从选择 **30 亿参数**的目标模型大小开始。基于近期类似规模的模型，如 Qwen3 4B、Gemma 3 4B 和 Llama 3.2 3B，我们认为 3B 大小**足以具备有意义的能力**（例如推理和工具调用），但又**足够小**以实现超快速的推理和高效的本地使用。

为了选择训练时长，我们首先注意到最近的模型被**极度过度训练**——例如，前面提到的 Qwen3 系列据称训练了 36 万亿 Token！因此，训练时长通常由**可用的计算资源**决定。我们预计使用 384 块 H100s 一个月时间作为训在 **11 万亿 Token** 上进行训练的预算（假设 MFU 约为 30%）。

**扩展定律的价值**

尽管存在这些偏差，扩展定律仍然具有**实际价值**。它们为**实验设计提供基线**，人们经常使用 Chinchilla 最优设置来获取消融实验的信号，并且它们有助于预测一个模型大小是否能达到目标性能。正如 de Vries 在这篇博客中指出的，通过缩小模型大小，你可以达到一个**临界模型大小**：达到给定 Loss 所需的**最小容量**，低于这个容量，你将开始获得**递减的回报**。

现在我们已经确定了模型架构、训练设置、模型大小和训练时长，我们需要准备两个关键组件：**将教会我们模型的数据混合配比**，以及**将可靠地训练它的基础设施**。SmolLM3 的架构设定为 30 亿参数，我们需要策划一个能带来强大**多语言、数学和代码性能**的数据混合配比，并建立一个足够稳健以支持 **11 万亿 Token** 训练的基础设施。将这些基础工作做好至关重要，即使是最好的架构选择也无法从糟糕的数据策划或不稳定的训练系统中拯救我们。

## 5. 数据策划的艺术

想象一下：你花了数周时间完善架构、调整超参数，并搭建了最稳健的训练基础设施。你的模型完美收敛，然后……它无法编写连贯的代码，在基础数学上挣扎，甚至可能在句子中途切换语言。**哪里出错了？**

答案通常在于**数据**。

当我们沉迷于花哨的架构创新和超参数扫描时，**数据策划（Data Curation）**通常决定了我们的模型是成为**真正有用的工具**，还是只是又一个昂贵的实验。这是在**随机的网络爬取**与**精心策划的高质量数据集**之间做选择的区别，后者能真正教会模型我们希望它学习的技能。

如果说**模型架构**定义了你的模型**如何学习**，那么**数据**就定义了它**学什么**，任何计算资源或优化器调优都无法弥补在**错误内容**上进行训练的损失。

此外，搞定训练数据不仅关乎拥有好的数据集，更关乎组建**正确的混合配比（Mixture）**：平衡相互冲突的目标（例如强大的英语能力 vs. 稳健的多语言能力），并调整数据比例以符合我们的性能目标。这个过程与其说是寻找一个通用的最佳混合，不如说是提出正确的问题，并设计具体的计划来回答它们：

1.  我们希望我们的模型擅长什么？
2.  每个领域最好的数据集是什么？我们如何混合它们？
3.  对于我们的目标训练规模，我们是否有足够的高质量数据？

本节旨在通过 **原则性方法、消融实验和一点点“炼金术”** 的混合，来驾驭这些问题，将一堆优秀的数据集变成一个优秀的训练混合配比。

### 5.1 什么是好的数据混合配比？为什么它最重要？

我们对语言模型寄予厚望：它们应该能帮助我们写代码、提供建议、回答几乎任何问题、使用工具完成任务等等。像网络这样丰富的预训练数据源，并不能覆盖这些任务所需的全部知识和能力范围。因此，最近的模型额外依赖于**针对特定领域**（如数学和编码）的更专业的预训练数据集。

*(我们过去在策划数据集方面做了很多工作，但对于 SmolLM3，我们主要使用了预先存在的数据集。要了解更多关于数据集策划的信息，请查看我们关于构建 FineWeb 和 FineWeb-Edu、FineWeb2、Stack-Edu 和 FineMath 的报告。)*

#### 5.1.1 数据混合的非直观性

如果你是训练语言模型的新手，找到一个好的数据混合配比可能看起来很简单：确定你的目标能力，为每个领域收集高质量的数据集，然后将它们组合起来。

现实更加复杂，因为**某些领域可能会为了你的训练预算而相互竞争**。当专注于像编码这样的特定能力时，增加与任务相关的数据（如源代码）的权重是很诱人的。然而，**增加一个来源的权重就意味着隐式地降低了所有其他来源的权重**，这可能会损害语言模型在其他环境中的能力。因此，在不同来源的数据集合上进行训练，需要在下游能力之间达到某种**平衡**。

此外，在所有这些来源和领域中，通常有一部分 **“高质量”数据** 对提高语言模型的能力特别有帮助。为什么不把所有低质量的数据都扔掉，只在最高质量的数据上训练呢？对于 SmolLM3 的 11 万亿 Token 的庞大训练预算来说，进行这种极端过滤将导致数据重复多次。先前的工作表明，这种重复可能是有害的 (Muennighoff et al., 2025)，因此我们理想情况下应该能够在最大化模型性能的同时，利用**更高质量和较低质量的数据**。

为了平衡不同来源的数据并利用高质量数据，我们需要仔细设计**混合配比**：来自每个来源的训练文档的相对比例。由于语言模型在某个特定任务或领域上的性能很大程度上取决于它看到的与该任务相关的**数据量**，调整混合权重为我们提供了一种**直接平衡模型跨领域能力**的方法。因为这些权衡是**依赖于模型且难以预测**的，**消融实验至关重要**。

但混合配比在整个训练过程中**不必保持不变**。通过在训练过程中调整混合配比——我们称之为**多阶段训练（Multi-Stage Training）**或**课程学习（Curriculum）**——我们可以更好地利用高质量和较低质量的数据。

#### 5.1.2 训练课程的演变

在大型语言模型训练的早期，标准方法是在整个训练运行中**固定一个单一的数据混合配比**。像 GPT-3 和早期版本的 Llama 都是在从头到尾的静态混合配比上训练的。

最近，该领域已转向**多阶段训练** (Allal et al., 2025)，即数据混合配比在训练过程中发生变化。主要动机是：语言模型的最终行为受到**训练末期看到的数据**的强烈影响 (Y. Chen et al., 2025b)。这一洞察催生了一种实用策略：**在训练早期增加更丰富来源的权重，在接近尾声时混入更小、更高质量的来源**。

一个常见的问题是：如何决定何时改变混合配比？虽然没有通用的规则，但我们通常遵循以下原则：

1.  **性能驱动的干预：** 监控关键基准的评估指标，并调整数据集混合以解决特定的**能力瓶颈**。例如，如果数学性能停滞不前，而其他能力持续提高，这就是引入更高质量数学数据的信号。
2.  **为后期阶段保留高质量数据：** 小而高质量的数学和代码数据集在**退火阶段**（学习率衰减的最后阶段）引入时**影响最大**。

现在我们已经确定了为什么混合配比重要以及课程学习如何运作，接下来让我们讨论如何同时调整两者。

### 5.2 消融实验设置：如何系统地测试数据“配方”

在测试数据混合配比时，我们的方法与运行架构消融实验类似，但有一个区别：我们尝试在**目标模型规模**上运行它们。小型和大型模型具有不同的容量，例如，一个非常小的模型可能难以处理多种语言，而一个更大的模型可以吸收它们而不会牺牲其他地方的性能。因此，在**过小的规模**上运行数据消融实验，有**得出关于最优混合的错误结论**的风险。

对于 SmolLM3，我们直接在 **3B 模型**上运行了我们的主要数据消融实验，使用了 500 亿和 1000 亿 Token 的较短训练运行。我们还使用了另一种消融设置：**退火实验（Annealing Experiments）**。我们没有用不同的混合配比从头开始训练，而是从主运行中取一个**中间检查点**（例如在 7T Token 处），然后用**修改后的数据组成**继续训练。这种方法允许我们测试用于多阶段训练（即在训练中途改变训练混合）的数据混合更改，并在最近的工作中（如 SmolLM2、Llama 3 和 Olmo 2）得到使用。

在评估方面，我们将我们的基准套件扩展到包括**多语言任务**，与我们的标准英语评估一起，确保我们能够正确评估不同语言比例之间的权衡。

最近的工作提出了自动寻找最优数据比例的方法，包括：

*   **DoReMi** (Xie et al., 2023)：使用一个小型代理模型来学习最小化验证 Loss 的领域权重。
*   **Rho Loss** (Mindermann et al., 2022)：根据保留集 Loss 选择单个训练点，优先选择可学习、与任务相关且模型尚未学会的样本。
*   **RegMix** (Q. Liu et al., 2025)：通过正则化回归确定最优数据混合比例，平衡多个评估目标和数据领域的性能。

我们在过去的项目中尝试了 DoReMi 和 Rho Loss，但发现它们倾向于收敛到大致反映**数据集大小自然分布**的分布，基本上是建议我们更多地使用我们拥有更多的东西。虽然理论上很吸引人，但在我们的设置中，它们**没有胜过仔细的手动消融实验**。最近的 SOTA 模型仍然依赖于通过**系统性消融实验和退火实验**进行的手动混合调优，这也是我们为 SmolLM3 采用的方法。

### 5.3 SmolLM3：策划数据混合（网络、多语言、数学、代码）

对于 SmolLM3，我们想要一个能处理**英语和其他多种语言**，并在**数学和代码**方面表现出色的模型。这些领域——网络文本、多语言内容、代码和数学——在大多数 LLM 中都很常见，但我们在这里描述的过程同样适用于你为**低资源语言**或**特定领域**（如金融或医疗保健）进行训练的情况。方法是相同的：**识别好的候选数据集，运行消融实验，并设计一个平衡所有目标领域的混合配比**。

*(我们在这里不讨论如何构建高质量的数据集，因为我们已经在早期的工作中（FineWeb、FineWeb2、FineMath 和 Stack-Edu）详细说明了这一点。相反，本节重点关注我们如何将这些数据集组合成一个有效的预训练混合配比。)*

#### 5.3.1 建立在经过验证的基础上

在预训练数据方面，好消息是我们很少需要从零开始。开源社区已经为大多数常见领域构建了强大的数据集。有时我们需要创造一些新的东西——就像我们用 Fine 系列（FineWeb、FineMath 等）所做的那样——但更多时候，挑战在于**选择和组合现有来源**，而不是重新发明它们。

SmolLM2 已经在 1.7B 参数下为英语网络数据建立了强大的配方，并确定了我们能接触到的最好的数学和代码数据集。我们的目标是在此基础上扩展到 30 亿参数，同时增加某些能力：**稳健的多语言能力、更强的数学推理和更好的代码生成**。

#### 5.3.2 英语网络数据：基础层

网络文本构成了任何通用 LLM 的**支柱**，但**质量和数量同等重要**。

从 SmolLM2，我们知道 **FineWeb-Edu** 和 **DCLM** 是训练时最强大的**开放英语网络数据集**。它们共同为我们提供了 **5.1 万亿 Token** 的高质量英语网络数据。问题是：最佳混合比例是什么？FineWeb-Edu 有助于教育和 STEM 基准，而 DCLM 改善了常识推理。

遵循 SmolLM2 的方法，我们在我们的 3B 模型上，用 1000 亿 Token 运行了一次扫描，测试了 20/80、40/60、50/50、60/40 和 80/20 的比例（FineWeb-Edu/DCLM）。将它们混合（大约 60/40 或 50/50）得到了**最佳的权衡**。我们在 3B 模型上，用 1000 亿 Token 重新运行了 SmolLM2 论文中的相同消融实验，并得出了相同的结论。

**我们为第一阶段（Stage 1）使用了 50/50 的比例。**

我们还添加了其他数据集，如 Pes2o、Wikipedia & Wikibooks 和 StackExchange，这些数据集对性能没有任何影响，但我们包含它们是为了**提高多样性**。

#### 5.3.3 多语言网络数据

对于多语言能力，我们瞄准了另外 5 种语言：**法语、西班牙语、德语、意大利语和葡萄牙语**。我们从 **FineWeb2-HQ** 中选择了它们，总共为我们提供了 6280 亿 Token。我们还以较小的比例包含了其他 10 种语言，如中文、阿拉伯语和俄语，不是为了在这些语言上达到 SOTA 性能，而是为了让人们能够轻松地在这些语言上对 SmolLM3 进行**持续预训练**。

关键问题是：我们的网络数据中应该有多少是非英语的？我们知道，模型在一种语言或领域中看到的数据越多，它在该语言或领域上的表现就越好。权衡来自于我们固定的计算预算：增加一种语言的数据意味着减少包括英语在内的其他语言的数据。

通过在 3B 模型上的消融实验，我们发现网络混合中 **12% 的多语言内容**达到了正确的平衡，在**不降低英语基准性能**的情况下提高了多语言性能。这符合 SmolLM3 的预期用途，其中英语仍将是主要语言。同样值得注意的是，只有 6280 亿 Token 的非英语数据，而英语数据有 5.1 万亿 Token，要再提高比例就需要更多地重复多语言数据。

#### 5.3.4 代码数据

我们第一阶段的代码来源是从 **The Stack v2** 和 **StarCoder2** 训练语料库中提取的：

*   The Stack v2（16 种语言），作为我们的基础，按照 StarCoder2Data 的方式过滤。
*   StarCoder2 GitHub Pull Requests，用于真实世界的代码审查推理。
*   Jupyter 和 Kaggle Notebooks，用于可执行的、逐步的工作流程。
*   GitHub Issues 和 StackExchange 线程，用于围绕代码的上下文讨论。

Aryabumi et al. (2024) 强调，代码不仅能提高语言模型的编码能力，还能提高**自然语言推理和世界知识**等方面的性能，并建议在训练混合中使用 **25% 的代码**。受此启发，我们以 25% 的代码开始了我们的消融实验。然而，我们观察到在英语基准（HellaSwag、ARC-C、MMLU）上**性能显著下降**。将代码比例降至 **10%** 后，我们没有看到与 0% 代码相比在英语基准套件上的改进，但我们还是包含了它，因为代码是模型中一个非常重要的能力。

我们推迟了添加 **Stack-Edu**——我们对 StarCoder2Data 进行教育性过滤的子集——直到后期阶段，遵循了为最大化后期训练影响而**分阶段引入高质量数据**的原则。

#### 5.3.5 数学数据

数学遵循了与代码类似的理念。早期，我们使用了更大、更通用的数据集 **FineMath3+** 和 **InfiWebMath3+**，后期我们**上采样了 FineMath4+** 和 **InfiWebMath4+**，并引入了新的高质量数据集：

*   **MegaMath** (Zhou et al., 2025)
*   指令和推理数据集，如 **OpenMathInstruct** (Toshniwal et al., 2024) 和 **OpenMathReasoning** (Moshkov et al., 2025)

我们在第一阶段使用了 **3% 的数学数据**，在 FineMath3+ 和 InfiWebMath3+ 之间平均分配。由于只有 540 亿 Token 可用，并且估计第一阶段需要 8T 到 9T Token，使用超过 3% 的数学数据将需要在该数据集上进行超过 5 个周期（Epochs）的训练。

#### 5.3.6 为新阶段寻找正确的混合配比

虽然我们从头开始运行消融实验来确定第一阶段的混合配比，但为了测试新阶段的新数据集（在我们的案例中是两个新阶段），我们使用了**退火消融实验**：我们在大约 7T Token 处（第一阶段的后期）取一个检查点，并用以下设置运行了 500 亿 Token 的退火实验：

*   **40% 基线混合：** 我们一直训练的精确的第一阶段混合。
*   **60% 新数据集：** 我们想要评估的候选数据集。

例如，为了测试 MegaMath 是否会提高我们的数学性能，我们运行了 40% 的第一阶段混合（保持 75/12/10/3 的领域分割）和 60% 的 MegaMath。

随着我们的数据经过精心策划并通过消融实验验证了混合配比，我们准备好踏上真正的训练之旅。接下来的章节将讲述 **SmolLM3 长达一个月的训练运行故事**：准备工作、意想不到的挑战以及在此过程中学到的教训。

## **6. 训练马拉松**

恭喜你走到了这一步！真正的乐趣即将开始。

此时，我们已经万事俱备：经过验证的架构、最终确定的数据混合配比，以及调优好的超参数。剩下的唯一事情就是设置基础设施并按下“训练”按钮。

对于 SmolLM3，我们在 **384 块 H100 GPU**（48 个节点）上训练了**近一个月**，处理了 **11 万亿 Token**。本节将带你了解在一次漫长的训练运行中**实际会发生什么**：起飞前的检查、不可避免的意外，以及我们如何保持稳定。你将亲眼看到为什么坚实的消融实验实践和可靠的基础设施都至关重要。我们将在最后一章涵盖 GPU 硬件、存储系统和优化吞吐量的技术基础设施细节。

我们的团队已经经历过很多次这样的过程：从 StarCoder 和 StarCoder2，到 SmolLM、SmolLM2，再到现在的 SmolLM3。**每一次运行都不同**。即使你已经训练了十几个模型，每一次新的运行都会以一种全新的方式给你带来“惊喜”。本节旨在为你增加胜算，让你为这些惊喜做好准备。

### **6.1 起飞前检查清单：在按下“训练”前要核实什么**

在按下“训练”按钮之前，我们会过一遍检查清单，以确保一切都能端到端地工作：

**基础设施准备情况：**

1.  **Slurm 预留：** 如果你的集群支持 Slurm 预留，请使用它们。对于 SmolLM3，我们在整个运行期间都有一个**固定的 48 节点预留**。这意味着没有排队延迟、吞吐量一致，并且能够随着时间的推移跟踪节点健康状况。
2.  **GPU 压力测试：** 在启动前对 GPU 进行压力测试（我们使用 GPU Fryer 和 DCGM Diagnostics），以捕获**节流（throttling）**或**性能下降**。对于 SmolLM3，我们发现了两块 GPU 节流，并在开始运行前更换了它们。
3.  **避免存储膨胀：** 我们的系统将每个检查点上传到 S3，然后在保存下一个检查点后立即删除本地副本，因此我们从不在快速的本地 GPU SSD 上存储超过一个检查点。

**评估设置：**

*   **自动化：** 评估工作非常耗时。即使一切都已实现，手动运行、记录结果和制作图表每次都会耗费数小时。因此，**尽量完全自动化它们**，并确保在运行开始前它们能够正确运行和记录。对于 SmolLM3，每个保存的检查点都会**自动触发集群上的评估作业**，并将结果记录到 Wandb 和 Trackio。

**检查点与自动恢复系统：**

*   **验证：** 验证检查点是否正确保存，以及训练作业是否可以**从最新的检查点恢复而无需手动干预**。在 Slurm 上，我们使用 `--requeue` 选项，这样失败的作业就会自动重新启动，并从最近的检查点恢复。

**指标记录：**

*   **确认：** 确认你正在记录所有你关心的指标：评估分数、吞吐量（Token/秒）、训练 Loss、梯度范数、节点健康状况（GPU 利用率、温度、内存使用），以及任何针对你运行的自定义调试指标。

**训练配置健全性检查：**

*   **仔细检查：** 仔细检查你的训练配置、启动脚本和 Slurm 提交命令。

### **6.2 规模化带来的“惊喜”**

在为 SmolLM3 运行了广泛的消融实验后，我们准备好了进行全面规模的运行。我们在 1000 亿 Token 上进行的 3B 消融实验看起来很有希望。与 SmolLM2 相比，架构上的更改（详见架构选择：GQA、NoPE、文档掩码、分词器）要么改进要么维持了性能，并且我们找到了一个很好的数据混合配比，平衡了英语、多语言、代码和数学的性能（参见数据策划的艺术）。我们将配置优化为在 384 块 GPU（48 个节点）上达到**约 30% 的 MFU**。

我们准备好迎接那个大家伙：**11 万亿 Token**。就在那时，现实开始给我们**抛出曲线球**。

#### **6.2.1 谜团 #1 – 消失的吞吐量**

启动后几小时内，**吞吐量骤降**。这是一个巨大的跳跃，伴随着**反复的急剧下降**。

这在任何消融实验中都没有发生过，那么什么变了？**三件事**：

1.  **硬件状态**会随时间变化。在消融实验中工作良好的 GPU 可能会在持续负载下失败，网络连接也可能退化。
2.  **训练数据集的大小**。我们现在使用了完整的 **~24 TB 训练数据集**，而不是消融实验中的较小子集，尽管数据源本身是相同的。
3.  **训练步数**。我们将步数设置为 11 万亿 Token 的真实步数，而不是短暂的 1000 亿 Token 消融实验的范围。

其他一切都与吞吐量消融实验完全相同：节点数量、数据加载器配置、模型布局和并行化设置……

直觉上，数据集大小和步数都不应该导致吞吐量下降，所以我们自然首先怀疑是**硬件问题**。我们检查了我们的节点监控指标，发现巨大的吞吐量跳跃与**磁盘读取延迟的尖峰**相关。这直接将我们指向了我们的**数据存储**。

对于 SmolLM3 的 24TB 数据集，我们最初将数据存储在 **FSx (Weka)** 中。随着 24TB 的训练数据，再加上其他几个团队已经使用的存储，我们正在将 Weka 的存储推向极限。所以它开始在训练中途**驱逐数据集分片**，这意味着我们必须将它们重新取回，从而造成**停顿**，这解释了巨大的吞吐量跳跃。更糟糕的是：没有办法将我们的数据集文件夹**固定为热数据**以供整个训练使用。

**修复 #1 – 更换数据存储**

我们没有找到在 Weka 中将我们的数据集文件夹固定为热数据的方法，所以我们尝试更换存储方式。直接从 S3 流式传输速度很慢，所以我们决定将数据存储在**每个节点的本地存储 `/scratch`** 中。

这带来了一个问题：如果一个节点**死亡并被替换**，新的替换 GPU 上**没有数据**。用 s5cmd 从 S3 下载 24TB 数据需要 **3 小时**。我们通过从另一个健康的节点使用 fpsync 复制，而不是通过 S3，将时间缩短到 **1 小时 30 分钟**。鉴于所有节点都在同一个数据中心，这样做更快。

尽管如此，每个节点故障需要 1 小时 30 分钟的停机时间，并且需要立即手动将数据复制到新节点，这仍然很痛苦。最终使之可以忍受的技巧是：在我们的 Slurm 预留中**预留一个备用节点**，并**预加载好数据集**。如果一个节点死亡，我们立即用备用节点替换它，实现**零恢复延迟**。空闲时，备用节点可以运行评估或开发作业，所以没有浪费。

这解决了谜团 #1……我们当时是这么想的。

#### **6.2.2 谜团 #2 – 持续的吞吐量下降**

即使在移至 `/scratch` 之后，**个别的吞吐量下降仍在发生**，尽管我们在硬件监控指标中没有发现任何异常。下方的图表（*注：指代原文图表*）比较了我们在解决存储问题后得到的吞吐量（橙色）与我们在消融实验期间得到的吞吐量（蓝色）。如你所见，下降变得**更加剧烈**。

仍然怀疑是硬件问题，我们决定在**更少的节点**上进行测试。用 384 块 GPU，很有可能有什么东西会出故障。令人惊讶的是，我们在**单个节点**上复现了**完全相同的吞吐下降**，无论我们测试哪个特定节点。这**排除了硬件问题**。

还记得我们从消融实验中改变的三件事吗？我们已经通过移动到本地节点存储解决了数据存储问题。硬件现在被排除了。只剩下**一个变量：步数**。

我们通过回滚到**更小的步数**（从 300 万到 3.2 万）来测试这一点，吞吐量下降**变小了**！更大的步数产生了更剧烈、更频繁的下降。

为了测试这一点，我们运行了完全相同的配置，只将训练步数从 3.2 万更改为 320 万。结果很清楚：**较短的运行**只有微小的吞吐量下降，而**较长的步数**则产生了更剧烈、更频繁的下降。

所以问题**不是硬件**，而是**软件瓶颈**，很可能在**数据加载器**中！因为大多数其他训练组件处理每个批次的方式与步数无关。

就在那时，我们意识到我们**从未真正用 nanotron 的数据加载器进行过大规模的预训练**。SmolLM2 是用一个派生自 Megatron-LM 的数据加载器（TokenizedBytes）通过一个内部的 nanotron 包装器进行训练的，吞吐量一直很稳定。对于 SmolLM3，我们切换到了 nanotron 的**内置数据加载器（nanosets）**。

在深入研究其实现后，我们发现它**天真地构建了一个巨大的索引**，这个索引随着每个训练步骤而增长。对于非常大的步数，这导致了更高的共享内存，从而引发了吞吐量下降。

**修复 #2 – 引入 TokenizedBytes 数据加载器**

为了确认数据加载器确实是罪魁祸首，我们用我们的内部 SmolLM2 框架，使用 **TokenizedBytes 数据加载器**启动了相同的配置。**没有下降**。即使在 48 个节点上使用相同的数据集也是如此。

最快的解决方案：将这个数据加载器**复制到 nanotron 中**。下降消失了，吞吐量回到了目标水平。

我们准备好重新启动了……直到下一个曲线球。

#### **6.2.3 谜团 #3 – 嘈杂的 Loss**

用新的数据加载器，我们没有了吞吐量下降，但 **Loss 曲线看起来更嘈杂**。

nanosets 一直产生更平滑的 Loss，这种差异让我们想起了**一次古老的调试战争**：几年前，我们在我们的预训练代码中发现了一个**洗牌 Bug**，其中**文档被洗牌，但一个批次内的序列没有**，导致了小的尖峰。

检查我们新的数据加载器证实了这一点：它正在**按顺序**从每个文档中读取序列。这对于短文件来说没问题，但对于像代码这样的领域，一个**单一的、长的、低质量的文件**可能会填满整个批次，并导致 **Loss 尖峰**。

**修复 #3 – 在序列级别进行洗牌**

我们有两个选择：

1.  更改数据加载器以进行**随机访问**（风险：更高的内存使用）。
2.  **离线预洗牌**分词后的序列。

由于启动运行的时间压力和我们的集群预留正在运行，我们选择了**选项 #2**，作为**更安全、更快的修复**。分词后的数据已经在每个节点上，所以在本地重新洗牌成本很低（约 1 小时）。我们还为每个周期用**不同的种子**生成了洗牌后的序列，以避免在周期之间重复洗牌模式。

#### **6.2.4 启动，第二次尝试**

到目前为止，我们已经有了：

*   稳定的吞吐量（`/scratch` 存储 + 备用节点策略）
*   没有因步数引起的下降（TokenizedBytes 数据加载器）
*   干净的、序列级别的洗牌（每个周期离线预洗牌）

我们**重新启动**了。这一次，一切都稳住了。Loss 曲线平滑，吞吐量一致，我们终于可以**专注于训练**而不是救火了。

#### **6.2.5 谜团 #4 – 不尽如人意的性能**

在修复了吞吐量和数据加载器问题后，我们再次启动了运行，并顺利地训练了前两天。吞吐量稳定，Loss 曲线看起来符合预期，日志中也没有任何问题。然而，在大约 **1 万亿 Token** 的标记点，**评估结果揭示了一些意想不到的事情**。

作为我们监控的一部分，我们评估中间检查点并与历史运行进行比较。例如，我们有 SmolLM2 (1.7B) 的中间检查点，它是用类似的配方训练的，所以我们可以跟踪两个模型在相同训练阶段的进展。结果令人费解：尽管拥有**更多的参数**和**更好的数据混合**，但 3B 模型在相同训练点的**表现比 1.7B 更差**。Loss 仍在下降，基准分数也在提高，但**改进速度明显低于预期**。

鉴于我们已经彻底测试了 SmolLM3 中引入的每一个架构和数据更改，并且验证了训练框架，两个训练设置之间只剩下少数几个未经测试的差异。最明显的是**张量并行（Tensor Parallelism）**。SmolLM2 可以放在单个 GPU 上，并且没有使用 TP 进行训练，而 SmolLM3 需要 **TP=2** 才能装入内存。我们之前没有怀疑它或考虑测试它，因为在 3B 消融实验中使用了 TP，并且它们的结果是合理的。

**修复 #4 - 最终的修复**

为了测试 TP Bug 的假设，我们用与 SmolLM3 完全相同的设置训练了一个 **1.7B 模型**——相同的架构更改（文档掩码、NoPE）、相同的数据混合、相同的超参数——分别在**有和没有 TP** 的情况下进行。差异立竿见影：**TP 版本始终比非 TP 版本有更高的 Loss 和更低的下游性能**。这证实了我们正在处理一个与 TP 相关的 Bug。

然后我们详细检查了 TP 的实现，比较了 TP 和非 TP 运行的权重。问题原来是**微妙但重要的**：我们在**所有 TP 等级（Ranks）**上使用了**相同的随机种子**，而每个等级应该用**不同的种子**进行初始化。这导致了跨分片的**权重初始化相关**，从而影响了收敛。这个影响不是灾难性的——模型仍然在训练和改进——但它引入了足够多的**低效率**，足以解释我们在大规模上观察到的差距。

一旦我们修复了种子，使得每个 TP 等级使用不同的种子，我们重复了消融实验，并确认 TP 和非 TP 运行现在在 Loss 曲线和下游性能上都**匹配了**。为了确保没有其他隐藏问题，我们运行了额外的健全性检查：一个 3B 参数的 SmolLM2 风格（架构和数据上）的运行，以及一个单独的 3B 参数的 SmolLM3 运行，并将两者都与 SmolLM2 的检查点进行比较。结果现在与预期一致：1.7B SmolLM2 的表现比 3B SmolLM2 变体差，而后者又低于 SmolLM3 的 3B 性能。

这次调试过程强化了我们在这篇博客前面概述的核心原则之一：

> **“一个坚实的消融设置的真正价值，并不仅仅在于构建一个好的模型。当我们的主训练运行中不可避免地出问题时（无论我们准备得多么充分，问题总会发生），我们希望对我们所做的每一个决定都充满信心，并能快速识别哪些组件没有经过充分测试，可能是问题的根源。这种准备工作可以节省调试时间，并保持我们的理智。没有什么比盯着一个神秘的训练失败，却不知道 Bug 可能藏在哪里更糟糕的了。”**

因为我们训练中的所有其他组件都已得到验证，我们可以将 **TP** 指向为**唯一可能的原因**，并在发现性能差距的**一天内修复了 Bug**。

至此，我们解决了自启动以来出现的一系列意外问题中的最后一个。**事不过三**，从那时起，剩下的一个月训练相对平稳，只是将数万亿 Token 变成一个完成的模型的稳定工作，偶尔因节点故障而中断重启。

### **6.3 坚持到底**

正如上一节所示，从消融实验扩展到全面预训练并非只是“即插即用”。它带来了意想不到的挑战，但我们成功地识别并解决了每一个问题。本节涵盖了大规模训练运行所需的基本监控设置和注意事项。我们将解决关键问题：在遇到问题后，你何时应该**重启训练**？如何处理在运行**深入后出现的问题**？哪些**指标**真正重要？你应该在整个训练过程中**保持固定的数据混合**吗？

#### **6.3.1 训练监控：超越 Loss 曲线**

我们之所以能发现**张量并行 Bug**，不是因为 Loss 曲线（它看起来没问题），而是因为**下游评估结果落后于预期**。此外，拥有 SmolLM2 的**中间检查点评估结果**至关重要：它们为我们提供了一个健全性检查，让我们早期就发现 3B 模型没有走在正确的轨道上。

因此，如果你正在训练大型模型，请**尽早开始运行下游评估**。如果你正在与一个开源模型进行比较，可以询问作者是否能提供中间检查点，这些可以作为**无价的参考点**。

在基础设施方面，最重要的指标是**吞吐量**，以 **Token/秒** 为单位测量。对于 SmolLM3，我们期望在整个运行过程中吞吐量稳定在 **13,500–14,000 Token/秒** 之间，任何持续的偏差都是一个**危险信号**。但仅有吞吐量是不够的：你还需要**持续的硬件健康监控**来预测和检测硬件故障。我们跟踪的一些关键指标包括：**GPU 温度、内存使用和计算利用率**。我们将它们记录到 Grafana 仪表板，并为硬件异常设置了**实时 Slack 警报**。

#### **6.3.2 修复后重启 vs. 动态修复**

鉴于我们在 1 万亿 Token 后重启了我们的运行，一个重要的问题出现了：当出现问题时，你总是需要重启吗？答案取决于**问题的严重程度和根本原因**。

在我们的案例中，TP 种子 Bug 意味着我们**从一开始就走错了路**，我们一半的权重没有被正确初始化。模型的性能与 SmolLM2 相似，并在相似的点上停滞不前，这意味着我们最终可能会得到一个性能相同但训练成本几乎高出一倍的模型。**重启是合理的**。

然而，许多问题可以在**运行中途进行修正**，以避免浪费计算资源。最常见的问题涉及 **Loss 尖刺**，这些训练 Loss 的突然跳跃可能预示着小问题或发散。

正如 Stas Bekman 在《机器学习工程开放手册》中所说：“训练 Loss 图就像心跳图——有好有坏，还有你应该担心的那些。”

**Loss 尖刺分为两类：**

1.  **可恢复的尖刺：** 这些可以快速恢复（尖刺后立即）或缓慢恢复（需要更多训练步骤才能回到尖刺前的轨迹）。你通常可以继续训练。如果恢复非常缓慢，你可以尝试**回滚到前一个检查点**以跳过有问题的批次。
2.  **不可恢复的尖刺：** 模型要么发散，要么在比尖刺前更差的性能上停滞不前。这些需要比简单地回滚到前一个检查点更重大的干预。

虽然我们不完全理解训练不稳定性，但我们知道它们在**大规模时更频繁**。常见的罪魁祸首，假设架构和优化器都保守，包括：

*   **高学习率：** 这些会在训练早期引起不稳定性，可以通过降低学习率来修复。
*   **坏数据：** 通常是可恢复尖刺的主要原因，尽管恢复可能很慢。这可能在训练深入时，当模型遇到低质量数据时发生。
*   **数据-参数状态相互作用：** PaLM (Chowdhery et al., 2022) 观察到，尖刺通常是由**特定数据批次和模型参数状态**的组合引起的，而不仅仅是“坏数据”。从不同的检查点在相同的有问题批次上训练，并没有重现尖刺。
*   **糟糕的初始化：** OLMo 2 最近的工作 (OLMo et al., 2025) 表明，从缩放初始化切换到简单的正态分布（均值=0，标准差=0.02）可以提高稳定性。
*   **精度问题：** 虽然现在没有人再用 FP16 进行训练了，但 BLOOM 发现与 BF16 相比，它非常不稳定。

**在尖刺发生前，就内置稳定性：**

*   **数据过滤和洗牌：** 确保你的数据干净且充分洗牌可以防止尖刺。例如，OLMo 2 发现，移除重复 n-gram 的文档（1-13 Token 片段重复 32 次以上）显著降低了尖刺频率。
*   **训练修改：** Z-loss 正则化可以防止输出 Logits 过大而不会影响性能。将嵌入层排除在权重衰减之外也有帮助。
*   **架构更改：** QK-Norm（在注意力前对查询和键投影进行归一化）已被证明是有效的。OLMo 2 和其他团队发现它有助于稳定性，有趣的是，Marin 团队发现它甚至可以**在运行中途应用**来修复发散问题。

**当尖刺仍然发生时 - 损害控制：**

即使有这些预防措施，尖刺仍然可能发生。以下是一些修复它们的选项：

*   **跳过有问题的批次：** 回滚到尖刺前并跳过有问题的批次。这是修复尖刺最常见的方法。Falcon 团队 (Almazrouei et al., 2023) 跳过了 10 亿 Token 来解决他们的尖刺，而 PaLM 团队 (Chowdhery et al., 2022) 发现跳过尖刺位置周围 200-500 个批次可以防止复发。
*   **收紧梯度裁剪：** 临时降低梯度范数阈值。
*   **应用架构修复：** 例如 QK-Norm，如 Marin 所做。

我们已经探讨了规模化挑战，从吞吐量下降到 TP Bug，以及早期发现问题的监控实践，以及预防和修复 Loss 尖刺的策略。让我们通过讨论**多阶段训练**如何增强模型的最终性能来结束本章。

### **6.4 中途训练**

现代 LLM 预训练通常涉及**具有不同数据混合的多个阶段**，通常随后是**扩展上下文长度的最后阶段**。例如，Qwen3 (A. Yang, Li, et al., 2025) 使用了三阶段方法：在 30T Token 上，以 4k 上下文进行**通用阶段**；一个**推理阶段**，使用 5T 更高质量的 Token，强调 STEM 和编码；最后是一个**长上下文阶段**，在 32k 上下文长度上使用数百亿 Token。SmolLM3 遵循类似的理念，有**计划的干预**以引入更高质量的数据集和扩展上下文，同时根据性能监控进行**反应性调整**。

正如我们在数据策划部分解释的，数据混合不必在整个训练过程中保持不变。**多阶段训练**允许我们随着训练的进展**策略性地改变数据集比例**。一些干预从一开始就计划好了：对于 SmolLM3，我们知道我们将在第二阶段引入更高质量的 **FineMath4+** 和 **Stack-Edu**，然后在最终衰减阶段添加策划的 **Q&A 和推理数据**。其他干预是**反应性**的，由训练期间的性能监控驱动。例如，在 SmolLM2 中，当我们发现数学和代码性能落后于我们的目标时，我们策划了全新的数据集（FineMath 和 Stack-Edu），并在训练中途引入了它们。这种灵活性——无论是遵循计划好的课程还是适应新出现的差距——使我们能够**最大化我们计算预算的价值**。

#### **6.4.1 第二阶段和第三阶段的混合**

下方的图表（*注：指代原文图表*）显示了我们 3 个训练阶段以及我们在训练期间网络/代码/数学比例的进展。每个阶段的 SmolLM3 训练配置及其确切的数据权重可在此处获得。有关每个阶段背后的理由和组成的更多详细信息，请参阅数据策划部分。

*   **第一阶段：基础训练（8T Token，4k 上下文）**
    基础阶段使用我们的核心预训练混合：网络数据（FineWeb-Edu、DCLM、FineWeb2、FineWeb2-HQ）、来自 The Stack v2 和 StarCoder2 的代码，以及来自 FineMath3+ 和 InfiWebMath3+ 的数学。所有训练都在 4k 上下文长度下进行。
*   **第二阶段：高质量注入（2T Token，4k 上下文）**
    我们引入了更高质量的过滤数据集：用于代码的 **Stack-Edu**，用于数学的 **FineMath4+** 和 **InfiWebMath4+**，以及用于高级数学推理的 **MegaMath**（我们还添加了 Qwen Q&A 数据、合成重写和文代码交错块）。
*   **第三阶段：带推理和 Q&A 数据的学习率衰减（1.1T Token，4k 上下文）**
    在学习率衰减阶段，我们进一步**上采样高质量的代码和数学数据集**，同时引入指令和推理数据，如 **OpenMathReasoning**、**OpenCodeReasoning** 和 **OpenMathInstruct**。Q&A 样本只是简单地连接起来，并用换行符分隔。

#### **6.4.2 长上下文扩展：从 4k 到 128k Token**

上下文长度决定了你的模型可以处理多少文本，这对于分析长文档、保持连贯的多轮对话或处理整个代码库等任务至关重要。SmolLM3 从 4k Token 开始训练，但我们需要扩展到 128k 以适应现实世界的应用。

**为什么在训练中途扩展上下文？**

*   从一开始就在长上下文上训练**计算成本高昂**，因为注意力机制随序列长度**二次方扩展**。
*   研究表明，在训练末期或持续预训练期间，用**几十到一百亿 Token** 扩展上下文，就足以达到良好的长上下文性能 (Gao et al., 2025)。

**顺序扩展：4k→32k→64k**

我们没有直接跳到 128k。相反，我们**分阶段逐渐扩展上下文**，让模型在每个长度上都有时间适应，然后再进一步推进。我们运行了两个长上下文阶段：首先从 **4k 到 32k**，然后从 **32k 到 64k**（128k 的能力来自**推理时的外推**，而不是训练）。

我们发现，为每个阶段**重新开始一个 500 亿 Token 的学习率调度**，比在主衰减阶段的最后 1000 亿 Token 中扩展上下文效果更好。在每个阶段，我们都运行了消融实验来寻找一个好的**长上下文数据混合**和 **RoPE theta 值**，并在 Ruler 基准上进行评估。

在此阶段，通常会**上采样长上下文文档**（如冗长的网页和书籍）以提高长上下文性能 (Gao et al., 2025)。我们运行了几个消融实验，上采样了书籍、文章，甚至遵循 Qwen 2.5-1M 的方法 (A. Yang, Yu, et al., 2025) 用 FineWeb-Edu 和 Python-Edu 合成生成的用于检索和填空任务的文档。令人惊讶的是，我们**没有观察到**比仅使用第三阶段的基线混合有任何改进，后者在 Ruler 上已经与 Llama 3.2 3B 和 Qwen 2.5 3B 等其他 SOTA 模型具有竞争力。我们推测这是因为基线混合自然地包含了来自网络数据和代码的长文档（估计占 Token 的 10%），并且使用 **NoPE** 也有帮助。

*   **RoPE ABF（带调整基础频率的 RoPE）：** 从 4k 到 32k 时，我们将 RoPE theta（基础频率）增加到 **2M**，从 32k 到 64k 时，我们将其增加到 **5M**。我们发现使用像 10M 这样的更大值会略微提高 RULER 分数，但会损害像 GSM8k 这样的某些短上下文任务，所以我们保留了 5M，它不影响短上下文。在此上下文扩展阶段，我们还借机进一步**上采样了数学、代码和推理 Q&A 数据**，并添加了几十万个 **ChatML 格式**的样本。
*   **YARN 外推：达到 128k**
    即使在 64k 上下文上训练后，我们仍希望 SmolLM3 能在推理时处理 128k。我们没有在 128k 序列上进行训练（成本高得令人望而却步），而是使用了 **YARN** (B. Peng et al., 2023)，它允许模型**外推到其训练长度之外**。理论上，YARN 允许序列长度增加四倍。我们发现使用 **64k 检查点**在 128k 上比使用 32k 检查点表现更好，证实了在更接近目标推理长度上训练的好处。然而，推到 256k（从 64k 增加四倍）显示 Ruler 性能下降，因此我们建议将模型使用到 **128k**。

至此，我们已经走过了 SmolLM3 的完整预训练旅程，从规划和消融实验到最终的训练运行，以及在此过程中所有幕后的挑战。

### **6.5 预训练收官**

我们已经涵盖了很多内容。从帮助我们决定**为什么和训练什么**的训练指南针，到战略规划、验证每个架构选择的**系统性消融实验**，再到实际的**训练马拉松**，其中在规模化时出现了意外（吞吐量神秘崩溃、数据加载器瓶颈，以及一个迫使我们在 1T Token 时重启的微妙的张量并行 Bug）。

那些光鲜的技术报告背后**混乱的现实**现在清晰可见：训练 LLM 与其说是关于架构创新和数据策划，不如说是关于**纪律严明的实验和快速调试**。

*   **规划**确定了什么值得测试。
*   **消融实验**验证了每个决定。
*   **监控**早期发现问题。
*   当事情不可避免地出问题时，**系统性的去风险化**会告诉你确切地应该从哪里找。

具体到 SmolLM3，这个过程交付了我们最初设定的目标：一个在 **11 万亿 Token** 上训练的 3B 模型，在数学、代码、多语言理解和长上下文任务上具有竞争力，处于 Qwen3 模型的帕累托前沿。

随着我们的基础模型检查点保存完毕，训练完成，GPU 终于冷却下来，我们可能会想称之为大功告成。毕竟，我们已经有了一个能很好地预测文本、在基准测试中获得高分，并展示了我们目标能力的模型。

**但还不够。** 因为今天人们想要的是**助手和编码智能体**，而不是原始的**下一个 Token 预测器**。

这就是 **后训练（Post-Training）** 发挥作用的地方。而就像预训练一样，**现实比论文所暗示的更加混乱**。

## **7. 超越基础模型 — 2025 年的后训练**
> *“预训练结束后，我们应该在一天内拿到一个 SFT 基线模型。”*

预训练赋予了 SmolLM3 原始的能力，但在 GPU 冷却下来之前，我们进入了模型能力的下一个前沿：**后训练（Post-Training）**。这包括**监督式微调、强化学习、模型合并**等等——所有这些都旨在弥合 **“一个能预测文本的模型”** 与 **“一个人们能实际使用的模型”** 之间的差距。

如果说**预训练**是**将知识强行灌输**到权重中，那么**后训练**就是将这种原始能力**雕琢**成某种**可靠且可控**的东西。而就像预训练一样，那些光鲜的后训练论文并没有捕捉到深夜的意外：GPU 熔毁、挑剔的数据混合，或者一个看似微不足道的 **聊天模板（Chat Template）** 决定如何波及下游基准测试。

在本节中，我们将展示我们如何驾驭后训练这个混乱的世界，将 SmolLM3 从一个强大的**基础模型**转变为一个**SOTA 的混合推理模型**。

### **7.1 后训练指南针：为什么 → 做什么 → 怎么做**

就像预训练一样，后训练也受益于一个清晰的指南针，以避免浪费研究和工程周期。以下是如何构建它：

*   **为什么要进行后训练？**
    我们在预训练指南针中概述的三种动机——**研究、生产和战略性开源**——同样适用于后训练。例如，你可能正在探索 RL 是否能解锁现有模型的新推理能力（研究），或者你可能需要出于延迟原因将一个大模型蒸馏成一个小模型（生产），或者你可能发现了一个特定用例中没有强大开放模型的空白（战略性开源）。

    区别在于，后训练是**建立在现有能力之上**，而不是从头创造它们。然而，在你启动你的 GPU 之前，请问问自己：
    *   **你真的需要进行后训练吗？** 许多开源模型现在在广泛的任务上与专有模型相媲美。有些甚至可以通过量化和适度的计算在本地运行。如果你想要一个**通用型助手**，Hugging Face Hub 上的现成模型可能已经能满足你的需求。
    *   **你是否有高质量、领域特定的数据？** 当你针对一个通用模型表现不佳的特定任务或领域时，后训练最有意义。用正确的数据，你可以调整模型以为你最关心的应用产生更准确的输出。
    *   **你能衡量成功吗？** 没有清晰的评估标准，你就不知道后训练是否真的有帮助。

*   **后训练应该实现什么？** 这取决于你的优先事项：
    *   你想要一个 **crisp 指令遵循者**，很少偏离主题吗？
    *   一个能按需切换**语气和角色**的多功能助手？
    *   一个能处理**数学、代码或智能体问题**的推理引擎？
    *   一个能用**多种语言**对话的模型？

*   **你将如何实现它？** 这就是“配方”发挥作用的地方。我们将涵盖：
    *   **监督式微调 (SFT)** 来灌输核心能力。
    *   **偏好优化 (PO)** 来直接从人类或 AI 偏好中学习。
    *   **强化学习 (RL)** 来超越监督数据，提炼可靠性和推理能力。
    *   **数据策划**来在多样性和质量之间取得正确的平衡。
    *   **评估**来跟踪进展并及早发现回归。

这个指南针让后训练的混乱得以落地。**“为什么”** 提供了方向，**“做什么”** 设定了优先事项，而 **“怎么做”** 则将雄心壮志变成了实际的训练循环。

让我们看看我们是如何为 SmolLM3 回答这些问题的：

*   **为什么？** 对我们来说，“为什么”很直接，因为我们有一个基础模型，需要在发布前进行后训练。同时，像 Qwen3 这样的**混合推理模型**越来越受欢迎，但展示如何训练它们的开放配方却很少。SmolLM3 给了我们一个解决这两个问题的机会：为一个真实世界的应用准备一个模型，并贡献一个完全开放的配方，使其与 Qwen3 的 1.7B 和 4B 模型一起处于帕累托前沿。
*   **做什么？** 我们着手训练一个为 SmolLM3 的优势量身定制的**混合推理模型**，主要是**推理质量应该在非英语语言中也保持良好**。而且由于现实世界的使用越来越多地涉及**工具调用**和**长上下文工作流**，这些成为了我们后训练配方的核心要求。
*   **怎么做？** 这就是本章的其余部分 😀。

就像预训练一样，我们从基础开始：**评估和基线**，因为每个大模型都始于一个小小的消融实验。但我们如何进行消融实验有一个关键区别。在预训练中，“小”通常意味着更小的模型和数据集。在后训练中，**“小”意味着更小的数据集和更简单的算法**。我们几乎从不为消融实验使用不同的基础模型，因为行为太依赖于模型，而且运行时间足够短，可以直接在目标模型上进行迭代。

让我们从许多模型训练者直到项目后期才避免的话题开始：**评估**。

### **7.2 首要任务：评估先于一切**

后训练的第一步——就像预训练一样——是决定**正确的评估集**。由于今天大多数 LLM 都被用作助手，我们发现，目标是**一个“好用”的模型**，比追求像 ARC-AGI 这样抽象的“智能”基准更有意义。那么，一个好的助手需要做什么呢？至少，它应该能够：

*   处理模糊的指令
*   进行分步规划
*   编写代码
*   在适当时调用工具

这些行为依赖于**推理、长上下文处理，以及数学、代码和工具使用技能**的混合。小到 3B 甚至更小的模型都可以作为好用的助手，尽管性能通常在 1B 以下急剧下降。

在 Hugging Face，我们使用一个**分层的评估套件**，呼应了我们在预训练消融实验部分详述的预训练原则（单调性、低噪音、高于随机的信号、排名一致性）。

以下是评估后训练模型的多种方式：

#### **能力评估（Capability Evals）**

这类评估针对基本技能，如推理以及有竞争力的数学和编码。

*   **知识：** 我们目前使用 **GPQA Diamond** (Rein et al., 2024) 作为科学知识的主要评估。这个基准由研究生水平的多项选择题组成。对于小模型，它远未饱和，比 MMLU 及其同类提供更好的信号，同时运行速度更快。另一个很好的事实性测试是 **SimpleQA** (Wei et al., 2024)，尽管小模型由于知识有限，在这个基准上往往表现不佳。
*   **数学：** 为了衡量数学能力，今天大多数模型都在最新版本的 **AIME**（目前是 2025 年版）上进行评估。**MATH-500** (Lightman et al., 2023) 对于小模型仍然是一个有用的健全性测试，但基本上已被推理模型饱和。要获得更全面的数学评估，我们推荐来自 **MathArena** 的评估。
*   **代码：** 我们使用最新版本的 **LiveCodeBench** 来跟踪编码能力。尽管针对的是竞技编程问题，我们发现 LiveCodeBench 上的改进确实能转化为更好的编码模型，尽管仅限于 Python。**SWE-bench Verified** 是一种更复杂的编码技能衡量标准，但对于小模型来说往往太难了，因此我们通常不考虑它。
*   **多语言性：** 不幸的是，在测试模型的多语言能力方面，选择并不多。我们目前依赖 **Global MMLU** (Singh et al., 2025) 来针对我们模型应该表现良好的主要语言，并包含 **MGSM** (Shi et al., 2022) 作为多语言数学能力的测试。

#### **综合任务评估（Integrated Task Evals）**

这些评估测试的是接近我们将要交付的东西：在半现实环境中的**多轮推理、长上下文使用和工具调用**。

*   **长上下文：** 最常用的长上下文检索测试是 **Needle in a Haystack (NIAH)** (Kamradt, 2023)，其中一个随机事实（“针”）被放置在一个长文档（“草堆”）的某个地方，模型必须将其检索出来。然而，这个基准太肤浅，无法区分长上下文理解能力，因此社区开发了更全面的评估，如 **RULER** (Hsieh et al., 2024) 和 **HELMET** (Yen et al., 2025)。最近，OpenAI 发布了 **MRCR** 和 **GraphWalks** 基准，扩展了长上下文评估的难度。
*   **指令遵循：** **IFEval** (J. Zhou et al., 2023) 是目前最流行的衡量指令遵循的评估，它使用自动评分来对照“可验证的指令”。**IFBench** (Pyatkin et al., 2025) 是 Ai2 的一个新扩展，它包含了比 IFEval 更多样化的约束，并减轻了最近模型发布中出现的一些 **“刷榜（benchmaxxing）”** 现象。对于**多轮指令遵循**，我们推荐 **Multi-IF** (He et al., 2024) 或 **MultiChallenge** (Sirdeshmukh et al., 2025)。
*   **对齐：** 衡量模型与用户意图对齐的程度通常通过**人类标注员**或**公共排行榜**（如 LMArena）来完成。这是因为像**自由形式生成、风格或整体帮助性**这样的质量很难用自动化指标进行量化衡量。然而，在所有情况下，运行这些评估都非常昂贵，这就是为什么社区转而使用 **LLM 作为人类偏好的代理**。这种风格最流行的基准包括 **AlpacaEval** (Dubois et al., 2025)、**ArenaHard** (T. Li et al., 2024) 和 **MixEval** (Ni et al., 2024)，后者与 LMArena 上的人类 Elo 评分**相关性最强**。
*   **工具调用：** **BFCL** 提供了对工具调用的全面测试，尽管它通常很快就会饱和。**TAU-Bench** (Barres et al., 2025) 测试了模型在模拟客户服务环境中使用工具和解决用户问题的能力，也已成为一个流行的报告基准。

#### **防止过拟合评估（Overfitting-Prevention Evals）**

为了测试我们的模型是否对特定技能过拟合，我们在我们的集合中包含了一些**鲁棒性或适应性评估**，如 **GSMPlus** (Q. Li et al., 2024)，它对 GSM8k (Cobbe et al., 2021) 中的问题进行**扰动**，以测试模型是否仍然能解决类似难度的问题。

#### **内部评估（Internal Evals）**

尽管公共基准在模型开发过程中可以提供一些有用的信号，但它们无法替代**实现你自己的内部评估**来针对特定能力，或要求**内部专家与你的模型互动**。例如，对于 SmolLM3，我们需要一个基准来评估模型是否能够进行**多轮推理**，所以我们实现了一个 **Multi-IF 的变体**来衡量这一点。

#### **“感觉”评估和竞技场（Vibe Evaluations and Arenas）**

同样，我们发现 **“感觉测试”（vibe testing）** 中间检查点（即与你的模型互动）对于揭示评估分数无法捕捉到的模型行为的**微妙怪癖**至关重要。正如我们稍后讨论的，感觉测试揭示了我们数据处理代码中的一个 Bug，即所有**系统消息都从语料库中被删除了**！这也可以大规模地进行以衡量人类偏好，就像在流行的 **LMArena** 上一样。然而，众包的人类评估往往是**脆弱的**（偏爱**谄媚和华丽的言辞**而不是实际的有用性），因此将其视为一种**低信号反馈**很重要。

具体到 SmolLM3，我们想要一个能够**可靠地遵循指令**并在数学和代码等流行领域**良好推理**的**混合推理模型**。我们还想确保我们保留了基础模型的**多语言能力**和**长上下文检索**能力。

这引导我们选择了以下评估集：

| 基准 | 类别 | 提示数量 | 指标 |
| :--- | :--- | :--- | :--- |
| **AIME25** | 竞技数学 | 30 | avg@64 |
| **LiveCodeBench (v4 验证, v5 最终发布)** | 竞技编程 | 100 (268) | avg@16 |
| **GPQA Diamond** | 研究生水平推理 | 198 | avg@8 |
| **IFEval** | 指令遵循 | 541 | accuracy |
| **MixEval Hard** | 对齐 | 1000 | accuracy |
| **BFCL v3** | 工具使用 | 4441 | mixed |
| **Global MMLU (lite 验证)** | 多语言问答 | 590,000 (6,400) | accuracy |
| **GSMPlus (mini 验证)** | 鲁棒性 | 10,000 (2,400) | accuracy |
| **RULER** | 长上下文 | 6,500 | accuracy |

让我们看几个来自每个评估的示例问题，以具体感受这些评估实际测试了什么：

浏览上面的示例，看看每个基准中的问题类型。注意领域的多样性如何确保我们在整个消融实验中测试了模型能力的不同方面。

对于我们正在处理的 3B 模型规模，我们认为这些评估会给我们**可操作的信号**，运行**比训练本身更快**，并让我们相信**改进是真实的**，而不仅仅是抽样带来的噪音。我们还跟踪了我们的**预训练评估**（完整列表见消融实验部分），以确保我们没有在基础模型性能上过多地回归。

#### **7.2.1 评估准则 (Rules of Engagement)**

让我们用一些我们从评估数千个模型中获得的来之不易的教训来总结本节：

1.  在模型开发期间，**使用小子集来加速评估**。例如，LiveCodeBench v4 与 v5 高度相关，但运行时间只有一半。或者，使用像 **tinyBenchmarks** (Polo et al., 2024) 这样的方法，它旨在找到可靠匹配完整评估的**最小提示子集**。
2.  对于**推理模型**，从**计分**的输出中**剥离思维链（Chain-of-Thought）**。这消除了假阳性，也直接影响了像 IFEval 这样的基准，后者会惩罚违反“写一首 50 词以内的诗”等约束的响应。
3.  如果一个评估使用 **LLM 作为裁判**，请**固定裁判及其版本**，以便进行苹果对苹果的比较。更好的是，使用一个**开源模型**，这样即使提供商弃用了裁判模型，评估也是可复现的。
4.  警惕基础模型中的**污染**。例如，大多数在 AIME 2025 之前发布的模型表现远差于 AIME 2024，这表明存在一些**刷榜**行为。
5.  如果可能，将消融实验中使用的任何东西都视为**验证集**，而不是**测试集**。这意味着为最终模型报告保留一组**未见过的基准**，类似于 Tulu3 评估框架 (Lambert et al., 2025)。
6.  始终包含一小部分关于你**自己的数据和任务**的**“感觉评估”**，以捕获对公共套件的过拟合。
7.  对于问题数量较少（通常少于约 2k）的评估，**抽样 k 次**并报告 **avg@k 准确率**。这对于减轻可能导致开发过程中错误决策的噪音很重要。
8.  在实施一个新的评估时，确保你能够**复现**几个已发布模型的结果（在一定误差范围内）。如果做不到这一点，以后如果需要修复实现并重新评估许多检查点，将会浪费大量时间。
9.  当有疑问时，**总是回到评估数据**，特别是检查你正在用什么提示你的模型。

有了评估在手，是时候训练一些模型了！在此之前，我们首先需要选择一个**后训练框架**。

### **7.3 行业工具**

每个后训练配方背后都有一个支持大规模实验的**框架和库的工具箱**。每个框架都带来了自己的一套支持的算法、微调方法和可扩展性特性。下表总结了主要的支持领域，从**监督式微调 (SFT)** 到**偏好优化 (PO)** 和**强化学习 (RL)**：

| 框架 | SFT | PO | RL | 多模态 | 全量微调 (FullFT) | LoRA | 分布式 |
| :--- | :-- | :- | :- | :-- | :--- | :--- | :-- |
| **TRL** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| **Axolotl** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| **OpenInstruct** | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ |
| **Unsloth** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| **vERL** | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ✅ |
| **Prime RL** | ✅ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ |
| **PipelineRL** | ❌ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ |
| **ART** | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | ❌ |
| **TorchForge** | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ✅ |
| **NemoRL** | ✅ | ✅ | ✅ | ❌ | ✅ | ❌ | ✅ |
| **OpenRLHF** | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ |

这里，**FullFT** 指的是**全量微调**，其中所有模型参数都在训练期间更新。**LoRA** 代表**低秩适应**，一种参数高效的方法，只更新小的低秩矩阵，同时保持基础模型冻结。**多模态**指的是是否支持训练文本以外的模态（例如图像），**分布式**表示是否可以在多于一个 GPU 上训练模型。

在 Hugging Face，我们开发和维护 **TRL**，所以它是我们的首选框架，也是我们用来后训练 SmolLM3 的框架。

#### **7.3.1 为什么还要费心使用框架？**

有一类研究人员喜欢抱怨使用训练框架，并认为你应该总是从头开始实现一切。这里的隐含主张是，“真正”的理解只来自于重新实现每一个 RL 算法，手动编码每一个分布式训练原语，或拼凑一个一次性的评估工具。

但这种立场忽略了现代研究和生产的现实。以 RL 为例。像 PPO 和 GRPO 这样的算法是**出了名的难以正确实现** (Huang et al., 2024)，在归一化或 KL 惩罚中的微小错误可能导致数天的计算和努力浪费。

同样，尽管写一个单文件实现的某些算法很诱人，但那个脚本能从 1B 扩展到 100B+ 参数吗？

框架之所以存在，正是因为**基础知识已经得到很好的理解**，无休止地重新发明它们是对时间的**低效利用**。这并不是说低级修补没有价值。从头开始实现一次 PPO 是一个很好的学习练习。在没有框架的情况下编写一个玩具 Transformer 可以让你了解注意力是如何真正工作的。但在大多数情况下，**只需选择一个你喜欢的框架，并为你的目的修改它**。

吐槽完了，让我们看看我们通常从哪里开始我们的训练运行。

### **7.4 为什么（几乎）每个后训练流程都从 SFT 开始**

如果你最近在 X 上花时间，你会认为**强化学习 (RL)** 是城里唯一的游戏。每天都有新的缩写词、算法调整和关于 RL 是否能引出新能力的激烈辩论 (Chu et al., 2025; Yue et al., 2025)。

当然，RL 并不新鲜。OpenAI 和其他实验室严重依赖来自人类反馈的强化学习 (RLHF) (Lambert et al., 2022) 来对齐他们的早期模型，但直到 DeepSeek-R1 (DeepSeek-AI, Guo, et al., 2025) 的发布，基于 RL 的后训练才真正在开源生态系统中流行起来。

但有一件事没有改变：几乎每个有效的后训练流程仍然以**监督式微调 (SFT)** 开始。原因很简单：

*   **它便宜：** 与 RL 相比，SFT 需要适度的计算。你通常可以在不需要烧掉一大堆硅的情况下获得有意义的收益，并且时间只是 RL 的一小部分。
*   **它稳定：** 与 RL 不同，后者对奖励设计和超参数非常敏感，SFT “就是好用”。
*   **它是正确的基线：** 一个好的 SFT 检查点通常能给你带来你所追求的大部分收益，并且它使像 DPO 或 RLHF 这样的后续方法更有效。

在实践中，这意味着 SFT 不仅仅是因为它简单而成为第一步；它是**在尝试任何更复杂的方法之前，始终能提高性能的一步**。当你处理基础模型时，尤其如此。除了少数例外，基础模型太粗糙，无法从高级的后训练方法中受益。

所以，如果 SFT 是大多数流程开始的地方，下一个问题是：你应该**微调什么**？这从选择正确的基础模型开始。

#### **7.4.1 挑选基础模型**

在为后训练选择基础模型时，一些实际的维度最重要：

*   **模型大小：** 尽管小模型随着时间的推移已大大改进，但今天仍然是**更大的模型泛化得更好**，而且通常用更少的样本。选择一个能代表你计划在训练后如何使用或部署模型的模型大小。在 Hugging Face Hub 上，你可以按模态和大小过滤模型以找到合适的候选者。
*   **架构（MoE vs. 密集型）：** MoE 模型每个 Token 激活一个参数子集，并提供**每单位计算的更高容量**。它们非常适合大规模服务，但根据我们的经验，**微调起来更棘手**。相比之下，**密集型模型更简单易训**，并且在较小规模上通常优于 MoE。
*   **后训练记录：** 基准测试很有用，但如果基础模型已经产生了一系列**与社区产生共鸣的强大后训练模型**，那就更好了。这为模型是否易于训练提供了一个**代理指标**。

根据我们的经验，来自 **Qwen、Mistral 和 DeepSeek** 的基础模型最适合后训练，其中 **Qwen** 是一个明显的宠儿，因为每个模型系列通常覆盖**很大的参数范围**（例如 Qwen3 模型的大小从 0.6B 到 235B！）。这个特性使得扩展变得更加直接。

一旦你选择了一个符合你部署需求的基础模型，下一步就是建立一个**简单、快速的 SFT 基线**来探查其核心技能。

#### **7.4.2 训练简单的基线**

对于 SFT，一个好的基线应该**训练快**，专注于模型的**核心技能**，并且在某个特定能力不达标时**易于用更多数据扩展**。选择用于初始基线的数据集涉及一些**品味和熟悉度**，需要了解哪些数据集可能质量高。

总的来说，**避免过度依赖**在学术基准上报告高分的公共数据集，而应专注于那些被用来训练像 **OpenHermes** 这样的优秀模型的数据集。例如，在 SmolLM1 的开发中，我们最初在 **WebInstruct** 上运行了 SFT，它在纸面上是一个很好的数据集。然而，在我们的感觉测试中，我们发现它**太偏向科学**了，因为模型会对像“你好吗？”这样的简单问候用**方程**来回应。

这促使我们创建了 **Everyday Conversations** 数据集，它对于在小模型中灌输基本的聊天能力至关重要。

对于 SmolLM3，我们着手训练一个混合推理模型，并最初选择了一小组数据集来针对**推理、指令遵循和可控性**。下表显示了每个数据集的统计数据：

| 数据集 | 推理模式 | 示例数量 | 示例百分比 | Token 数量 (M) | Token 百分比 | 平均每个示例 Token 数 | 平均上下文 Token 数 | 平均响应 Token 数 | 平均轮次 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Everyday Conversations** | /no_think | 2,260 | 2.3% | 0.6 | 0.8% | 260.2 | 222.3 | 94.0 | 7.8 |
| **SystemChats 30k** | /no_think | 33,997 | 35.2% | 21.5 | 28.2% | 631.9 | 422.8 | 267.7 | 6.3 |
| **Tulu 3 SFT Personas IF** | /no_think | 29,970 | 31.0% | 13.3 | 17.5% | 444.5 | 119.8 | 380.7 | 2 |
| **Everyday Conversations (Qwen3-32B)** | /think | 2,057 | 2.1% | 3.1 | 4.1% | 1,522.4 | 376.8 | 1,385.6 | 4 |
| **SystemChats 30k (Qwen3-32B)** | /think | 27,436 | 28.4% | 29.4 | 38.6% | 1070.8 | 84.6 | 1,042.7 | 2 |
| **s1k-1.1** | /think | 835 | 0.9% | 8.2 | 10.8% | 8,859.3 | 370.9 | 9,728.5 | 2 |
| **总计** | - | 96,555 | 100.0% | 76.1 | 100.0% | 2,131.5 | 266.2 | 2,149.9 | 4.0 |

正如我们在 SmolLM3 的开发过程中了解到的，训练混合推理模型比标准 SFT **更棘手**，因为你不能只是将数据集混合在一起；你需要**跨模式配对数据**。每个示例都必须清楚地指示模型是应该进行**扩展推理**还是给出**简洁的答案**，理想情况下，你想要平行的示例来教它何时切换模式。

从上表中需要注意的另一件事是，你应该**按 Token 而不是示例来平衡你的数据混合**：例如，s1k-1.1 数据集约占总示例的 1%，但由于长的推理响应，占了总 Token 的约 11%。

这为我们最关心的技能提供了基本覆盖，但也引入了一个新的挑战：每个数据集都必须以不同的方式格式化，取决于它是否应该启用扩展思考。为了统一这些格式，我们需要一个**一致的聊天模板**。

#### **7.4.3 挑选一个好的聊天模板**

在选择或设计聊天模板时，没有一刀切的答案。在实践中，我们发现有几个问题值得预先考虑：

*   **用户能否自定义系统角色？** 如果用户应该能够定义自己的系统提示（例如“像个海盗一样说话”），模板需要能干净地处理这一点。
*   **模型需要工具吗？** 如果你的模型需要调用 API，模板需要能容纳工具调用和响应的结构化输出。
*   **它是一个推理模型吗？** 推理模型使用像 `<think> ... </think>` 这样的模板来将模型的“思考”与最终答案分开。一些模型在对话中会跨轮次丢弃推理 Token，聊天模板需要处理这种逻辑。
*   **它能与推理引擎一起工作吗？** 像 vLLM 和 SGLang 这样的推理引擎有专门的解析器用于推理和工具。与这些解析器的兼容性可以省去很多麻烦，尤其是在复杂的智能体基准中，一致的工具调用至关重要。

下表显示了一些流行的聊天模板，以及它们在关键考量上的比较：

| 聊天模板 | 系统角色自定义 | 工具 | 推理 | 推理兼容性 | 备注 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **ChatML** | ✅ | ✅ | ❌ | ✅ | 简单，适用于大多数用例。 |
| **Qwen3** | ✅ | ✅ | ✅ | ✅ | 混合推理模板。 |
| **DeepSeek-R1** | ❌ | ❌ | ✅ | ✅ | 用 `<think>` 预填充推理内容。 |
| **Llama 3** | ✅ | ✅ | ❌ | ✅ | 内置工具，如 Python 代码解释器。 |
| **Gemma 3** | ✅ | ❌ | ❌ | ❌ | 系统角色自定义在第一个用户轮次定义。 |
| **Command A Reasoning** | ✅ | ✅ | ✅ | ❌ | 每个模型有多个聊天模板。 |
| **GPT-OSS** | ✅ | ✅ | ✅ | ✅ | 基于 Harmony 响应格式。复杂但多功能。 |

在大多数情况下，我们发现 **ChatML** 或 **Qwen** 的聊天模板是一个很好的起点。对于 SmolLM3，我们需要一个用于**混合推理的模板**，并发现 Qwen3 是少数几个在我们关心的维度上取得良好平衡的模板之一。然而，它有一个我们不完全满意的怪癖：**推理内容在对话中除了最后一轮外都被丢弃**。

尽管这对于推理是有意义的（为了避免撑爆上下文），我们得出结论，对于训练来说，**保留所有轮次的推理 Token** 以便适当地条件化模型是很重要的。

相反，我们决定**制作我们自己的聊天模板**，具有以下特点：

*   一个**结构化的系统提示**，像 Llama 3 和那些从专有模型中“越狱”出来的一样。我们也想提供完全覆盖系统提示的灵活性。
*   支持**代码智能体**，执行任意 Python 代码而不是进行 JSON 工具调用。
*   通过系统消息**显式控制推理模式**。

为了迭代聊天模板的设计，我们使用了 **Chat Template Playground**。这个方便的应用程序由我们在 Hugging Face 的同事开发，可以轻松预览消息如何渲染和调试格式问题。

一旦你确定了一些初始数据集和一个聊天模板，是时候训练一些基线了！

#### **7.4.4 婴儿基线 (Baby Baselines)**

在我们深入优化并挤压每一点性能之前，我们需要建立一些 **“婴儿基线”**。这些基线不是为了达到 SOTA（还不是），而是旨在**验证聊天模板是否按你所愿工作**，以及**初始超参数集是否产生稳定的训练**。只有在我们有了这个基础之后，我们才开始大力调整超参数和训练混合。

在训练 SFT 基线时，以下是主要需要考虑的事情：

*   你将使用**全量微调 (FullFT)** 还是像 **LoRA** 或 **QLoRA** 这样的参数高效方法？
*   你需要什么类型的**并行化**？对于小模型或用 LoRA 训练的模型，你通常可以用**数据并行**。对于更大的模型，你需要 **FSDP2** 或 **DeepSpeed ZeRO-3** 来共享模型权重和优化器状态。对于用长上下文训练的模型，使用像**上下文并行**这样的方法。
*   如果你的硬件支持，请使用像 **FlashAttention** 和 **Liger** 这样的内核。
*   **屏蔽 Loss**，只在助手 Token 上进行训练。
*   **调整学习率**；除了数据，这是决定你的模型是“平庸”还是“优秀”的最重要因素。
*   **打包训练样本**并调整序列长度以匹配你的数据分布。这将大大加快训练速度。TRL 有一个方便的应用程序来为你做这件事。

让我们看看这些选择在 SmolLM3 中是如何发挥作用的。对于我们的第一个基线实验，我们想要一个简单的健全性检查：**聊天模板是否真的能引出混合推理？** 为了测试这一点，我们比较了我们表格中的三种数据混合：

1.  **Instruct：** 在非推理示例上训练。
2.  **Thinking：** 在推理示例上训练。
3.  **Hybrid：** 在所有示例上训练。

对于每种混合，我们用 FullFT 在 SmolLM3-3B-Base 上运行了 SFT，学习率为 1e-5，有效批次大小为 128，并训练了 1 个周期。这些实验很快，在 8 块 H100 的一个节点上，根据子集的不同，耗时在 30-90 分钟之间。

这些结果很快向我们表明，混合模型表现出一种 **“分裂大脑”**，即一种推理模式的数据混合对另一种几乎没有影响。

#### **7.4.5 “感觉测试”你的基线**

尽管评估看起来还行，但当我们试图让混合模型扮演不同角色（例如像个海盗）时，它**始终忽略我们在系统消息中放置的任何内容**。经过一番挖掘，我们发现原因在于我们格式化数据的方式：

我们数据处理代码中的一个 **Bug** 将 `custom_instructions` 设置为 `None`，这实际上从每个训练样本中**移除了系统消息** 🙈！这尤其对 SystemChats 子集有问题，其中所有角色都通过 `custom_instructions` 定义，因此模型倾向于在对话中途随机切换角色。

修复这个 Bug 对评估没有影响，但最终我们确信聊天模板和数据集格式化是正常的。一旦你的设置稳定并且你的数据管道检查无误，下一步就是专注于开发特定的能力。

#### **7.4.6 针对特定能力**

在 Open-R1 的开发中，我们注意到完全在**单轮推理数据**上训练的基础模型将**无法泛化到多轮**。这并不奇怪；在没有此类示例的情况下，模型正在其训练分布之外进行测试。

为了为 SmolLM3 定量地衡量这一点，我们从 Qwen3 中获得灵感，他们开发了一个名为 **ThinkFollow** 的内部评估，它随机插入 `/think` 或 `/no_think` 标签来测试模型是否能持续切换推理模式。在我们的实现中，我们从 Multi-IF 中获取提示，然后检查模型是否生成了用 `<think>` 和 `</think>` 标签包裹的空或非空思考块。正如预期的那样，我们混合基线的结果显示，模型在第一轮之后就**惨不忍睹地无法启用推理模式**。

为了修复这个能力，我们构建了一个名为 **IFThink** 的新数据集。基于 Multi-IF 流程，我们从 Tulu 3 的指令遵循子集中获取单轮指令，并使用 Qwen3-32B 将它们扩展为多轮交流，以生成可验证的指令和推理轨迹。

在我们的基线混合中包含这些数据产生了**戏剧性的改进**。

#### **7.4.7 哪些超参数真正重要？**

在 SFT 中，只有少数几个超参数真正重要。**学习率、批次大小和打包**几乎决定了你的模型训练效率和泛化能力的一切。

*   **屏蔽用户轮次（Masking user turns）**
    在大多数聊天式数据集中，每个训练示例都由交替的用户和助手消息组成。如果我们训练模型预测所有 Token，它实际上是在学习自动完成用户查询，而不是专注于产生高质量的助手响应。屏蔽用户轮次可以防止这种情况，确保模型的 Loss 只在助手输出上计算。
*   **打包还是不打包？（To pack or not to pack?）**
    序列打包是对训练效率产生巨大影响的训练细节之一。在 SFT 中，大多数数据集包含可变长度的样本，这意味着每个批次包含大量的填充 Token，浪费计算并减慢收敛。打包通过将多个序列连接在一起直到达到期望的最大 Token 长度来解决这个问题。根据批次大小的不同，我们看到**打包将吞吐量提高了 3-5 倍**！然而，打包会略微改变训练动态：虽然你处理了更多的数据，但你进行的梯度更新更少，这可能影响最终性能，尤其是在小数据集上。
*   **调整学习率（Tuning the learning rate）**
    在 SFT 中，最优学习率通常比预训练时使用的**小一个数量级（或更多）**。这是因为我们从一个具有丰富表示的模型初始化，激进的更新可能导致**灾难性遗忘**。我们的建议是运行扫描，学习率范围在你 SFT 学习率的 **5 倍到 20 倍**之间。你很可能会在这个范围内找到你的最优性能！
*   **扩展周期数（Scaling the number of epochs）**
    在我们的消融实验中，我们通常只训练一个周期以快速迭代。一旦你确定了一个好的数据混合并调整了像学习率这样的关键参数，下一步就是为最终训练**增加周期数**。

#### **7.4.8 通过持续预训练提升推理能力**

**持续预训练**——或者如果你想听起来更时髦，**中途训练（mid-training）**——意味着在进行 SFT 之前，将一个基础模型在大量领域特定的 Token 上进一步训练。当中途训练的目标能力与 SFT 共享一个共同的核心技能，例如编码或推理时，中途训练非常有用。

在我们的实验中，我们发现使用像**NVIDIA 的后训练数据集**或 **OpenThoughts** 这样的推理数据集进行中途训练，然后再进行 SFT，**效果是戏剧性的**：对于扩展思考，我们在 AIME25 和 LiveCodeBench v4 上的性能几乎**翻了三倍**，而 GPQA-D 获得了整整 **10 个点**的提升。这些结果为我们提供了明确的证据，即对于推理模型，如果你的基础模型在预训练期间没有看到大量的推理数据，进行一定量的中途训练**几乎总是有意义的**。

### **7.5 从 SFT 到偏好优化：教模型什么是“更好”**

尽管你可以用更多数据来扩展 SFT，但在某个点上，你会观察到收益递减或失败模式，比如你的模型无法修复自己的错误代码。为什么？因为 SFT 是一种**模仿学习**，所以模型只学习复制它训练数据中的模式。如果数据中没有好的修复方案，或者如果期望的行为很难通过蒸馏引出，模型就没有明确的信号来判断什么是 **“更好”**。

这就是**偏好优化**发挥作用的地方。我们不只是复制演示，而是给模型**比较性的反馈**，比如“响应 A 比响应 B 好”。这些偏好为质量提供了更直接的训练信号，并使模型性能能够超越仅 SFT 的限制。

另一个好处是，你通常需要的**数据远少于 SFT**，因为起点已经是一个相当不错的模型，能够遵循指令并拥有先前训练阶段的知识。

#### **7.5.1 创建偏好数据集**

历史上，偏好数据集是通过向人类标注员提供成对的模型响应并要求他们评判哪个更好来创建的。这种方法仍然被 LLM 提供商用来收集**人类偏好**标签，但它**极其昂贵**且**难以扩展**。

最近，LLM 已经能够产生高质量的响应，并且通常以一种成本效益高的方式。这些进步使得 LLM **生成**许多应用的偏好变得可行。在实践中，有两种常见的方法：

1.  **强 vs. 弱（Strong vs. weak）**
2.  **在策略（On-policy）与评分**

在 SmolLM3 的开发时，不存在带有推理轨迹的偏好数据，所以我们决定用“强 vs. 弱”的方法生成一些我们自己的数据。我们使用来自 Ai2 的 Tulu 3 偏好混合中的提示，用 Qwen3-0.6B 和 Qwen3-32B 在 `/think` 模式下生成响应。结果是一个包含 25 万多个 LLM 生成偏好的大规模数据集，准备好用偏好优化算法同时在多个轴上改进我们的 SFT 检查点。

#### **7.5.2 我应该选择哪种算法？**

**直接偏好优化 (DPO)** (Rafailov et al., 2024) 是第一个在开源领域获得广泛采用的偏好优化算法。它的吸引力在于**实现简单、实践中稳定**，并且即使有适量的偏好数据也有效。因此，DPO 已成为在尝试更复杂的技术（如 RL）之前改进 SFT 模型的默认方法。

但研究人员很快发现有很多方法可以改进 DPO，如今有各种各样的替代方案可供探索。我们发现最有效的几种包括：

*   **卡尼曼-特沃斯基优化 (KTO)**
*   **优势比偏好优化 (ORPO)**
*   **锚定偏好优化 (APO)**

幸运的是，许多这些选择在 TRL 的 `DPOTrainer` 中只是**一行代码的更改**。如上图所示，在 IFEval 上，**APO-zero** 比 SFT 检查点提高了 **15-20 个百分点**！

#### **7.5.3 对于偏好优化，哪些超参数最重要？**

对于偏好优化，通常只有**三个**影响训练动态的超参数：

1.  **学习率**，通常比 SFT 使用的**小 10-100 倍**。
2.  **$\beta$ 参数**，通常控制偏好对之间的**边际大小**。
3.  **批次大小**。

#### **7.5.4 偏好优化准则 (Rules of Engagement)**

总结我们关于偏好优化的发现，可能对你未来的项目有用：

*   **不要害怕创建你自己的偏好数据！**
*   **选择 DPO 作为你的初始基线**，并从那里开始迭代。
*   使用**比 SFT 小约 10 倍的学习率**。
*   扫描 **$\beta$**，通常在 **0.01 到 0.5** 的范围内。
*   由于大多数偏好算法在一个周期后会过拟合，**划分你的数据并迭代训练**以获得最佳性能。

偏好优化通常是**简单性与性能之间的甜蜜点**，但它仍然继承了一个关键的限制：它**只和你收集的离线偏好数据一样好**。在某个点上，静态数据集的信号耗尽，你需要能够在模型与提示和环境互动时**在线生成新的训练反馈**的方法。这就是偏好优化与更广泛的**在策略和基于 RL 的方法**相遇的地方。

### **7.6 走向在策略及超越监督标签**

如果你希望你的模型能持续解决数学问题、生成可执行代码或跨多个步骤进行规划，你通常需要一个**奖励信号**，而不仅仅是“A 比 B 好”。

这就是 RL 开始有意义的地方。你不是用偏好来监督模型，而是让它与一个**环境**（可能是数学验证器、代码执行器，甚至真实的用户反馈）互动，并**直接从结果中学习**。RL 在以下情况中大放异彩：

*   你可以**自动检查正确性**，例如，单元测试、数学证明、API 调用，或有高质量的验证器或奖励模型。
*   任务需要**多步推理或规划**，其中局部偏好可能无法捕捉长期成功。
*   你想要优化**超越偏好标签的目标**，例如通过代码的单元测试或最大化某个目标。

在 LLM 领域，RL 主要有两种风格：

1.  **来自人类反馈的强化学习 (RLHF)**
2.  **带可验证奖励的强化学习 (RLVR)**

对于 SmolLM3，我们**完全跳过了 RL**，主要是由于时间限制，以及我们的模型已经通过离线偏好优化达到了同类最佳。然而，自发布以来，我们重新审视了这个主题，并将通过分享我们从将 RLVR 应用于混合推理模型的一些教训来结束本章。

#### **7.6.1 将 RLVR 应用于混合推理模型**

混合推理模型给 RLVR 带来了额外的复杂性，因为生成长度根据推理模式的不同而有很大差异。例如，在下图中，我们绘制了 SmolLM3 最终 APO 检查点在 AIME25 上的 Token 长度分布。

正如你所见，`/no_think` 模式生成的解决方案中位长度约为 2k Token，而 `/think` 模式要大得多，有 16k Token 和一个肥尾分布。理想情况下，我们希望用 RLVR 提高两种模式的整体性能，而不彻底改变它们各自的长度分布。

令我们惊讶的是，**天真地应用 GRPO** 会导致一种**奖励黑客（Reward Hacking）**：尽管从未被提示发出长的 CoT，模型学会了利用其基础推理能力来增加奖励。

这个问题可以通过包含一个**过长完成惩罚**来缓解，该惩罚会对超过一定长度的完成进行惩罚。

#### **7.6.2 RL 是城里唯一的游戏吗？**

其他在策略学习方法将偏好优化和蒸馏扩展到迭代循环中，随着模型的演变刷新训练信号：

*   **在线 DPO (Online DPO)**
*   **在策略蒸馏 (On-policy Distillation)**

这些方法模糊了静态偏好优化和完整 RL 之间的界限：你仍然可以从适应模型的当前分布中获益，但没有设计和稳定强化学习循环的全部复杂性。

#### **7.6.3 我应该选择哪种方法？**

尽管有无数关于哪种在策略方法“最好”的研究论文，但在实践中，决定取决于下表所示的几个因素：

| 算法 | 何时使用 | 权衡 | 最适合模型大小 |
| :--- | :--- | :--- | :--- |
| **在线 DPO** | 你可以廉价地获得偏好标签。最适合将行为与不断演变的分布对齐。 | 易于迭代扩展，比 RL 更稳定，但取决于标签质量和覆盖范围。在少数训练框架中支持。 | 任何大小，其中偏好能捕捉到超越模仿的改进。 |
| **在策略蒸馏** | 你可以访问一个更强的教师模型，并希望高效地转移能力。 | 实现简单，运行便宜，继承教师偏见，天花板受限于教师。仅在 TRL 和 NemoRL 中支持。 | 对中小型模型（<30B）最有效。 |
| **强化学习** | 当你有可验证的奖励或需要多步推理/规划的任务时最好。可以与奖励模型一起使用，但存在奖励黑客等挑战。 | 灵活且强大，但成本高且难以稳定；需要仔细的奖励塑造。在大多数后训练框架中支持。 | 中到大型模型（20B+），其中额外的容量让他们能利用结构化的奖励信号。 |

总的来说，我们认为在有效扩展 RL (Khatri et al., 2025) 和探索其他计算效率方法方面还有很多工作要做。确实是激动人心的时代！

### **7.7 后训练收官**

如果你已经走到了这一步，恭喜：你现在拥有了成功进行后训练所需的所有核心要素。你现在准备好运行许多实验并测试不同的算法以获得 SOTA 结果了。

但你可能已经意识到，知道如何训练伟大的模型只是故事的一半。要真正将这些模型变为现实，你需要**正确的基础设施**。让我们用 LLM 训练的**无名英雄**来结束这部巨著。

## **8. 基础设施 - 无名英雄**

现在你已经了解了我们关于模型创建和训练的所有知识，让我们来谈谈那个关键但被低估的组件，它能决定你的项目（以及你的银行账户）的成败：**基础设施**。无论你关注的是框架、架构还是数据策划，了解基础设施的基础知识都有助于**识别训练瓶颈、优化并行策略和调试吞吐量问题**。（至少，它能改善你与基础设施团队的沟通 😉）。

大多数训练模型的人都非常关心架构和数据，但很少有人了解基础设施的细节。基础设施的专业知识通常掌握在框架开发者和集群工程师手中，而其他人则将其视为一个已解决的问题：租一些 GPU，安装 PyTorch，然后你就可以开始了。

我们用 **384 块 H100** 训练了 SmolLM3 **近一个月**，总共处理了 **11 万亿 Token**……而这并非一帆风顺！在此期间，我们处理了**节点故障、存储问题和运行重启**（参见“训练马拉松”部分）。你需要有好的应急计划和策略来为这些问题做准备，并保持训练的平稳和低维护。

本章旨在弥合这一知识鸿沟。请将它视为一个**关于硬件层的实用指南**，专注于对训练重要的问题。（注意：每个小节都以一个 **“一句话总结”** 开始，这样你可以选择你的阅读深度。）

前两个部分将探讨硬件工作的基础知识：GPU 实际上由什么组成？内存层次结构是如何工作的？CPU 和 GPU 如何通信？我们还将介绍你在获取 GPU 时应考虑的因素，以及在投入长期训练运行前如何测试它们。最重要的是，我们将在每一步向你展示如何**自己测量和诊断这些系统**。接下来的部分则更具应用性，我们将看到如何使你的基础设施**对故障具有弹性**，以及如何**最大限度地优化你的训练吞吐量**。

**本章的游戏规则是：找到并修复瓶颈！**

请将这视为建立你对**某些设计决策为何重要**的直觉。当你理解你的模型激活需要流经多个具有不同带宽和延迟特性的缓存级别时，你自然会开始思考如何构建你的训练以最小化数据移动。当你看到节点间通信比节点内通信慢几个数量级时，你就会理解为什么并行策略如此重要。

让我们从打开一个 GPU，看看里面有什么开始。

### **8.1 GPU 内部：内部架构**

GPU 从根本上说是一个**为吞吐量而非延迟**而优化的**大规模并行处理器**。与擅长快速执行少量复杂指令流的 CPU 不同，GPU 通过**同时执行数千个简单操作**来实现性能。

理解 GPU 性能的关键在于认识到，它不仅仅是原始计算能力，而是**计算与数据移动之间的相互作用**。一个 GPU 可能有数万亿次浮点运算的理论计算能力，但如果数据无法足够快地到达计算单元，这些潜力就会被浪费。这就是为什么我们需要同时理解**内存层次结构（数据如何移动）**和**计算管线（工作如何完成）**。

因此，在最高层次上，GPU 执行两个基本任务：

1.  **移动和存储数据（内存系统）**
2.  **用数据做有用的工作（计算管线）**

#### **8.1.1 计算单元和 FLOPs**

> **一句话总结：** GPU 以 **FLOPs**（每秒浮点运算次数）衡量性能。像 H100 这样的现代 GPU 在**较低精度下**提供显著更高的吞吐量：BF16 为 990 TFLOPs，而 FP32 为 67 TFLOPs。然而，由于内存瓶颈，**实际性能是理论峰值的 70-77%**。SOTA 训练实现了 **20-41% 的端到端效率**，也称为**模型 FLOPs 利用率（MFU）**。在规划训练运行时，请使用**实际数字**，而不是市场宣传的规格。

GPU 计算性能以 **FLOPs（每秒浮点运算次数）** 来衡量。一个 FLOP 是一个单一的算术运算，通常是像 `a + b` 这样的浮点数加法，现代 GPU 每秒可以执行数万亿次（TFLOPs）。

GPU 计算的基本构建块是**流式多处理器（Streaming Multiprocessors, SMs）**，它们是并行执行指令的独立处理单元。每个 SM 包含两种类型的核心：用于标准浮点运算的 **CUDA 核心**，以及为矩阵乘法（深度学习中的主力操作，对 Transformer 性能至关重要）优化的专用 **Tensor Cores**。

现代 GPU 在芯片上组织了数百个这样的 SM！例如，H100 SXM5 版本（我们集群上使用的 GPU）包含 132 个 SM。每个 SM 独立操作，同步执行称为 **Warps** 的 32 个线程组。为了帮助实现这一点，SM 依赖于另一个组件，即 **Warp 调度器**：通过向不同的 Warp 平衡指令，当一个 Warp 被阻塞时，它们可以通过在 Warp 之间切换来“隐藏延迟”。这种 **SIMT（单指令，多线程）** 执行模型意味着一个 Warp 中的所有线程同时在不同的数据上执行相同的指令。

由于有数百个 SM，每个 SM 同时执行多个 Warp，一个 GPU 可以**同时运行数万个线程**。这种大规模并行性正是 GPU 擅长处理主导深度学习工作负载的矩阵运算的原因！

在讨论 FLOPs 时，**精度非常重要**。Tensor Cores 可以在不同的精度下操作（FP64、FP32、FP16/BF16、FP8、FP4）。因此，可达到的吞吐量根据数据类型的不同而有巨大差异，通常是数量级的差异。较低的精度格式能够实现更高的吞吐量，因为它们需要更少的数据移动，并且可以在相同的硅面积上封装更多的操作，但由于训练不稳定性，以前被避免使用。然而，如今，由于一系列新技术的出现，训练和推理都越来越多地被推向更低的精度，达到了 FP8 和 FP4。

下表显示了不同 NVIDIA GPU 代和精度的理论峰值性能：

| 精度 \ GPU 类型 | A100 | H100 | H200 | B100 | B200 |
| :--- | :-- | :-- | :-- | :-- | :-- |
| **FP64** | 9.7 | 34 | 34 | 40 | 40 |
| **FP32** | 19.5 | 67 | 67 | 80 | 80 |
| **FP16/BF16** | 312 | 990 | 990 | 1750 | 2250 |
| **FP8** | - | 3960 | 3960 | 4500 | 5000 |
| **FP4** | - | - | - | 9000 | 10000 |

较低精度下吞吐量的巨大提升不仅仅是原始速度的问题，它反映了我们对数值计算思考方式的根本转变。FP8 和 FP4 使模型能够**每瓦特和每秒执行更多操作**，使它们对于大规模训练和推理至关重要。

**理解这些数字：** 这些理论峰值 FLOPs 代表了在**理想条件下**可达到的最大计算吞吐量，即所有计算单元都得到充分利用且数据随时可用。在实践中，实际性能在很大程度上取决于你的工作负载能否让计算单元得到充足的数据供应，以及你的操作是否能有效地映射到可用的硬件上。

对于 SmolLM3，我们将在 NVIDIA H100 80GB HBM3 GPU 上进行训练，所以我们首先想将 H100 的理论 TFLOPs 规格与实际性能进行比较。为此，我们使用了 **SemiAnalysis GEMM 基准测试**：它测试了来自 Meta 的 Llama 70B 训练的真实世界矩阵乘法形状的吞吐量。

**验证理论性能：** 我们的实验揭示了理论峰值与可达到性能之间的差距。

*   对于 **BF16** 操作，我们持续达到了 **714-758 TFLOPs**，约占 H100 理论 990 TFLOPs 峰值的 **72-77%**。在实践中，这对于真实世界的工作负载来说是一个**极好的利用率**！
*   **FP8** 的结果更为微妙。使用 PyTorch 的 `torch._scaled_mm` 内核和 e4m3 精度，我们达到了 **1,210-1,457 TFLOPs**，约占理论 3,960 TFLOPs 峰值的 **31-37%**。😮 为什么？这种较低的利用率（在 FP8 中）实际上**并不表示性能不佳**；相反，它反映了随着计算吞吐量的增长，这些操作变得**越来越受内存限制**。Tensor Cores 处理 FP8 数据的速度比内存系统提供数据的速度快，使得**内存带宽成为限制因素**。

对于 SmolLM3 的训练，这些实际测量帮助我们设定了**现实的吞吐量期望**。在规划你自己的训练运行时，请使用这些**可达到的数字**而不是理论峰值来设定你的期望。

正如我们所见，当计算在低精度下变得过快时，GPU 内存似乎成为了一个瓶颈。让我们看看 GPU 内存是如何工作的，以及导致瓶颈发生的原因！

#### **8.1.2 GPU 内存层次结构：从寄存器到 HBM**

> **一句话总结：** GPU 将内存组织成一个从**快而小**（寄存器、共享内存）到**慢而大**（HBM 主内存）的层次结构。理解这个层次结构至关重要，因为现代 AI 通常是**受内存限制的**：瓶颈是**移动数据**，而不是对其进行计算。像 **Flash Attention** 这样的算子融合通过将中间结果保存在快速的**片上内存**中而不是写入慢速的 HBM，实现了 **2-4 倍的加速**。基准测试显示，H100 的 HBM3 在实践中提供了 **~3 TB/s** 的带宽，与大传输的理论规格相符。

为了可视化内存操作如何在 GPU 中实际流动，让我们首先看看 **NVIDIA Nsight Compute** 的**内存图**，这是一个剖析图，它以图形方式表示数据如何在任何你选择的内核的不同内存单元之间移动。

一般来说，内存图显示了**逻辑单元**（绿色）和**物理单元**（蓝色）。单元之间的链接表示单元之间发生的 **指令（Inst）** 或 **请求（Req）** 的数量，颜色表示峰值利用率的百分比：从未使用（0%）到以峰值性能运行（100%）。

现在让我们了解使这个图表成为可能的底层内存层次结构。现代 GPU 将内存组织成一个**平衡速度、容量和成本**的层次结构，这种设计是由基本的物理学和电路约束决定的。

在这个层次结构的底部是 **HBM（高带宽内存）**：GPU 的**主内存**，也称为**全局内存**或**设备内存**。H100 配备了 HBM3，理论带宽为 3.35 TB/s。HBM 是内存层次结构中**最大但最慢**的一层。

向上移动到计算单元，我们会发现**越来越快但越来越小**的内存层：

*   **L2 缓存：** 一个跨 GPU 共享的大型基于 SRAM 的缓存，通常几十兆字节。在 H100 上，这是 50 MB，带宽约为 **~13 TB/s**。
*   **L1 缓存和共享内存 (SMEM)：** 每个流式多处理器 (SM) 都有自己的 L1 缓存和程序员管理的共享内存，它们共享相同的物理 SRAM 存储。在 H100 上，这个组合空间是每个 SM 256 KB，每个 SM 的带宽约为 **~31 TB/s**。
*   **寄存器文件 (RMEM)：** 在层次结构的顶部，**寄存器是最快的存储**，直接位于计算单元旁边。寄存器是单个线程私有的，每个 SM 提供以 **~100s TB/s** 计的带宽。

**为什么这很重要：** 理解这个层次结构对于内核优化至关重要。关键的洞察是，**受内存限制的操作受限于你移动数据的速度，而不是你计算的速度**。这就是为什么**算子融合（operator fusion）**如此强大的原因：通过将多个操作组合成一个内核，你可以将**中间结果保存在快速的 SRAM 中**，而不是在操作之间将它们写回慢速的 HBM。**Flash Attention** 是这一原则在行动中的一个完美例子。

##### **8.1.2.1 示例：在实践中验证我们的 HBM3 带宽**

现在我们了解了内存层次结构，让我们将理论付诸实践，并验证我们 H100 GPU 上的**实际带宽**！这就是基准测试工具变得至关重要的地方。

**NVBandwidth** 是 NVIDIA 的开源基准测试工具，专门用于测量 GPU 系统的**带宽和延迟**。

让我们用它来测量我们 H100 的本地内存带宽，使用 `device_local_copy` 测试。结果揭示了内存系统的一个重要特性：对于**小消息尺寸（< 1 MB）**，我们是**受延迟限制**而不是受带宽限制。启动内存传输的开销主导了性能，阻止我们达到峰值带宽。然而，对于**大消息尺寸（≥ 1 MB）**，我们为读写操作都达到了 **~1,500 GB/s** 的持续带宽。

由于 HBM 带宽同时考虑了读写操作，我们将它们相加得到 **3 TB/s** 的总双向带宽（1,519 读 + 1,519 写），这与 H100 的理论 3.35 TB/s HBM3 规格非常接近。

#### **8.1.3 Roofline 模型**

理解你的内核是**受计算限制**还是**受内存限制**，决定了哪种优化会有帮助。

**Roofline 模型**提供了一个可视化的框架来理解这些性能特征并识别优化机会。它可以在我们之前提到的 **NSight Compute** 剖析工具中找到。

我们可以通过查看图表的两个划分区域来解释性能：

*   **受内存限制（Memory Bound）：** 这个区域的内核受内存带宽限制。GPU 在等待数据，增加计算能力无济于事。
*   **受计算限制（Compute Bound）：** 这个区域的内核受计算吞吐量限制。GPU 有足够的数据但处理不够快。

在我们的示例中，内核位于**受内存限制的区域**，表明通过优化内存流量仍有改进的空间！

现在我们了解了 GPU 内部发生的事情，让我们放大视野，探索 GPU 如何与世界其他部分通信。

### **8.2 GPU 外部：GPU 如何与世界对话**

现在我们了解了 GPU 如何利用其内部内存层次结构进行计算，我们需要解决一个关键的现实：**GPU 并非孤立运作**。在任何计算发生之前，数据必须加载到 GPU 的内存中。CPU 需要调度内核并协调工作。在分布式训练中，GPU 必须不断地相互交换激活、梯度和模型权重。

这就是**外部通信基础设施**变得至关重要的地方。无论你的 GPU 计算单元有多强大，如果数据无法足够快地到达它们——无论是从 CPU、从存储，还是从其他 GPU——你昂贵的硬件就会闲置。

在本节中，我们将探讨连接 GPU 与外部世界的四个关键通信链路：

1.  **GPU-CPU：** CPU 如何调度工作并将数据传输到 GPU。
2.  **GPU-GPU 节点内：** 同一台机器上的 GPU 如何通信。
3.  **GPU-GPU 节点间：** 不同机器上的 GPU 如何通过网络通信。
4.  **GPU-存储：** 数据如何从存储流向 GPU 内存。

#### 8.2.1 GPU-到-CPU

**一句话总结：** CPU 通过 PCIe 连接来指挥 GPU 工作，但在我们的 p5 实例中，CPU 到 GPU 的传输瓶颈约为 14.2 GB/s (PCIe Gen4 x8)。CPU-GPU 之间的延迟约为 1.4 微秒，这增加了内核启动（kernel launch）的开销，对于那些包含大量小型内核的工作负载来说是个大问题。CUDA Graphs 可以通过批量处理操作来减少这种开销。在多 CPU 插槽的系统上，NUMA 亲和性至关重要；在错误的 CPU 插槽上运行 GPU 进程会引入显著的延迟。像 Grace Hopper 这样的现代架构通过 NVLink-C2C（900 GB/s vs 128 GB/s）彻底消除了 PCIe 的瓶颈。

CPU 是 GPU 计算的总指挥官。它负责启动计算核心（kernels）、管理内存分配以及协调数据传输。但问题是，CPU 与 GPU 的通信速度究竟能有多快？这取决于它们之间的 PCIe（Peripheral Component Interconnect Express）连接。

理解这个连接至关重要，因为它直接影响到：

*   **内核启动延迟**：CPU 能多快地在 GPU 上调度工作。
*   **数据传输速度**：我们能在 CPU 和 GPU 内存之间多快地移动数据。
*   **同步开销**：CPU 与 GPU 协调点所需付出的代价。

在现代 GPU 服务器中，CPU-GPU 连接已经有了长足的进步。早期系统使用直接的 PCIe 连接，而像 DGX H100 这样的现代高性能系统则采用更复杂的拓扑结构，通过 PCIe 交换机来高效管理多个 GPU。而在最新的 GB200 架构中，NVIDIA 更进一步，将 CPU 和 GPU 放在同一块印刷电路板上，完全无需外部交换机。

为了识别潜在的瓶颈，我们首先需要了解我们 p5 实例的物理拓扑，然后测量这个关键链路的实际性能。

通过分析系统拓扑，我们可以发现系统中两个关键的 PCIe 带宽值：

1.  **15.75 GB/s**：对应 PCIe Gen4 x8 链路（从 CPU 到 PCIe 交换机）。
2.  **63.02 GB/s**：对应 PCIe Gen5 x16 链路（从 PCIe 交换机到 GPU）。

为了更好地理解整个拓扑结构，我们可以将其可视化。一个清晰的系统拓扑图会展示出我们系统的层级结构：

*   它包含两个 NUMA（非一致性内存访问）节点（每个 CPU 插槽对应一个 NUMA 内存区域）。
*   每个 CPU 插槽通过 PCIe Gen4 x8 链路（15.75GB/s）连接到四个 PCIe 交换机。
*   每个 PCIe 交换机通过 PCIe Gen5 x16 链路（63.02GB/s）连接到一个 H100 GPU。
*   …（我们将在后续章节探讨 NVSwitch、EFA 网卡和 NVMe 硬盘等其他组件。）

不同代的 PCIe 规范，其每条通道的传输速率都翻了一番。值得注意的是，传输速率（Transfer Rate）以 GT/s（每秒千兆次传输）为单位，代表原始信令速率；而吞吐量（Throughput）以 GB/s（每秒千兆字节）为单位，它考虑了编码开销，代表了实际可用的带宽：

| PCIe 版本 | 传输速率 (每通道) | 吞吐量 (GB/s) | | |
| :--- | :--- | :--- | :--- | :--- |
| | | **×1** | **×2** | **×4** |
| 1.0 | 2.5 GT/s | 0.25 | | |
| 2.0 | 5.0 GT/s | 0.5 | | |
| 3.0 | 8.0 GT/s | 0.985 | | |
| 4.0 | 16.0 GT/s | 1.969 | | |
| 5.0 | 32.0 GT/s | 3.938 | | |
| 6.0 | 64.0 GT/s | 7.563 | | |
| 7.0 | 128.0 GT/s | 15.125 | | |

从拓扑结构和 PCIe 带宽表中，我们可以看到 CPU 到 GPU 的路径经过了两次 PCIe 跳转：首先是从 CPU 到 PCIe 交换机（PCIe Gen4 x8，15.754 GB/s），然后是从 PCIe 交换机到 GPU（PCIe Gen5 x16，63.015 GB/s）。这意味着 CPU-GPU 通信的瓶颈在于第一跳，即 **15.754 GB/s**。让我们用另一个工具来验证这一点！

通过测量从主机（CPU）内存到设备（GPU）内存的异步拷贝带宽，结果确实显示，对于小消息，我们受限于延迟；但对于大消息，我们达到了约 **14.2 GB/s** 的速度，这大约是 PCIe Gen4 x8 理论带宽 15.754 GB/s 的 90%。这证实了在 CPU-GPU 通信中，从 CPU 到 PCIe 交换机的链路确实是我们的瓶颈。

除了带宽，延迟对于 CPU-GPU 通信同样重要，因为它决定了我们能多快地调度内核。我们通过一个采用“指针追逐”内核的测试来测量往返延迟，这种测试通过在主机（CPU）上分配一个缓冲区，并让 GPU 通过一个特殊的内核来访问它，从而模拟 CPU-GPU 通信的真实世界延迟。

结果显示，延迟大约为 **1.4 微秒**。这就解释了为什么我们在机器学习工作负载中经常观察到几微秒的内核启动开销。对于需要启动大量小型内核的工作负载，累加的延迟可能会成为瓶颈；否则，这种开销可以通过重叠执行来隐藏。

在像我们的 AMD EPYC 7R13 节点（2 个插槽，每个 48 核）这样的多插槽系统上，**NUMA 亲和性对 GPU 性能至关重要**。它指的是将进程运行在与其目标设备（如 GPU）共享同一插槽的 CPU 核心上。当你的 GPU 进程运行在与 GPU 连接的 NUMA 节点不同的 CPU 上时，操作必须跨越 CPU 互连（AMD Infinity Fabric），这会增加显著的延迟和带宽限制。

通过分析 NUMA 拓扑和节点间的“距离”，我们可以更好地理解性能影响。数据显示，访问同一 NUMA 节点上的内存（距离 10）比跨到另一个 NUMA 节点（距离 32）快得多。这种 **3.2 倍**的内存访问延迟差异，在你将进程绑定到错误的 NUMA 节点时，会严重影响 GPU 性能。

#### 8.2.2 节点内 GPU-到-GPU

在分布式训练中，GPU 必须频繁交换梯度、权重和激活值，每次迭代的数据量通常达到千兆字节。如此巨大的数据量需要对通信进行精细化处理。虽然 H100 的内部 HBM 读取速度可达约 3 TB/s，但错误地使用标志（flags）可能会彻底摧毁你的 GPU-到-GPU 通信带宽！

让我们来探究一下在同一节点内，GPU 之间所有可能的通信方式（以及你应该——或不应该——设置的所有标志）🙂

**一句话总结：** 节点内的 GPU 有三种通信方式：通过 CPU（最慢，约 3 GB/s，瓶颈在 PCIe），通过 GPUDirect RDMA over EFA NICs（约 38 GB/s），或通过 GPUDirect RDMA over NVLink（约 786 GB/s 双向）。NVLink 要快 9 到 112 倍，并完全绕过了 CPU/PCIe。NCCL 在可用时会自动优先选择 NVLink。NVLink SHARP (NVLS) 提供硬件加速的集合通信，将 allreduce 性能提升 1.3 倍，达到 480 GB/s。然而，alltoall 操作（340 GB/s）无法从 NVLS 加速中受益。

#### 8.2.3 通过 CPU

最朴素的方法是使用主机内存（SHM）：数据从 GPU1 出发，经过 PCIe 交换机到达 CPU，进入主机内存，再回到 CPU，再次穿过 PCIe 交换机，最终到达 GPU2。这种方式可以通过设置 `NCCL_P2P_DISABLE=1` 和 `FI_PROVIDER=tcp` 环境变量来实现（但不推荐）。

这种迂回的路径涉及多次内存拷贝，并会占满 PCIe 和 CPU 内存总线，从而造成拥塞。在我们的拓扑结构中，4 个 H100 共享相同的 CPU 内存总线，当多个 GPU 尝试同时通信时，它们会争夺有限的 CPU 内存带宽，这个问题就变得更加严重……😢

在这种由 CPU 介导的方法中，我们从根本上受限于 CPU 和 PCIe 交换机之间约 16 GB/s 的 PCIe Gen4 x8 链路。幸运的是，我们的 GPU 有一种更好的通信方式，无需 CPU 的介入：**GPUDirect RDMA**。

#### 8.2.4 通过 Libfabric EFA

GPUDirect RDMA（远程直接内存访问）是一项允许 NVIDIA GPU 之间直接通信的技术，通过直接访问对方的 GPU 内存来实现。这省去了数据经过系统 CPU 的步骤，避免了通过系统内存的缓冲拷贝，相比传统的 CPU 介导传输，性能提升可达 10 倍。GPUDirect RDMA 可通过 PCIe 在节点内实现快速的 GPU-到-GPU 通信，也可以利用支持 RDMA 的 NICs（网络接口卡）实现跨节点通信。

回顾我们的拓P扑图，可以看到每个 PCIe 交换机连接了 4 个 EFA（弹性光纤适配器）网卡，这意味着每个 GPU 都可以访问 4 个 EFA 适配器。EFA 是 AWS 为云实例定制的高性能网络接口，旨在提供低延迟、高吞吐量的实例间通信。在 p5 实例上，EFA 提供了应用程序可以使用的 libfabric 接口（一种专为高性能计算设计的通信 API），并提供类似 RDMA 的功能，从而支持 GPUDirect RDMA 实现跨节点的直接 GPU-到-GPU 通信。

每个 EFA 链路提供 100 Gbps（12.5 GB/s）的带宽。每个 GPU 有 4 个 EFA 网卡，每个节点有 8 个 GPU，这使得每个节点的总带宽达到 100 × 4 × 8 = 3200 Gbps（400GB/s）。

为了确保我们启用了基于 EFA 的 GPUDirect RDMA，你应该设置 `FI_PROVIDER=efa` 和 `NCCL_P2P_DISABLE=1` 环境变量。

虽然通过 EFA 的 GPUDirect RDMA 相比 CPU 介导的传输有了显著改进，每个 GPU 使用 4 个 EFA 网卡可达到约 50 GB/s 的速度，但我们还能更进一步吗？这就是 NVLink 发挥作用的地方。

#### 8.2.5 通过 NVLink

NVLink 是 NVIDIA 的高速、直连的 GPU-到-GPU 互连技术，可在服务器内部实现快速的多 GPU 通信。H100 采用了第四代 NVLink (NVLink 4.0)，通过 18 条链路为每个 GPU 提供 900 GB/s 的双向带宽，每条链路的双向速率为 50 GB/s。

在 DGX H100 架构中，4 个第三代 NVSwitch 以分层拓扑连接 8 个 GPU，确保任意两个 GPU 对之间都有多条直接路径，且跳数恒定为 1 个 NVSwitch，从而实现了 3.6 TB/s 的总双向 NVLink 网络带宽。

| | NVLink 2.0 (Volta) | NVLink 3.0 (Ampere) | NVLink 4.0 (Hopper) | NVLink 5.0 (Blackwell) |
| :--- | :--- | :--- | :--- | :--- |
| **带宽** | 300 GB/s | 600 GB/s | 900 GB/s | 1800 GB/s |
*表格：各代 NVLink 带宽对比，显示理论规格*

默认情况下，当可用时，NCCL 会优先使用 NVLink 进行节点内 GPU 通信，因为它提供了同一台机器上 GPU 之间延迟最低、带宽最高的路径。但是，如果你的标志设置不当，可能会阻止 NVLink 的使用！😱

NVLink 实现了直接的 GPU-到-GPU 内存访问，无需 CPU 或系统内存的参与。当 NVLink 不可用时，NCCL 会回退到基于 PCIe 的 GPUDirect P2P，或者在跨插槽 PCIe 传输性能不佳时使用共享内存（SHM）传输。

与 EFA 的约 50 GB/s 相比，NVLink 4.0 的理论带宽为 900 GB/s，我们预期在节点内通信上会有 18 倍的优势。为了在实践中验证这一点，我们测量了不同通信路径下的实际带宽：

结果毫无疑问地显示了 NVLink 的效率有多高：它达到了 **364.93 GB/s**，而 EFA 为 **38.16 GB/s**（快 9 倍，或双向 18 倍），CPU 基线为 **3.24 GB/s**（快 112.6 倍）。这些测量结果证实了为什么 NCCL 优先选择 NVLink 进行节点内 GPU 通信。为了进一步检验 NVLink 的性能，我们使用工具测量了所有 GPU 对之间同时双向拷贝的双向带宽：

测得的 **786 GB/s** 的双向带宽达到了 NVLink 4.0 理论 900 GB/s 规格的 85%。使用 NVLink 完全绕过了 CPU 瓶颈（对于 GPU-到-GPU 通信而言）！

但这在集合通信模式中表现如何呢？让我们用 NCCL 测试中的 `all_reduce_perf` 基准来测量单个节点内的 allreduce 性能。

等等……我们达到了 **480 GB/s**，这超过了 NVLink 4.0 的理论单向带宽 450 GB/s 😮 这是什么黑魔法？怎么可能？

深入研究文档后，答案似乎在于 **NVLink SHARP (NVLS)**，这是 NVIDIA 的硬件加速集合操作技术。它们为单节点 H100 GPU 上的 allreduce 操作提供了大约 **1.3 倍**的加速！

对于 alltoall 操作，我们达到了 **340 GB/s**，这与已发布的 H100 系统（使用 NVLink 4.0）的基准测试结果相符。与 allreduce 不同，alltoall 操作无法从 NVLS 硬件加速中受益，这就解释了为什么这里的速度是 340 GB/s，而不是 allreduce 实现的 480 GB/s。alltoall 模式需要在所有 GPU 对之间进行更复杂的点对点数据交换，纯粹依赖于 NVLink 的基础带宽，而非 NVSwitch 的集合加速功能。

虽然 NVLink 在单个节点内提供了卓越的带宽，但训练前沿模型需要扩展到多个节点。这就引入了一个新的潜在瓶颈：节点间网络互连，其带宽远低于 NVLink。

#### 8.2.6 跨节点 GPU-到-GPU

**一句话总结：** 多节点 GPU 通信使用 InfiniBand (400 Gbps) 或 RoCE (100 Gbps) 等高速网络。Allreduce 扩展性很好（在多节点间稳定在 320-350 GB/s），从而能够支持大规模训练集群。由于算法复杂性，Alltoall 的性能下降更剧烈。延迟从节点内的约 13μs 跃升至跨节点的 55μs+。对于需要频繁进行 all-to-all 操作的 MoE 工作负载，NVSHMEM 提供了异步的、由 GPU 发起的通信，其性能远超由 CPU 协调的传输。

随着模型规模超出单个节点的容纳能力，训练需要将计算分布到通过高速网络连接的多个节点上。在深入探讨基准测试之前，让我们先了解一下在多节点 GPU 集群中你会遇到的 3 种关键网络技术：

*   **以太网 (Ethernet)** 已从 1 Gbps 发展到 100+ Gbps，并仍在 HPC 和数据中心集群中广泛使用。
*   **RoCE (RDMA over Converged Ethernet)** 将 RDMA 功能引入以太网，使用 ECN 进行拥塞控制，而非传统的 TCP 机制。
*   **InfiniBand** 是 NVIDIA 的行业标准交换网络，提供高达 400 Gbps 的带宽和亚微秒级的延迟，其 RDMA 支持通过 GPUDirect RDMA 实现直接的 GPU-到-GPU 内存访问，同时绕过主机 CPU。

总结如下：

| 名称 | 以太网 (25–100 Gbps) | 以太网 (200–400 Gbps) | RoCE | Infiniband |
| :--- | :--- | :--- | :--- | :--- |
| **制造商** | 多家 | 多家 | 多家 | NVIDIA/Mellanox |
| **单向带宽 (Gbps)** | 25–100 | 200–400 | 100 | 400 |
| **端到端延迟 (μs)** | 10-30 | N/A | ~1 | <1 |
| **RDMA** | 否 | 否 | 是 | 是 |

对于 AWS p5 实例，我们使用弹性光纤适配器（**EFA**）作为网卡，每个 GPU 通过 PCIe Gen5 x16 通道连接到四个 100 Gbps 的 EFA 网卡。

如前所述，当 GPU 和网卡连接到同一个 PCIe 交换机时，GPUDirect RDMA 使它们的通信能够仅通过该交换机进行。这种设置允许充分利用 PCIe Gen5 x16 的带宽，并避免涉及其他 PCIe 交换机或 CPU 内存总线。理论上，每个节点 8 个 PCIe 交换机 × 每个交换机 4 个 EFA 网卡 × 每个 EFA 网卡 100 Gbps = **3200 Gbps (400GB/s)** 的带宽，这与我们在 AWS p5 规格中找到的带宽一致。那么在实践中表现如何呢？让我们通过在不同节点间运行相同的基准测试来找出答案！

##### 8.2.6.1 带宽分析

*   **点对点发送/接收**操作在 2-4 个节点上达到约 42-43 GB/s，但在 5 个以上节点时降至约 21 GB/s。这种性能下降是因为当扩展到超过 4 个节点时，NCCL 会自动将每个对等方的点对点通道数从 2 个减少到 1 个，这实际上将可用带宽利用率减半，而理论最大值仍为约 50 GB/s（4 个 EFA 网卡 × 12.5 GB/s）。
*   **all-reduce** 操作在单个节点内表现出色，达到 480 GB/s 的总线带宽。扩展到 2 个节点时，带宽几乎保持不变，为 479 GB/s，之后在 3-16 个节点时稳定在 320-350 GB/s 左右。这种模式揭示了一个重要特性：虽然在跨越节点边界时由于从 NVLink 转换到节点间网络结构而出现初始下降，但随着我们增加更多节点，带宽几乎保持不变。
*   **all-to-all** 操作显示出更严峻的扩展挑战：从单个节点的 344 GB/s 开始，带宽在 2 个节点时降至 81 GB/s，并继续下降到在更大集群中约为 45-58 GB/s。这种更陡峭的下降反映了 all-to-all 模式对网络的密集需求，其中每个 GPU 必须与跨节点的所有其他 GPU 通信，从而产生比 all-reduce 操作严重得多的网络拥塞。

##### 8.2.6.2 延迟分析

*   **发送/接收**操作在所有多节点配置中保持相对稳定的 40-53 μs 延迟，表明点对点通信延迟主要由基础网络往返时间决定，而非集群大小。
*   **All-reduce** 操作在单个节点内的延迟极小，为 12.9 μs，但在 2 个节点时跃升至 55.5 μs，并随着集群规模的扩大几乎呈线性增长，在 16 个节点时达到 235 μs。
*   **All-to-all** 操作表现出类似的趋势，从单节点通信的 7.6 μs 开始，但在 2 个节点时攀升至 60 μs，在 16 个节点时达到 621 μs。all-to-all 操作延迟的超线性增长表明，随着更多节点参与集合通信，网络拥塞和协调开销会复合增长。

#### 8.2.7 互连故障排查

如果你的带宽低于预期，系统地检查以下几个方面：

1.  **库版本**
    过时的 NCCL、EFA 或 CUDA 库可能缺少关键的性能优化或错误修复。始终确认你正在运行最新的、兼容的通信库版本。

2.  **CPU 亲和性配置**
    不正确的 CPU 亲和性设置可能导致不必要的跨 NUMA 流量，从而严重影响 NCCL 性能。每个 GPU 都应绑定到同一 NUMA 节点上的 CPU，以最小化内存访问延迟。

3.  **网络拓扑和放置**
    了解你的网络拓扑是诊断性能问题的关键。云服务商的放置组虽有帮助，但并不能保证实例间的网络跳数最少。在现代数据中心的胖树（fat-tree）拓扑中，放置在不同顶级交换机下的实例会因路由路径中额外的网络跳数而经历更高的延迟和可能更低的带宽。

4.  **正确的环境变量**
    缺失或不正确的网络适配器环境变量会严重限制带宽利用率。像 NCCL 这样的通信库依赖特定的配置标志来启用最佳性能特性，如自适应路由、GPU 发起的传输和适当的缓冲区大小。

5.  **容器特定注意事项**
    使用容器（如 Docker）时，几个配置步骤对优化 NCCL 性能至关重要：
    *   **共享和固定内存**：Docker 容器默认的共享和固定内存资源有限。用 `-shm-size=1g --ulimit memlock=-1` 启动容器以防止初始化失败。
    *   **NUMA 支持**：Docker 默认禁用 NUMA 支持。通过使用 `-cap-add SYS_NICE` 调用 Docker 来启用 NUMA 支持。
    *   **PCI 拓扑发现**：确保 `/sys` 被正确挂载，以便 NCCL 能够发现 GPU 和网卡的 PCI 拓扑。

现在你已经知道如何调试 GPU-CPU 和 GPU-GPU 通信中的瓶颈了，让我们来看看一个通常被忽视的 GPU 通信部分——与存储层的通信！

#### 8.2.8 GPU-到-存储

GPU 和存储系统之间的连接常常被忽视，但它会显著影响训练效率。在训练期间，GPU 需要持续从存储中读取数据（数据加载，特别是对于包含大型图像/视频文件的多模态数据），并定期将模型状态写回存储（即检查点）。对于现代大规模训练任务，如果这些 I/O 操作没有得到适当优化，就可能成为瓶颈。

**一句话总结：** GPU-存储 I/O 通过数据加载和检查点影响训练。GPUDirect Storage (GDS) 实现了 GPU 到存储的直接传输，绕过 CPU 以获得更好的性能。即使我们的集群没有启用 GDS，本地 NVMe RAID（8×3.5TB 硬盘组成的 RAID 0）也能提供 26.59 GiB/s 的速度和 337K IOPS（比网络存储快 6.3 倍），使其成为检查点的理想选择。

**GPUDirect Storage**
一个很自然的问题是，GPU 能否直接访问 NVMe 硬盘而无需 CPU 的介入？答案是肯定的，通过 **GPUDirect Storage (GDS)**。

GPUDirect Storage 是 NVIDIA GPUDirect 技术家族的一部分，它在存储（本地 NVMe 或远程 NVMe-oF）和 GPU 内存之间建立了一条直接数据路径。它通过允许存储控制器附近的 DMA 引擎直接将数据移入或移出 GPU 内存，从而消除了通过 CPU “反弹缓冲区”造成的不必要的内存拷贝。这降低了 CPU 开销、减少了延迟，并显著提高了数据密集型工作负载的 I/O 性能。

**块存储设备**
通过分析系统中的块设备，我们可以观察到：
*   一个 Amazon EBS 卷作为根文件系统。
*   八个 NVMe 硬盘（nvme1n1 到 nvme8n1）被配置成一个名为 MY_RAID 的 RAID 阵列。
*   这个 RAID 阵列被暴露为 `/dev/md0`，格式化为 XFS，并挂载在 `/scratch`，可用空间为 28TB（8x3.5TB）。

**网络存储**
除了本地 NVMe 存储，系统还可以访问网络附加存储系统，如 FSx Lustre 或 WekaFS。

**存储带宽基准测试**
为了理解每个存储系统的性能特征，我们可以对其读写速度进行基准测试。基准测试评估了吞吐量、延迟、IOPS，以及不同传输方式（`GPU_DIRECT` vs. `CPU_GPU` vs. `CPUONLY`）的效率。

基准测试揭示了我们四种存储系统之间巨大的性能差异：

*   **/scratch (本地 NVMe RAID)** 以 **26.59 GiB/s** 的吞吐量和 **337K IOPS** 遥遥领先，吞吐量比 FSx 快 **6.3 倍**，IOPS 高 **6.6 倍**。这个由 8 个 3.5TB NVMe 硬盘组成的本地 RAID 阵列提供了最低的延迟，并且随着线程数的增加扩展性极佳。
*   **/fsx (WekaFS)** 提供了坚实的网络存储性能，为 **4.21 GiB/s** 和 **51K IOPS**，是需要合理性能的共享数据的最佳选择。
*   **/admin (FSx Lustre)** 和 **/root (EBS)** 文件系统的吞吐量表现相似，约为 1.1 GiB/s，但在 IOPS 能力上差异显著。Admin 的 IOPS 性能（17K）比 Root（730）好得多，使其更适合有许多小操作的工作负载。Root 糟糕的 IOPS 性能证实了它只适合大型顺序操作。
*   **最佳配置模式**：在所有存储类型中，最大吞吐量出现在 1M 的 I/O 大小时，而最大 IOPS 出现在测试的最小尺寸（64K）时。这种经典的权衡意味着需要根据工作负载特性在原始带宽（大 I/O）和操作并发性（小 I/O）之间做出选择。

#### 8.2.9 总结

如果你坚持看到了这里，恭喜你！你现在对我们训练基础设施中的存储层次结构以及不同组件如何交互有了全面的了解。但我们希望你带走的核心观点是：**识别瓶颈是将理论知识转化为实际优化的分水岭**。

在本指南中，我们测量了技术栈中每一层的实际带宽：单个 GPU 内 HBM3 的 3TB/s，节点内 GPU 间 NVLink 的 786 GB/s，CPU-GPU 传输的 PCIe Gen4 x8 的 14.2 GB/s，节点间网络的 42 GB/s 点对点通信，以及从 26.59 GB/s（本地 NVMe）到 1.1 GB/s（共享文件系统）不等的存储系统。这些测量结果揭示了你的训练流程会在哪里变慢，对于实现高的模型 FLOPs 利用率（MFU）至关重要。

然而，原始带宽数据本身并不能说明全部问题。现代训练系统可以将计算与通信**重叠**，有效地将通信成本隐藏在计算操作之后。这种并行化有助于缓解瓶颈，即使互连速度较慢。

我们可以将所有基准测试的测量结果汇总起来，形成一个清晰的视图，它展示了随着距离 GPU 核心越来越远，带宽是如何急剧下降的。

现在我们知道了如何识别硬件和软件设置中的瓶颈，接下来让我们看看如何更进一步，确保我们拥有一个能够稳定运行数月的弹性系统。

### 8.3 构建弹性的训练系统

拥有快速的硬件只是拥有良好稳定的大模型训练基础设施的入场券。要从业余训练者走向专业，我们需要超越原始速度，关注那些不那么光鲜但却至关重要的基础设施部分，它们能让整个训练体验更顺畅，停机时间最少。

#### 8.3.1 节点健康监控与替换

拥有足够多的快速 GPU 对训练很重要，但由于大模型训练通常持续数周或数月，而非几天，因此长期跟踪 GPU 的健康状况变得至关重要。

*   **前期测试**：在启动训练前，我们使用多种工具对 GPU 进行了全面的诊断，包括内部的压力测试工具和 NVIDIA 的 DCGM 诊断工具。这些前期测试帮助我们揪出了两个有问题的 GPU，避免了它们在训练中引发故障。
*   **节点预留**：我们在 Slurm 管理的集群上为整个运行预留了固定的 48 个节点。这种设置使我们能够长期跟踪完全相同的节点的健康和性能。我们还预留了一个备用节点（就像汽车的备胎），一旦有节点出现故障，我们可以立即换上，而无需等待维修。
*   **持续监控**：在训练期间，我们跟踪所有节点的关键指标，如 GPU 温度、内存使用、计算利用率和吞吐量波动。我们使用 Prometheus 收集所有 GPU 的 DCGM 指标，并在 Grafana 仪表盘中进行实时监控。一个 Slack 机器人会在任何节点出现可疑行为时提醒我们，让我们能在硬件故障导致整个训练崩溃前主动更换它。

**热量现实检查：当 GPU 变慢时**

市场宣传的规格都假设有完美的散热，但现实要复杂得多。当 GPU 过热时，它们会自动降低时钟速度，即使在设计良好的系统中，性能也会降到理论最大值以下。我们通过监控 NVIDIA DCGM 的 `DCGM_FI_DEV_CLOCK_THROTTLE_REASONS` 指标来检测热节流。

热节流不仅会损害受影响的 GPU，它还会波及整个分布式训练设置。在我们的测试中，我们观察到一个节流的节点如何显著影响集合通信性能。木桶效应在这里体现得淋漓尽致：**你的速度取决于最慢的那个节点**。

👉 **关键教训**：在投入长期训练运行之前，使用前面提到的工具对你的硬件进行压力测试，以识别热量和功率限制。使用 DCGM 遥测持续监控温度，并为实际的热量限制做好规划。

#### 8.3.2 检查点管理

检查点是我们长期训练运行中的安全网。我们定期保存它们有三个实际原因：从故障中恢复、通过评估监控训练进度，以及与社区分享中间模型用于研究。其中，恢复方面最为重要。

在实现你的恢复机制时，要记住两个重要的细节：
1.  检查点保存应在后台进行，不影响训练吞吐量。
2.  注意你的存储空间。在一个 24 天的运行中，每 4 小时保存一次意味着约 144 个检查点。对于大型模型和优化器状态，这会迅速累积。我们的策略是，一次只在本地存储一个检查点（最新保存的），其余的卸载到 S3，以避免占满集群存储。

一个惨痛的教训：在我们过去的一次大规模运行中，脚本末尾一个用于旧吞吐量测试的 `rm -rf $CHECKPOINT_PATH` 命令被遗留下来。这个破坏性命令只有在 Slurm 作业实际完成时才会触发，而这在之前的重启中从未发生过。幸运的是，我们有前一天保存的检查点，只损失了一天的重训时间。教训是明确的：**永远不要在生产脚本中留下破坏性命令**，并且在保存后立即自动化检查点备份，而不是依赖手动干预。

#### 8.3.3 自动化评估

手动运行评估很快就会成为瓶颈。运行基准测试、跟踪和绘制每次运行的结果会累积成巨大的开销。解决方案？**从一开始就自动化一切**。

我们使用 LightEval 在 nanotron 检查点上运行评估。每个保存的检查点都会在集群上触发一个评估作业。结果直接推送到 Weights & Biases，所以我们只需打开仪表盘，观察曲线的演变。这为我们节省了大量时间，并使评估跟踪在整个运行期间保持一致。

**如果你在训练设置中只能自动化一件事，那就自动化评估。**

### 8.4 优化训练吞吐量

#### 8.4.1 我们需要多少 GPU？

在讨论了所有规格和基准之后，你仍然需要解决一个实际问题：你到底应该租用或购买多少个 GPU？

确定合适的 GPU 数量需要在训练时间、成本和扩展效率之间取得平衡。我们使用的框架如下：

**基本规模估算公式：**

`GPU 数量 = 总所需 FLOPs / (单个 GPU 吞吐量 × 目标训练时间)`

这个公式将问题分解为三个关键部分：
1.  **总所需 FLOPs**：训练你的模型所需的计算工作量。
2.  **单个 GPU 吞吐量**：每个 GPU 实际能提供的每秒 FLOPs（不是理论峰值！）。
3.  **目标训练时间**：你愿意等待多长时间来完成训练。

关键在于：你需要估算**真实的吞吐量**，而不是峰值规格。这意味着要考虑模型 FLOPs 利用率（MFU）：你实际达到的理论峰值性能的百分比。

对于我们的 3B 参数模型 SmolLM3，我们的计算如下：
*   **模型大小**：3B 参数
*   **训练 token 数**：11 万亿
*   **目标训练时间**：约 4 周
*   **预期 MFU**：30%（基于类似规模的实验）

首先，我们使用标准的 `6N` FLOPs/token 近似法计算总 FLOPs（N=参数量）：
`总 FLOPs = 6 × 3×10⁹ 参数 × 11×10¹² tokens = 1.98×10²³ FLOPs`

考虑到 30% 的 MFU，我们每个 GPU 的有效吞吐量为：
`有效吞吐量 = 720×10¹² FLOPs/sec × 0.30 = 216×10¹² FLOPs/sec`

现在代入我们的规模估算公式：
`GPU 数量 = 1.98×10²³ FLOPs / (216×10¹² FLOPs/sec × 4 周 × 604,800 秒/周) ≈ 379 个 GPU`

这个计算结果指向了 375-400 个 H100 GPU，最终我们获得了 384 个 H100，这个数量与我们的并行策略非常吻合，并给了我们一个现实的 4 周时间线，同时为节点故障和重启等意外问题留出了一些缓冲。

#### 8.4.2 找到最优的并行配置

一旦你准备好了 GPU，下一个挑战就是配置它们以实现高效训练。为此，并行策略变得至关重要。我们遵循“超大规模剧本”（Ultra-Scale Playbook）的方法来寻找最优的训练配置，该方法将问题分解为三个连续的步骤：首先确保模型能装入内存，然后达到目标批量大小，最后优化以获得最大吞吐量。

#### 8.4.3 步骤 1：让一个训练步骤能装入内存

第一个问题很简单：我们的 SmolLM3 3B 模型能装进单个 H100 的 80GB 内存吗？我们使用 nanotron 的 `predict_memory` 工具来估算内存消耗。

结果显示，我们已经非常接近 80GB 的上限。这意味着我们需要某种形式的并行策略来减少每个 GPU 的内存占用，无论是张量并行（TP）、流水线并行（PP），还是 ZeRO 优化器分片。

#### 8.4.4 步骤 2：达到目标全局批量大小

现在我们知道模型在某种并行策略下可以装入内存，我们需要确定如何达到我们约 200 万 token 的目标全局批量大小（GBS）。这个约束给了我们第一个方程：

`GBS = DP × MBS × GRAD_ACC × SEQLEN ≈ 2M tokens`

同时，我们还有来自 384 个 H100 的硬件约束：

`DP × TP × PP = 384`

这两个方程定义了我们的搜索空间。我们需要找到满足这两个约束条件，同时又能最大化训练吞吐量的值。

#### 8.4.5 步骤 3：优化训练吞吐量

我们的硬件设置呈现出两种截然不同的互连类型：用于节点内通信的 NVLink（900 GB/s）和用于节点间通信的 EFA（约 50 GB/s）。这种拓扑结构自然地建议我们使用至少两种并行形式来匹配我们的网络特性。

在排除了流水线并行（对于我们这个相对较小的 3B 模型，通信开销过大）和高于 ZeRO-0 的 ZeRO 等级（all-gather 和 reduce-scatter 操作损害了吞吐量）之后，我们将搜索空间大大缩小，专注于结合**数据并行**和适度**张量并行**的配置。

👉 为了评估每种配置，我们运行 5 次迭代的基准测试，并记录每个 GPU 每秒处理的 token 数（tok/s/gpu），这最终是我们关心的指标。

在系统地对 nanotron 中可用的选项进行基准测试后，我们确定了 **DP = 192** 的配置，它利用节点间的 EFA 带宽进行数据并行的梯度同步。对于张量并行，我们选择了 **TP = 2**，将张量并行的通信限制在单个节点内，以充分利用 NVLink 的高带宽。我们的微批量大小（Micro Batch Size）为 **3**，在内存使用和计算效率之间取得了平衡。最后，我们选择了 **ZeRO-0**，即不进行优化器状态分片。

这个配置达到了我们约 200 万 token 的目标全局批量大小（192 × 3 × 1 × 4096 ≈ 2.3M），同时在我们 384 个 H100 的集群上最大化了吞吐量。

## 9. 总结

我们的旅程始于一个简单的问题：在 2025 年，训练一个高性能的大语言模型（LLM）究竟需要什么？在走完从预训练到后训练的完整流程后，我们向你展示的不仅仅是技术，更是让这些技术行之有效的方法论。

**规模化预训练**。我们介绍了“训练罗盘”框架，用于决策是否要从头训练模型，然后展示了如何将目标转化为具体的架构决策。你已经看到如何建立可靠的消融实验流程，如何独立测试变更，以及如何从几十亿 token 的实验扩展到数万亿 token 的运行。我们记录了规模化训练中可能出现的各种基础设施挑战（吞吐量崩溃、数据加载器瓶颈、细微的 bug），以及如何通过监控和系统性的风险规避来及早发现并快速调试它们。

**后训练的实践**。我们展示了从一个基础模型到一个生产级助理需要其自身系统化的方法：在开始任何训练前先建立评估体系，迭代优化监督微调（SFT）的数据混合，应用偏好优化，并可选择地通过强化学习（RL）进一步提升。你已经看到，“感觉测试”（vibe testing）如何捕捉到指标遗漏的 bug，聊天模板如何悄无声息地破坏指令遵循能力，以及为什么在后训练中，数据混合的平衡与在预训练中同等重要。

在贯穿这两个阶段的过程中，我们反复回到同样的核心洞见：**通过实验验证一切，一次只改变一件事，预料到规模化会以新的方式打破常规，并让你的用例驱动决策，而不是追逐每一篇新论文**。遵循这一流程，我们训练出了 SmolLM3：一个具备竞争力的、支持长上下文的 3B 多语言推理模型。一路上，我们学到了很多关于什么可行、什么会出问题，以及当事情出错时如何调试。我们尽力将这一切都记录下来，无论是成功还是失败。

**接下来呢？**

这篇博客涵盖了现代 LLM 训练的基础知识，但这个领域发展迅速。这里有一些深入探索的途径：

*   **亲手做实验**。阅读关于消融实验的文章很有用，但亲手运行自己的实验才能教会你什么才是真正重要的。选择一个小模型，建立评估体系，然后开始实验吧。
*   **阅读源代码**。像 nanotron、TRL 等训练框架都是开源的。理解它们的实现细节能揭示出论文中一笔带过的关键之处。
*   **关注最新工作**。近期最先进模型的论文展示了该领域的发展方向。文末的参考文献部分包含了我们精选的有影响力的论文和资源。

我们希望这篇博客能帮助你更清晰、更自信地着手你的下一个训练项目，无论你是在一个推动前沿的大型实验室，还是一个解决特定问题的小团队。

现在，去训练点什么吧。当你凌晨两点发现损失函数（loss）神秘地飙升时，请记住：**每个伟大的模型背后都有一堆调试的故事**。

愿开源与开放科学的原力与你同在！

## 原文

https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook
